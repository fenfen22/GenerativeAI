Question,Answer,Source,Retrived_doc_1_source,Retrived_doc_1_id,Retrived_doc_1_content,Retrived_doc_1_score,Retrived_doc_2_source,Retrived_doc_2_id,Retrived_doc_2_content,Retrived_doc_2_score,Retrived_doc_3_source,Retrived_doc_3_id,Retrived_doc_3_content,Retrived_doc_3_score,Retrived_doc_4_source,Retrived_doc_4_id,Retrived_doc_4_content,Retrived_doc_4_score,Retrived_doc_5_source,Retrived_doc_5_id,Retrived_doc_5_content,Retrived_doc_5_score,Retrived_doc_6_source,Retrived_doc_6_id,Retrived_doc_6_content,Retrived_doc_6_score,Retrived_doc_7_source,Retrived_doc_7_id,Retrived_doc_7_content,Retrived_doc_7_score,Retrived_doc_8_source,Retrived_doc_8_id,Retrived_doc_8_content,Retrived_doc_8_score,Retrived_doc_9_source,Retrived_doc_9_id,Retrived_doc_9_content,Retrived_doc_9_score,Retrived_doc_10_source,Retrived_doc_10_id,Retrived_doc_10_content,Retrived_doc_10_score,recall@1,recall@2,recall@3,recall@4,recall@5,recall@6,recall@7,recall@8,recall@9,recall@10
What is PyTorch?,"It’s a Python based scientific computing package targeted at two sets of audiences:
A replacement for numpy to use the power of GPUs
a deep learning research platform that provides maximum flexibility and speed",notebook 3_1,notebook 3_1,0.0,"'markdown' cell: '['# Credits', '', 'This is heavily based on https://github.com/pytorch/tutorials']'

 'markdown' cell: '['# What is PyTorch?', '', '> **NOTE** In the last part of this lab cuda is used. If you have a cuda enabled machine, read the README.md in the root of this repo on how to use nvidia-docker.', '', '', 'It’s a Python based scientific computing package targeted at two sets of', 'audiences:', '-  A replacement for numpy to use the power of GPUs', '-  a deep learning research platform that provides maximum flexibility', '   and speed']'

 'markdown' cell: '['# Getting Started', '', 'In this lab you will get a quick start on what pytorch is and how to use it.', '', '## 1. Tensors', '', 'Tensors are similar to numpy’s ndarrays, with the addition being that', 'Tensors can also be used on a GPU to accelerate computing.']'

 'code' cell: '['import torch']'

 'markdown' cell: '['Construct a 5x3 matrix, uninitialized']'

 'code' cell: '['x = torch.Tensor(5, 3)', 'print(x)']'

 'markdown' cell: '['Construct a randomly initialized matrix']'

 'code' cell: '['x = torch.rand(5, 3)', 'print(x)']'

 'markdown' cell: '['Get its size']'

 'code' cell: '['print(x.size())']'",0.5742931365966797,notebook 3_2,0.0,"'markdown' cell: '['# Credits', '', 'This is heavily influenced or copied from https://github.com/pytorch/tutorials']'

 'markdown' cell: '['# Autograd: automatic differentiation', '', 'Central to all neural networks in PyTorch is the ``autograd`` package.', 'Let’s first briefly visit this, and we will then go to training our first neural network.', '', 'The `autograd` package provides automatic differentiation for all operations on Tensors.', 'It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.', '', 'Let us see this in more simple terms with some examples.']'",0.5791113376617432,notebook 3_2,4.0,"'code' cell: '['gradients = torch.FloatTensor([0.1, 1.0, 0.0001])', 'y.backward(gradients)', '', 'print(x.grad)']'

 'markdown' cell: '['**Read later** \\', '*Documentation* \\', '`Tensor`: https://pytorch.org/docs/stable/tensors.html \\', '`Function`: http://pytorch.org/docs/autograd']'

 'markdown' cell: '['# Assignments', '', '1. Define a tensor and set `requires_grad` to `True`', '3. Multiply the tensor by 2 and assign the result to a new python variable (i.e. `x = result`)', ""4. Sum the variable's elements and assign to a new python variable"", '5. Print the gradients of all the variables', '6. Now perform a backward pass on the last variable (NOTE: for each new python variable that you define, call `.retain_grad()`)', '7. Print all gradients again', '  - what did you notice?']'

 'code' cell: '[]'",0.6205123066902161,notebook 3_1,1.0,"'code' cell: '['x = torch.rand(5, 3)', 'print(x)']'

 'markdown' cell: '['Get its size']'

 'code' cell: '['print(x.size())']'

 'markdown' cell: '['**NOTE**: `torch.Size` is in fact a tuple, so it supports the same operations that a tuple supports.']'

 'code' cell: '['x[1:3] = 2', 'print(x)']'

 'markdown' cell: '['# Assignment', '', 'Make use of the pytorch docs <http://pytorch.org/docs/torch>', '1. Make a tensor of size (2, 17)', '2. Make a torch.FloatTensor of size (3, 1)', '3. Make a torch.LongTensor of size (5, 2, 1)', '  - fill the entire tensor with 7s', '4. Make a torch.ByteTensor of size (5,)', '  - fill the middle 3 indices with ones such that it records [0, 1, 1, 1, 0]']'

 'markdown' cell: '['## 2. Operations', ""There are multiple syntaxes for operations. Let's see addition as an example:"", '', '### 2.1 Addition: syntax 1']'

 'code' cell: '['y = torch.rand(5, 3)', 'print(x + y)']'

 'markdown' cell: '['### 2.2 Addition: syntax 2']'

 'code' cell: '['print(torch.add(x, y))']'",0.6254520416259766,notebook 5_3,12.0,"'markdown' cell: '[""When working with more complex data than what we use in this exercise, creating a PyTorch `DataLoader` on top of the dataset can be beneficial. A data loader is basically a fancy generator/iterator that we can use to abstract away all of the data handling and pre-processing + it's super useful for processing batches of data as well! Data loaders will come in handy later when you start to work on your projects, so be sure to check them out!"", '', 'For more information on how to use datasets and data loaders in PyTorch, [consult the official guide](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).']'

 'markdown' cell: '['## One-hot encodings']'

 'markdown' cell: '['We now create a simple function that returns the one-hot encoded representation of a given index of a word in our vocabulary. Notice that the shape of the one-hot encoding is equal to the entire vocabulary (which can be huge!). Additionally, we define a function to automatically one-hot encode a sentence.']'",0.6579793095588684,notebook 5_2,14.0,"'markdown' cell: '[""When working with more complex data than what we use in this exercise, creating a PyTorch `DataLoader` on top of the dataset can be beneficial. A data loader is basically a fancy generator/iterator that we can use to abstract away all of the data handling and pre-processing + it's super useful for processing batches of data as well! Data loaders will come in handy later when you start to work on your projects, so be sure to check them out!"", '', 'For more information on how to use datasets and data loaders in PyTorch, [consult the official guide](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).']'

 'markdown' cell: '['## Nanograd utilities']'

 'markdown' cell: '['We load necessary utility functions for the Nanograd library, which we saw in Lab 2.']'",0.6622036099433899,notebook 4_2,2.0,"'code' cell: '['# The output of torchvision datasets are PIL images in the range [0, 1]. ', '# We transform them to PyTorch tensors and rescale them to be in the range [-1, 1].', 'transform = transforms.Compose(', '    [transforms.ToTensor(),', '     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # subtract 0.5 and divide by 0.5', '    ]', ')', '', 'batch_size = 64  # both for training and testing', '', '# Load datasets', ""train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)"", ""test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"", 'train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)', 'test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)', '', '# Map from class index to class name.', 'classes = {index: name for name, index in train_set.class_to_idx.items()}']'",0.6653860807418823,notebook 4_3,7.0,"'markdown' cell: '['## Define a neural network', '', '**Assignment 1:** Adapt the CNN from the previous lab (CIFAR-10) to handle 224x224 images. We recommend reducing significantly the size of the tensors before flattening, by adding either convolutional layers with stride>1 or [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) layers (but see e.g. also [AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html)).']'

 'code' cell: '['class Model(nn.Module):', '', '    def __init__(self, num_classes):', '        super().__init__()', '        ', '        self.net = ...   # Your code here!', '', '    def forward(self, x):', '        return self.net(x)', '    ', '    ', 'model = Model(num_classes=len(np.unique(labels)))', ""device = torch.device('cuda')  # use cuda or cpu"", 'model = model.to(device)', 'print(model)']'

 'markdown' cell: '['## Define loss function and optimizer', '', '**Assignment 2:** Implement the criterion and optimizer, as in the previous notebook.']'

 'code' cell: '['loss_fn = None  # Your code here!', 'optimizer = None  # Your code here!']'

 'markdown' cell: '['## Train the network']'",0.6736620664596558,notebook 5_1,37.0,"while step < num_steps:', '        for batch in train_loader:', '            # concatenate the `token_ids``', '            batch_token_ids = make_batch(batch)', '            batch_token_ids = batch_token_ids.to(DEVICE)', '', '            # forward through the model', '            optimiser.zero_grad()', '            batch_logits = rnn(batch_token_ids)', '', '            # compute the loss (negative log-likelihood)', '            p_ws = torch.distributions.Categorical(logits=batch_logits) ', '', '            # Exercise: write the loss of the RNN language model', '            # hint: check the doc https://pytorch.org/docs/stable/distributions.html#categorical', ""            # NB: even with the right loss, training is slow and the generated samples won't be very good."", '            loss = ... # <- YOUR CODE HERE ', '', '            # backward and optimize', '            loss.backward()', '",0.6790587902069092,notebook 7_1,10.0,"'markdown' cell: '['Following we define the PyTorch functions for training and evaluation.']'

 'code' cell: '['import torch.optim as optim', '', '# if you want L2 regularization, then add weight_decay to SGD', 'optimizer = optim.SGD(net.parameters(), lr=0.25)', '', '# We will use pixel wise mean-squared error as our loss function', 'loss_function = nn.MSELoss()']'

 'markdown' cell: '['We can test the forward pass by checking whether the output shape is the same as the as the input.']'

 'code' cell: '['# test the forward pass', '# expect output size of [32, num_features]', 'x, y = next(iter(train_loader))', 'print(f""x.shape = {x.shape}"")', '', 'if cuda:', '    x = x.cuda()', '', 'outputs = net(x)', 'print(f""x_hat.shape = {outputs[\'x_hat\'].shape}"")']'

 'markdown' cell: '['In the training loop we sample each batch and evaluate the error, latent space, and reconstructions on every epoch.', '', '**NOTE** this will take a while on CPU.']'",0.6792935729026794,1.0,1.0,1.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0
What is the MNIST dataset?,"MNIST is a dataset that is often used for benchmarking. The MNIST dataset consists of 70,000 images of handwritten digits from 0-9. The dataset is split into a 50,000 images training set, 10,000 images validation set and 10,000 images test set. The images are 28x28 pixels, where each pixel represents a normalised value between 0-255 (0=black and 255=white).
<image>",notebook 3_4,notebook 3_4,1.0,"'markdown' cell: '['# MNIST dataset', 'MNIST is a dataset that is often used for benchmarking. The MNIST dataset consists of 70,000 images of handwritten digits from 0-9. The dataset is split into a 50,000 images training set, 10,000 images validation set and 10,000 images test set. The images are 28x28 pixels, where each pixel represents a normalised value between 0-255 (0=black and 255=white).', '', '![MNIST.Exampel](../static_files/mnist.png)', '', '', '## Primer', ""We use a feedforward neural network to classify the 28x28 mnist images. `num_features` is therefore $28 * 28=784$, i.e. we represent each image as a vector. The ordering of the pixels in the vector does not matter, so we could permutate all images using the same permutation and still get the same performance. (You are of course encouraged to try this using ``numpy.random.permutation`` to get a random permutation. This task is therefore called the _permutation invariant_ MNIST. Obviously this throws away a lot of structure in the data. In the next module we'll fix this with the convolutional neural network wich encodes prior knowledgde about data that has either spatial or temporal structure.  ""]'

 'markdown' cell: '['## MNIST', ""First let's load the MNIST dataset and plot a few examples:""]'",0.3596521019935608,notebook 7_1,5.0,"'markdown' cell: '['## MNIST', 'First let us load the MNIST dataset and plot a few examples. In this notebook we will use the *dataloaders* and *datasets* provided by PyTorch. Defining the loading of datasets using a dataloader has the advantage that it only load the data that is *neccessary* into memory, which enables us to use very large scale datasets.', '', 'We only load a limited amount of classes defined by the `classes` variable to speed up training.']'",0.4994814991950989,notebook 7_1,7.0,"'code' cell: '['# Plot a batch of MNIST examples', 'f, axarr = plt.subplots(4, 16, figsize=(16, 4))', '', '# Load a batch of images into memory', 'images, labels = next(iter(train_loader))', '', 'for i, ax in enumerate(axarr.flat):', '    ax.imshow(images[i].view(28, 28), cmap=""binary_r"")', ""    ax.axis('off')"", '    ', ""plt.suptitle('MNIST handwritten digits')"", 'plt.show()']'

 'markdown' cell: '['### Building the model', 'When defining the model the latent layer $z$ must act as a bottleneck of information. We initialize the AE with 1 hidden layer in the encoder and decoder using ReLU units as nonlinearities. The latent layer has a dimensionality of 2 in order to make it easy to visualise. Since $x$ are pixel intensities that are normalized between 0 and 1, we use the sigmoid nonlinearity to model the reconstruction.']'",0.5029288530349731,notebook 7_2,27.0,"'code' cell: '['#plot a few MNIST examples', 'f, axarr = plt.subplots(4, 16, figsize=(16, 4))', '', '# Load a batch of images into memory', 'images, labels = next(iter(train_loader))', '', 'for i, ax in enumerate(axarr.flat):', '    ax.imshow(images[i].view(28, 28), cmap=""binary_r"")', ""    ax.axis('off')"", '    ', ""plt.suptitle('MNIST handwritten digits')"", 'plt.show()']'

 'markdown' cell: '['## Building the model', 'When defining the model the latent layer must act as a bottleneck of information, so that we ensure that we find a strong internal representation. We initialize the VAE with 1 hidden layer in the encoder and decoder using relu units as non-linearity.']'",0.5222554206848145,notebook 3_4,2.0,"'markdown' cell: '['## MNIST', ""First let's load the MNIST dataset and plot a few examples:""]'

 'code' cell: '['mnist_trainset = MNIST(""./temp/"", train=True, download=True)', 'mnist_testset = MNIST(""./temp/"", train=False, download=True)']'

 'code' cell: '[""# To speed up training we'll only work on a subset of the data"", 'x_train = mnist_trainset.data[:1000].view(-1, 784).float()', 'targets_train = mnist_trainset.targets[:1000]', '', 'x_valid = mnist_trainset.data[1000:1500].view(-1, 784).float()', 'targets_valid = mnist_trainset.targets[1000:1500]', '', 'x_test = mnist_testset.data[:500].view(-1, 784).float()', 'targets_test = mnist_testset.targets[:500]', '', 'print(""Information on dataset"")', 'print(""x_train"", x_train.shape)', 'print(""targets_train"", targets_train.shape)', 'print(""x_valid"", x_valid.shape)', 'print(""targets_valid"", targets_valid.shape)', 'print(""x_test"", x_test.shape)', 'print(""targets_test"", targets_test.shape)']'",0.5512701869010925,notebook 3_4,3.0,"'code' cell: '['# plot a few MNIST examples', 'idx, dim, classes = 0, 28, 10', '# create empty canvas', 'canvas = np.zeros((dim*classes, classes*dim))', '', '# fill with tensors', 'for i in range(classes):', '    for j in range(classes):', '        canvas[i*dim:(i+1)*dim, j*dim:(j+1)*dim] = x_train[idx].reshape((dim, dim))', '        idx += 1', '', '# visualize matrix of tensors as gray scale image', 'plt.figure(figsize=(4, 4))', ""plt.axis('off')"", ""plt.imshow(canvas, cmap='gray')"", ""plt.title('MNIST handwritten digits')"", 'plt.show()']'",0.5652905702590942,notebook 4_1,6.0,"'code' cell: '['# Load the MNIST data. ', '', '# Note that we reshape the data from:', '#   (nsamples, num_features) = (nsamples, channels * height * width)', '# to:', '#   (nsamples, channels, height, width)', '# in order to retain the spatial arrangements of the pixels.', '', ""data = np.load('mnist.npz')"", 'channels, height, width = 1, 28, 28', '', '', 'def get_data(split, size):', '    x = data[f""X_{split}""][:size].astype(\'float32\')', '    x = x.reshape((-1, channels, height, width))', '    targets = data[f""y_{split}""][:size].astype(\'int64\')', '    return torch.from_numpy(x), torch.from_numpy(targets)', '', '', ""x_train, targets_train = get_data('train', 50000)"", ""x_valid, targets_valid = get_data('valid', 2000)"", ""x_test, targets_test = get_data('test', 5000)"", '', 'num_classes = len(np.unique(targets_train))', '', 'print(""Information on dataset"")', 'print(""Shape of x_train:"", x_train.shape)', 'print(""Shape of targets_train:"", targets_train.shape)', 'print(""Shape of x_valid:"", x_valid.shape)', 'print(""Shape of targets_valid:"", targets_valid.shape)', 'print(""Shape of x_test:"", x_test.shape)', 'print(""Shape of targets_test:"", targets_test.shape)']'",0.5800671577453613,notebook 4_1,5.0,"'markdown' cell: '['# Load packages']'

 'code' cell: '['%matplotlib inline', 'import numpy as np', 'import matplotlib.pyplot as plt', 'import seaborn as sns', 'import torch', 'from torch import nn', 'import torch.nn.functional as F', 'import torch.optim as optim', '', 'from sklearn.metrics import accuracy_score', 'from torch.utils.data import TensorDataset, DataLoader', 'from torchvision.utils import make_grid', '', 'sns.set_style(""whitegrid"")']'

 'markdown' cell: '['# Load MNIST data', '', 'The code below downloads and loads the same MNIST dataset as before.', 'Note however that the data has a different shape this time: `(num_samples, num_channels, height, width)`.']'

 'code' cell: '['# Download the MNIST dataset, if you have not already.', '!if [ ! -f mnist.npz ]; then wget -N https://www.dropbox.com/s/qxywaq7nx19z72p/mnist.npz; else echo ""mnist.npz already downloaded""; fi']'",0.6146206855773926,notebook 7_2,26.0,"'code' cell: '['from torch.utils.data import DataLoader', 'from torch.utils.data.sampler import SubsetRandomSampler', 'from torchvision.datasets import MNIST', 'from torchvision.transforms import ToTensor', 'from functools import reduce', '', '# Flatten the images into a vector', 'flatten = lambda x: ToTensor()(x).view(28**2)', '', '# Define the train and test sets', 'dset_train = MNIST(""./"", train=True,  transform=flatten, download=True)', 'dset_test  = MNIST(""./"", train=False, transform=flatten)', '', '# The digit classes to use', 'classes = [3, 7]', '', 'def stratified_sampler(labels):', '    """"""Sampler that only picks datapoints corresponding to the specified classes""""""', '    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))', '    indices = torch.from_numpy(indices)', '    return SubsetRandomSampler(indices)', '', '', 'batch_size = 64', 'eval_batch_size = 100', '# The loaders perform the actual work', 'train_loader = DataLoader(dset_train, batch_size=batch_size,', '                          sampler=stratified_sampler(dset_train.train_labels))', 'test_loader  = DataLoader(dset_test, batch_size=eval_batch_size, ', '                          sampler=stratified_sampler(dset_test.test_labels))']'",0.6161596179008484,notebook 4_1,7.0,"'code' cell: '['# Plot a few MNIST examples', 'plt.figure(figsize=(7, 7))', 'plt.imshow(make_grid(x_train[:100], nrow=10).permute(1, 2, 0))', ""plt.axis('off')"", 'plt.show()']'

 'markdown' cell: '['# Define a simple feed forward neural network']'",0.6284283399581909,1.0,1.0,1.0,1.0,2.0,3.0,3.0,3.0,3.0,3.0
Which optimizers are mentioned in the exercise notebook?,"Optimizer and learning rate:
SGD + Momentum: learning rate 0.01 - 0.1
ADAM: learning rate 3e-4 - 1e-5
RMSPROP: somewhere between SGD and ADAM",notebook 3_4,notebook 5_3,37.0,"'markdown' cell: '['### Optimization']'

 'markdown' cell: '[""Now that we can do forward passes and compute gradients with backpropagation, we're ready to train our network. For that we will need an optimizer. A common and easy to implement optimization method is stochastic gradient descent (SGD), which has the update rule: $\\theta_{n+1} = \\theta_{n} - \\eta \\frac{\\partial E}{\\partial \\theta_{n}}$, where $\\eta$ is the learning rate and $E$ is our cost function. This is essentially what's going on behind the scenes when you run `optimizer.step()` in PyTorch using the SGD optimizer. If you want to learn more about optimization in a deep learning context, [this is a great starting point](https://arxiv.org/abs/1609.04747).""]'

 'code' cell: '['def update_parameters(params, grads, lr=1e-3):', '    # Take a step', '    for param, grad in zip(params, grads):', '        param -= lr * grad', '    ', '    return params']'

 'markdown' cell: '['### Training loop']'

 'markdown' cell: '['We need to define a full training loop with a forward pass, backward pass, optimization step and validation. Training will take approximately 5 minutes, so you might want to read on while the notebook is running.']'",0.5811474323272705,notebook 4_1,11.0,"'        optimizer.zero_grad()', '        ', '        # Compute gradients based on the loss from the current batch (backpropagation).', '        loss.backward()', '        ', '        # Take one optimizer step using the gradients computed in the previous step.', '        optimizer.step()', '        ', '        step += 1', '        ', '        # Compute accuracy.', '        predictions = output.max(1)[1]', '        train_accuracies_batches.append(accuracy_score(targets, predictions))', '        ', '        if step % validation_every_steps == 0:', '            ', '            # Append average training accuracy to list.', '            train_accuracies.append(np.mean(train_accuracies_batches))', '            ', '            train_accuracies_batches = []', '        ', '            # Compute accuracies on validation set.', '            valid_accuracies_batches = []', '",0.6031391620635986,notebook 4_3,10.0,"in the previous step.', '        optimizer.step()', '        ', '        step += 1', '        ', '        # Compute accuracy.', '        predictions = output.max(1)[1]', '        train_accuracies_batches.append(accuracy(targets, predictions))', '        ', '        if step % validation_every_steps == 0:', '            ', '            # Append average training accuracy to list.', '            train_accuracies.append(np.mean(train_accuracies_batches))', '            ', '            train_accuracies_batches = []', '        ', '            # Compute accuracies on validation set.', '            valid_accuracies_batches = []', '            with torch.no_grad():', '                model.eval()', '                for inputs, targets in test_loader:', '                    inputs, targets =",0.628729522228241,notebook 8_3,0.0,"'markdown' cell: '['# Solve cartpole with REINFORCE', '', '> By Jonas Busk ([jbusk@dtu.dk](mailto:jbusk@dtu.dk))', '', '**2019 update:** Changes have been made to the display of environments due to the previous `viewer` being incompatible with newer versions of Gym.', '', '**2022 update:** Rendering was disabled, and the notebook now uses the `colabgymrender` package to render a video.', '', 'In this part, we will create an agent that can learn to solve the [cartpole problem](https://gym.openai.com/envs/CartPole-v0/) from OpenAI Gym by applying a simple policy gradient method called REINFORCE.', 'In the cartpole problem, we need to balance a pole on a cart that moves along a track by applying left and right forces to the cart.', '', 'We will implement a probabilistic policy, that given a state of the environment, $s$, outputs a probability distribution over available actions, $a$:', '', '$$', 'p_\\theta(a|s)', '$$', '', 'The policy is a neural network with parameters $\\theta$ that can be trained with gradient descent.', 'When the set of available actions is discrete, we can use a network with softmax output do describe the distribution.', 'The core idea of training the policy network is quite simple: *we want to maximize the expected total reward by increasing the probability of good actions and decreasing the probability of bad actions*. ', '', 'To achieve this, we apply the gradient of the expected discounted total reward (return):', '', '$$', '\\begin{align}', '\\nabla_\\theta",0.629010021686554,notebook 3_3,12.0,"'markdown' cell: '['To train our neural network we need to update the parameters in the direction of the negative gradient w.r.t the cost function we defined earlier.', 'We can use [`torch.optim`](http://pytorch.org/docs/master/optim.html) to get the gradients with some update rule for all parameters in the network.', '', 'Heres a small animation of gradient descent: http://imgur.com/a/Hqolp, which also illustrates which challenges optimizers might face, e.g. saddle points.']'

 'code' cell: '['import torch.optim as optim', '', 'optimizer = optim.SGD(net.parameters(), lr=0.01)']'

 'markdown' cell: '['Next, we make the prediction functions, such that we can get an accuracy measure over a batch']'

 'code' cell: '['def accuracy(ys, ts):', '    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions', '    correct_prediction = torch.eq(torch.max(ys, 1)[1], torch.max(ts, 1)[1])', '    # averaging the one-hot encoded vector', '    return torch.mean(correct_prediction.float())']'

 'markdown' cell: '['The next step is to utilize our `optimizer` repeatedly in order to optimize our weights `W_1` and `b_1` to make the best possible linear seperation of the half moon dataset.']'",0.6297282576560974,notebook 4_3,9.0,"'code' cell: '['num_epochs = 10', 'validation_every_steps = 50', '', 'step = 0', 'model.train()', '', 'train_accuracies = []', 'valid_accuracies = []', '        ', 'for epoch in range(num_epochs):', '    ', '    train_accuracies_batches = []', '    ', '    for inputs, targets in train_loader:', '        inputs, targets = inputs.to(device), targets.to(device)', '        ', '        # Forward pass.', '        output = model(inputs)', '        ', '        # Compute loss.', '        loss = loss_fn(output, targets)', '        ', '        # Clean up gradients from the model.', '        optimizer.zero_grad()', '        ', '        # Compute gradients based on the loss from the current batch (backpropagation).', '        loss.backward()', '        ', '        # Take one optimizer step using the gradients computed in the previous step.', '        optimizer.step()', '        ', '        step += 1', '        ', '",0.6496273279190063,CoursePlan.txt,17.0,"and take notes for at least 3 questions to ask. Link to lecture slides 2016 slides and 2017 slides and 2020 slides.

    Reading material DL Chapter 14 and 20.10.3. (Further learning a course dedicated to generative modelling.)
    One exercise from the book chapters.
    Carry out computer exercises week 7 on autoencoder un- and semi-supervised. Hand in and peergrade on peergrade.io like in previous weeks.
    Project selection deadline is this week (see above).

Week 8 - Reinforcement learning 

    Watch week 6 video lectures 

    02456week6 1 1 reinforcement learning
    02456week6 1 2 reinforcement learning approaches
    02456week6 2 1 AlphaGo policy and value networks
    02456week6 2 2 AlphaGo steps 1 to 4
    02456week6 3 policy gradients
    02456week6 4 a few last words
    2017 Deep Q learning
    2017 Evolutionary strategies

and take notes for at least 3 questions to ask. Link to lectures here and here for 2017 update.

    Reading: another nice blog post by Andrei Karpathy. Optional reading material on the connection between variational and reinforcement learning.
    One exercise from the book chapters. 
    Computer exercises on reinforcement learning methods (policy gradient, deep Q learning, evolutionary strategies) in the openAI Gym. Carry out exercises week 8. Hand in and peergrade on peergrade.io like in previous weeks.
    Project work.",0.6521215438842773,notebook 8_3,5.0,"'markdown' cell: '['Taking random actions does not do a very good job at balancing the pole. Let us now apply the Policy Gradient method described above to solve this task!', '', ""Let's first define our network and helper functions.""]'

 'code' cell: '['class PolicyNet(nn.Module):', '    """"""Policy network""""""', '', '    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):', '        super(PolicyNet, self).__init__()', '        # network', '        self.hidden = nn.Linear(n_inputs, n_hidden)', '        self.out = nn.Linear(n_hidden, n_outputs)', '        # training', '        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)', '', '    def forward(self, x):', '        x = self.hidden(x)', '        x = F.relu(x)', '        x = self.out(x)', '        return F.softmax(x, dim=1)', '    ', '    def loss(self, action_probabilities, returns):', '        return -torch.mean(torch.mul(torch.log(action_probabilities), returns))']'

 'code' cell: '['def compute_returns(rewards, discount_factor):', '    """"""Compute discounted returns.""""""', '    returns = np.zeros(len(rewards))', '    returns[-1] = rewards[-1]', '    for t in reversed(range(len(rewards)-1)):', '        returns[t] = rewards[t] + discount_factor * returns[t+1]', '    return returns']'",0.6532965898513794,notebook 8_3,13.0,"'markdown' cell: '['## Exercises', '', 'Now it is your turn! Make sure you read and understand the code, then play around with it and try to make it learn better and faster.', '', 'Experiment with the:', '', '* number of episodes', '* discount factor', '* learning rate', '* network layers', '', '', '### Exercise 1 ', '', '*Describe any changes you made to the code and why you think they improve the agent. Are you able to get solutions consistently?*', '', '**Answer:**', '', '*Answer here...*', '', '### Exercise 2 ', '', '*Consider the following sequence of rewards produced by an agent interacting with an environment for 10 timesteps:*', '', '[0, 1, 1, 1, 0, 1, 1, 0, 0, 0]', '', '* *What is the total reward?*', '* *What is the total future reward in each timestep?*', '* *What is the discounted future reward in each timestep if $\\gamma = 0.9$?*', '', '*Hint: See introdution notebook.*', '', '**Answer:**', '', '*Answer here...*', '', '### Exercise 3', '', '*In the training output, you will sometimes observe the validation reward starts out lower than the training reward but as training progresses they cross over and the validation reward becomes higher than the training reward. How can you explain this behavior?*', '', '*Hint: Do we use the policy network in the same way during training and validation?*', '', '**Answer:**', '', '*Answer here...*', '', '### Exercise 4', '', '*How does the policy gradient method we have used address the",0.6543055772781372,notebook 8_3,3.0,"'markdown' cell: '['*Note: For simple reinforcement learning problems (like the one we will address in this exercise) there are simpler methods that work just fine. However, the Policy Gradient method (with some extensions) has been shown to also work well for complex problems with high dimensional inputs and many parameters, where simple methods become inadequate.*']'

 'markdown' cell: '['## Policy gradient']'

 'code' cell: '['# Install colabgymrender to display gym environments in Colab', '!pip install gym[classic_control] > /dev/null 2>&1', '!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1', '!pip install colabgymrender > /dev/null 2>&1', '!pip install imageio==2.4.1 > /dev/null 2>&1']'

 'code' cell: '['%matplotlib inline', 'import matplotlib.pyplot as plt', 'import numpy as np', 'import torch', 'import torch.nn as nn', 'import torch.optim as optim', 'import torch.nn.functional as F', 'import gym', 'from gym import wrappers', 'from colabgymrender.recorder import Recorder']'

 'markdown' cell: '['First we create the environment:']'

 'code' cell: '[""env = gym.make('CartPole-v0') # Create environment""]'",0.6563199162483215,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
Describe the model given in the exercise notebook?,"The provided code defines a PyTorch neural network for a classification task with the following characteristics:

It has two layers: an input layer and a hidden layer.
The activation function used is Exponential Linear Unit (ELU).
The network is designed for a classification task with 10 output classes.
The number of hidden units in the hidden layer is 512.
The number of input features is determined based on the shape of the input data .
The forward method defines how data flows through the network by applying linear transformations followed by ELU activation.",notebook 3_3,notebook 8_3,11.0,"'code' cell: '['# plot results', 'def moving_average(a, n=10) :', '    ret = np.cumsum(a, dtype=float)', '    ret[n:] = ret[n:] - ret[:-n]', '    return ret / n', '', 'plt.figure(figsize=(16,6))', 'plt.subplot(211)', ""plt.plot(range(1, len(training_rewards)+1), training_rewards, label='training reward')"", 'plt.plot(moving_average(training_rewards))', ""plt.xlabel('episode'); plt.ylabel('reward')"", 'plt.xlim((0, len(training_rewards)))', 'plt.legend(loc=4); plt.grid()', 'plt.subplot(212)', ""plt.plot(range(1, len(losses)+1), losses, label='loss')"", 'plt.plot(moving_average(losses))', ""plt.xlabel('episode'); plt.ylabel('loss')"", 'plt.xlim((0, len(losses)))', 'plt.legend(loc=4); plt.grid()', 'plt.tight_layout(); plt.show()']'

 'markdown' cell: '[""Now let's review the solution!""]'

 'code' cell: '['env = Recorder(env, ""./gym-results"") # wrappers.Monitor(env, ""./gym-results"", force=True) # Create wrapper to display environment', 's = env.reset()', '', 'for _ in range(500):', '    a = policy(torch.from_numpy(np.atleast_2d(s)).float()).argmax().item()', '    s, r, done, _ = env.step(a)', '    if done: break', '    ', '# env.close()', 'env.play()']'",0.5010520219802856,notebook 8_5_Deep,11.0,"'code' cell: '['# plot results', 'def moving_average(a, n=10) :', '    ret = np.cumsum(a, dtype=float)', '    ret[n:] = ret[n:] - ret[:-n]', '    return ret / n', '', 'plt.figure(figsize=(16, 9))', 'plt.subplot(411)', ""plt.title('training rewards')"", 'plt.plot(range(1, num_episodes+1), rewards)', 'plt.plot(moving_average(rewards))', 'plt.xlim([0, num_episodes])', 'plt.subplot(412)', ""plt.title('training lengths')"", 'plt.plot(range(1, num_episodes+1), lengths)', 'plt.plot(range(1, num_episodes+1), moving_average(lengths))', 'plt.xlim([0, num_episodes])', 'plt.subplot(413)', ""plt.title('training loss')"", 'plt.plot(range(1, num_episodes+1), losses)', 'plt.plot(range(1, num_episodes+1), moving_average(losses))', 'plt.xlim([0, num_episodes])', 'plt.subplot(414)', ""plt.title('epsilon')"", 'plt.plot(range(1, num_episodes+1), epsilons)', 'plt.xlim([0, num_episodes])', 'plt.tight_layout(); plt.show()']'",0.5126402378082275,notebook 8_4_Q,7.0,"'code' cell: '['# plot results', '', 'def moving_average(a, n=10) :', '    ret = np.cumsum(a, dtype=float)', '    ret[n:] = ret[n:] - ret[:-n]', '    return ret / n', '', 'plt.figure(figsize=(16, 9))', 'plt.subplot(411)', ""plt.title('training rewards')"", 'plt.plot(range(1, num_episodes+1), rewards)', 'plt.plot(range(1, num_episodes+1), moving_average(rewards))', 'plt.xlim([0, num_episodes])', 'plt.subplot(412)', ""plt.title('training lengths')"", 'plt.plot(range(1, num_episodes+1), lengths)', 'plt.plot(range(1, num_episodes+1), moving_average(lengths))', 'plt.xlim([0, num_episodes])', 'plt.subplot(413)', ""plt.title('training loss')"", 'plt.plot(range(1, num_episodes+1), losses)', 'plt.plot(range(1, num_episodes+1), moving_average(losses))', 'plt.xlim([0, num_episodes])', 'plt.subplot(414)', ""plt.title('epsilon')"", 'plt.plot(range(1, num_episodes+1), epsilons)', 'plt.xlim([0, num_episodes])', 'plt.tight_layout(); plt.show()']'

 'markdown' cell: '[""Now let's review the solution! You can run the cell multiple times to see the behavior of the Q-network.""]'",0.5170090198516846,notebook 4_3,9.0,"'code' cell: '['num_epochs = 10', 'validation_every_steps = 50', '', 'step = 0', 'model.train()', '', 'train_accuracies = []', 'valid_accuracies = []', '        ', 'for epoch in range(num_epochs):', '    ', '    train_accuracies_batches = []', '    ', '    for inputs, targets in train_loader:', '        inputs, targets = inputs.to(device), targets.to(device)', '        ', '        # Forward pass.', '        output = model(inputs)', '        ', '        # Compute loss.', '        loss = loss_fn(output, targets)', '        ', '        # Clean up gradients from the model.', '        optimizer.zero_grad()', '        ', '        # Compute gradients based on the loss from the current batch (backpropagation).', '        loss.backward()', '        ', '        # Take one optimizer step using the gradients computed in the previous step.', '        optimizer.step()', '        ', '        step += 1', '        ', '",0.524674654006958,notebook 4_2,6.0,"'code' cell: '['batch_size = 64', 'num_epochs = 2', 'validation_every_steps = 500', '', 'step = 0', 'model.train()', '', 'train_accuracies = []', 'valid_accuracies = []', '        ', 'for epoch in range(num_epochs):', '    ', '    train_accuracies_batches = []', '    ', '    for inputs, targets in train_loader:', '        inputs, targets = inputs.to(device), targets.to(device)', '        ', '        # Forward pass, compute gradients, perform one training step.', '        # Your code here!', '        ', '        # Increment step counter', '        step += 1', '        ', '        # Compute accuracy.', '        predictions = output.max(1)[1]', '        train_accuracies_batches.append(accuracy(targets, predictions))', '        ', '        if step % validation_every_steps == 0:', '            ', '            # Append average training accuracy to list.', '            train_accuracies.append(np.mean(train_accuracies_batches))',",0.5497161149978638,notebook 4_3,15.0,"'code' cell: '['def initialize_model(model_name: str, *, num_classes: int, finetune_entire_model: bool = False):', '    """"""Returns a pretrained model with a new last layer, and a dict with additional info.', '', '    The dict now contains the number of model parameters, computed as the number of', '    trainable parameters as soon as the model is loaded.', '    """"""', '', '    print(', '        f""Loading model \'{model_name}\', with ""', '        f""finetune_entire_model={finetune_entire_model}, changing the ""', '        f""last layer to output {num_classes} logits.""', '    )', '    model = timm.create_model(', '        model_name, pretrained=True, num_classes=num_classes', '    )', '', '    num_model_parameters = 0', '    for p in model.parameters():', '        if p.requires_grad:', '            num_model_parameters += p.numel()', '', '    if not finetune_entire_model:', '        for name, param in model.named_parameters():', '            param.requires_grad = False', '', '        # Layer names are not consistent, so we have to consider a few cases. This might ', '",0.5520261526107788,notebook 7_2,35.0,"'code' cell: '['def reduce(x:Tensor) -> Tensor:', '    """"""for each datapoint: sum over all dimensions""""""', '    return x.view(x.size(0), -1).sum(dim=1)', '', 'class VariationalInference(nn.Module):', '    def __init__(self, beta:float=1.):', '        super().__init__()', '        self.beta = beta', '        ', '    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:', '        ', '        # forward pass through the model', '        outputs = model(x)', '        ', '        # unpack outputs', '        px, pz, qz, z = [outputs[k] for k in [""px"", ""pz"", ""qz"", ""z""]]', '        ', '        # evaluate log probabilities', '        log_px = reduce(px.log_prob(x))', '        log_pz = reduce(pz.log_prob(z))', '        log_qz = reduce(qz.log_prob(z))', '        ', '        # compute the ELBO with and without the beta parameter: ', '",0.5599562525749207,LearningObjectives.txt,0.0,"Demonstrate knowledge of machine learning terminology such as likelihood function, maximum likelihood, Bayesian inference, feed-forward, convolutional and Transformer neural networks, and error back propagation.

Understand and explain the choices and limitations of a model for a given setting.

Apply and analyze results from deep learning models in exercises and own project work.

Plan, delimit and carry out an applied or methods-oriented project in collaboration with fellow students and project supervisor.

Assess and summarize the project results in relation to aims, methods and available data.

Carry out the project and interpret results by use of computational framework for GPU programming such as PyTorch.

Structure and write a final short technical report including problem formulation, description of methods, experiments, evaluation and conclusion.

Organize and present project results at the final project presentation and in report.

Read, evaluate and give feedback to work of other students.",0.5614992380142212,notebook 5_1,77.0,"'markdown' cell: '['**Exercise 9**: Play with the temperature parameter, what is it controlling? how can you related it to the definition of a language model?', '', '> *Insert your your answer here*']'

 'markdown' cell: '['### V.b. Prompt-based learning', '', 'Prompt-based learning consists in framing a problem into a *prompt* which completion by a language model corresponds to the answer. In other words, it consists in using natural language to interface with language models.', '', '**Experiment** Do language models know what hygge is?', '', '**Exercise 10**: Write a prompt that triggers a language model to define ""hygge"".', '', '> *write your prompt here and test it using GPT-2 and GPT-3*']'

 'code' cell: '['prompt = ""Q: What is Hygge? A:""', ""input_ids = tokenizer.encode(prompt, return_tensors='pt')"", 'input_ids = input_ids.repeat(5, 1)', 'output = model.generate(input_ids, do_sample=True, temperature=1, max_length=50)', 'decoded = tokenizer.batch_decode(output)', 'for txt in decoded:', '    txt = txt.replace(""\\n"", """")', '    print(txt)']'

 'markdown' cell: '['**Exercise 11**: Test your prompt [with GPT-3](https://beta.openai.com/playground)', '', '> *Insert your prompt and the GPT-3 completion here*', '']'",0.5617420673370361,notebook 8_4_Q,6.0,"# 6. bookkeeping', '            s = s1', '            ep_reward += r', '            ep_loss += loss.item()', '            if done: break', '        # bookkeeping', '        epsilon *= num_episodes/(i/(num_episodes/20)+num_episodes) # decrease epsilon', '        epsilons.append(epsilon); rewards.append(ep_reward); lengths.append(j+1); losses.append(ep_loss)', ""        if (i+1) % val_freq == 0: print('{:5d} mean training reward: {:5.2f}'.format(i+1, np.mean(rewards[-val_freq:])))"", ""    print('done')"", 'except KeyboardInterrupt:', ""    print('interrupt')""]'",0.5643302202224731,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
"What is the initial assignment in exercise notebook 3.4-EXE-FFN-MNIST.ipynb, and what are the steps to execute it?","The first task is to use Kaiming He initialization instead of Xavier Glorot.
In order to implement this task:
We replace the Xavier initialization for self.W_1 and self.W_2 with init.kaiming_normal_, which initializes the weight matrices using the Kaiming He initialization method suitable for ReLU activation.
Code:
<code>",notebook 3_4,CoursePlan.txt,10.0,"Install software on your laptop or go directly to Google CoLab (see above). Installation guide for laptop and cloud may be found here.
    Carry out computer exercises week 1. It is encouraged to work together with other students. Type in everything yourself. Code answers are fine not to differ much within the group and text answers should be in your own words. Note that the computer exercises may experience minor change up to 3 days before the actual session. The hand-in is the notebook with your modifications. It is only allowed to hand in .ipynb files. Each week you should only hand in one file. It is the file with EXE in its name. You hand in on peergrade.io. In order to be able to hand in on peergrade you can use this invitation link https://app.peergrade.io/join/JFFJBF or login into peergrade and use the invitation code JFFJBF. There you will receive information on handing in exercises and deadlines for activities. Some students have previously by accident signed up twice with different emails or forgotten the “student” in their DTU student mail address. If you then submit and check with different email addresses it will look as though you have not handed in.
    Peergrade exercise from three other students through peergrade.io.",0.5273053646087646,notebook 7_2,40.0,"'code' cell: '['num_epochs = 100', '', 'device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")', 'print(f"">> Using device: {device}"")', '', '# move the model to the device', 'vae = vae.to(device)', '', '# training..', 'while epoch < num_epochs:', '    epoch+= 1', '    training_epoch_data = defaultdict(list)', '    vae.train()', '    ', '    # Go through each batch in the training dataset using the loader', '    # Note that y is not necessarily known as it is here', '    for x, y in train_loader:', '        x = x.to(device)', '        ', '        # perform a forward pass through the model and compute the ELBO', '        loss, diagnostics, outputs = vi(vae, x)', '        ', '        optimizer.zero_grad()', '        loss.backward()', '        optimizer.step()', '        ', '        # gather data for the current bach', '        for k, v in diagnostics.items():', '            training_epoch_data[k] += [v.mean().item()]', '            ', '', '    # gather",0.5771710872650146,notebook 5_2,53.0,"'code' cell: '['# Insert code here', '', 'class LSTMLayer:', '    def __init__(self, n_in: int, n_hid: int, act_fn, initializer = NormalInitializer(), initializer_hid = NormalInitializer()):', '        self.n_in = n_in', '        self.n_hid = n_hid', '        self.g_layer = ', '        self.i_layer = ', '        self.f_layer = ', '        self.o_layer = ', '        self.initial_hid = [Var(0.0) for _ in range(n_hid)]', '        self.stored_hid = [Var(0.0) for _ in range(n_hid)]', '        self.initial_c = [Var(0.0) for _ in range(n_hid)]', '        self.stored_c = [Var(0.0) for _ in range(n_hid)]', '        self.act_fn = act_fn', '    ', '    def __repr__(self):    ', ""        return 'Feed-forward: ' + repr(self.in_hid_layer) + ' Candidate: ' + repr(self.g_layer) + ' i gate ' + repr(self.i_layer) + ' f gate ' + repr(self.f_layer) + ' o gate ' + repr(self.o_layer) + ' Initial",0.5863522291183472,notebook 5_1,39.0,"'code' cell: '['# sample the RNN language model', 'with torch.no_grad():', '    sample = rnn.sample(num_steps=10, batch_size=10, temperature=0.5, prevent_repetitions=True)', '    rich.print(glove_tokenizer.decode_batch(sample.detach().cpu().numpy()))']'

 'code' cell: '['_ = rnn.cpu()', '# free-up memory if needed: delete the RNN model', '# del rnn']'

 'markdown' cell: '['**Exercise 3**: What would happen if the inputs of the RNN were not shifted to the right (in sample in the RNNLM class)?  ', '', '> *Insert your answer here.*']'",0.5960916876792908,notebook 3_4,2.0,"'markdown' cell: '['## MNIST', ""First let's load the MNIST dataset and plot a few examples:""]'

 'code' cell: '['mnist_trainset = MNIST(""./temp/"", train=True, download=True)', 'mnist_testset = MNIST(""./temp/"", train=False, download=True)']'

 'code' cell: '[""# To speed up training we'll only work on a subset of the data"", 'x_train = mnist_trainset.data[:1000].view(-1, 784).float()', 'targets_train = mnist_trainset.targets[:1000]', '', 'x_valid = mnist_trainset.data[1000:1500].view(-1, 784).float()', 'targets_valid = mnist_trainset.targets[1000:1500]', '', 'x_test = mnist_testset.data[:500].view(-1, 784).float()', 'targets_test = mnist_testset.targets[:500]', '', 'print(""Information on dataset"")', 'print(""x_train"", x_train.shape)', 'print(""targets_train"", targets_train.shape)', 'print(""x_valid"", x_valid.shape)', 'print(""targets_valid"", targets_valid.shape)', 'print(""x_test"", x_test.shape)', 'print(""targets_test"", targets_test.shape)']'",0.5966176986694336,notebook 4_1,7.0,"'code' cell: '['# Plot a few MNIST examples', 'plt.figure(figsize=(7, 7))', 'plt.imshow(make_grid(x_train[:100], nrow=10).permute(1, 2, 0))', ""plt.axis('off')"", 'plt.show()']'

 'markdown' cell: '['# Define a simple feed forward neural network']'",0.6059231758117676,notebook 3_4,3.0,"'code' cell: '['# plot a few MNIST examples', 'idx, dim, classes = 0, 28, 10', '# create empty canvas', 'canvas = np.zeros((dim*classes, classes*dim))', '', '# fill with tensors', 'for i in range(classes):', '    for j in range(classes):', '        canvas[i*dim:(i+1)*dim, j*dim:(j+1)*dim] = x_train[idx].reshape((dim, dim))', '        idx += 1', '', '# visualize matrix of tensors as gray scale image', 'plt.figure(figsize=(4, 4))', ""plt.axis('off')"", ""plt.imshow(canvas, cmap='gray')"", ""plt.title('MNIST handwritten digits')"", 'plt.show()']'",0.6059629917144775,notebook 5_2,34.0,"'markdown' cell: '['Now we can define a network and pass some data through it.']'

 'code' cell: '['NN = [', '    RNNLayer(1, 5, lambda x: x.tanh()),', '    DenseLayer(5, 1, lambda x: x.identity())', ']', '', 'def forward_batch(input: Sequence[Sequence[Sequence[Var]]], network, use_stored_hid=False):', '  ', '  def forward_single_sequence(x, network, use_stored_hid):', '    for layer in network:', '        if isinstance(layer, RNNLayer):', '            x = layer.forward_sequence(x, use_stored_hid) ', '        else:', '            x = layer.forward_sequence(x)', '    return x', '', '  output = [ forward_single_sequence(input[n], network, use_stored_hid) for n in range(len(input))]', '  return output', '', 'print(NN[0])', 'x_train =[', '          [[Var(1.0)], [Var(2.0)], [Var(3.0)]],', '          [[Var(1.0)], [Var(2.0)], [Var(3.0)]]', '          ]', '', 'output_train = forward_batch(x_train, NN)          ', 'output_train[0][0][0].backward()', '', 'print(output_train)']'",0.6076666116714478,notebook 5_3,42.0,"'markdown' cell: '['## Exercise E:']'

 'markdown' cell: '['Complete the training loop above and run the training. You can leave the hyper-parameters and network size unchanged.', '', 'A correct implementation should yield a loss of around **1** (using mean CE) or around **4** (using sum CE) after 1000 epochs. Does it work? If not, try to identify the issue -- perhaps something in the backward pass is not right?']'

 'markdown' cell: '['## Extrapolation']'

 'markdown' cell: '[""Now that we have trained an RNN, it's time to put it to test. We will provide the network with a starting sentence and let it `freestyle` from there!""]'",0.6084491610527039,notebook 8_2_Prerequisites_ipynb,1.0,"'markdown' cell: '['## Installation', '', 'Below is a brief guide on how to install OpenAI Gym. For more details, please refer to the repository on [GitHub](https://github.com/openai/gym) and the [docs](https://gym.openai.com/docs).', '', 'You can do a minimal install of the packaged version of Gym directly from PyPI:', '', '```', 'pip install gym', '```', '', 'Or you can perform a minimal install from GitHub:', '', '```', 'git clone https://github.com/openai/gym.git', 'cd gym', 'pip install -e .', '```', '', 'If you run in Colab, you can do a quick pip install of Gym in the cell below:']'

 'code' cell: '['!pip install gym[classic_control] > /dev/null 2>&1', '!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1', '!pip install colabgymrender > /dev/null 2>&1', '!pip install imageio==2.4.1 > /dev/null 2>&1']'

 'markdown' cell: '['You will also need an installation of `ffmpeg` locally. If you do not have it installed already, you can install it by one of the following commands depending on your system:', '', '```sudo apt-get install ffmpeg``` (Linux)', '', '```conda install -c conda-forge ffmpeg``` (Anaconda)']'",0.6111340522766113,0.0,0.0,0.0,0.0,1.0,1.0,2.0,2.0,2.0,2.0
What do we expect to learn from week4?,"In this lab, we will learn how to create your own convolutional classifier for different datasets, and the technologies to improve the performance of your convolutional classifier. ","notebook 4_1, notebook 4_2",CoursePlan.txt,12.0,"and take notes for at least 3 questions to ask. Link to lecture slides is here and here for 2017 updates.

    Reading material Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapter 6 (stop when reaching section called Other approaches to deep neural nets).
    Alternative textbook chapter in the deep learning book.
    One exercise from the book chapters.
    Carry out computer exercises week 4.
    Hand in the notebook marked with EXE on peergrade.io.
    Peergrade exercise from three other students through peergrade.io. You will receive instructions about this from peergrade.io.

Week 5 - Transformers and recurrent neural networks

    Watch week 3 video lectures",0.6459096670150757,CoursePlan.txt,17.0,"and take notes for at least 3 questions to ask. Link to lecture slides 2016 slides and 2017 slides and 2020 slides.

    Reading material DL Chapter 14 and 20.10.3. (Further learning a course dedicated to generative modelling.)
    One exercise from the book chapters.
    Carry out computer exercises week 7 on autoencoder un- and semi-supervised. Hand in and peergrade on peergrade.io like in previous weeks.
    Project selection deadline is this week (see above).

Week 8 - Reinforcement learning 

    Watch week 6 video lectures 

    02456week6 1 1 reinforcement learning
    02456week6 1 2 reinforcement learning approaches
    02456week6 2 1 AlphaGo policy and value networks
    02456week6 2 2 AlphaGo steps 1 to 4
    02456week6 3 policy gradients
    02456week6 4 a few last words
    2017 Deep Q learning
    2017 Evolutionary strategies

and take notes for at least 3 questions to ask. Link to lectures here and here for 2017 update.

    Reading: another nice blog post by Andrei Karpathy. Optional reading material on the connection between variational and reinforcement learning.
    One exercise from the book chapters. 
    Computer exercises on reinforcement learning methods (policy gradient, deep Q learning, evolutionary strategies) in the openAI Gym. Carry out exercises week 8. Hand in and peergrade on peergrade.io like in previous weeks.
    Project work.",0.6697936058044434,notebook 5_1,0.0,"'markdown' cell: '['# Week 5 - Deep learning with Transformers']'

 'markdown' cell: '['Some preliminary set-up.']'",0.6752393245697021,CoursePlan.txt,5.0,"Week 1 computer exercise. Deadline: Monday week 2.
    Week 2 computer exercise. Deadline: Monday week 3.
    Week 3 computer exercise and 1 exercise of your own choice from course material week 1. Deadline: Monday week 4
    Week 4 computer exercise  and 1 exercise of your own choice from course material week 1-2. Deadline: Monday week 5.
    Week 5 computer exercise. Deadline: Monday week 6.
    Week 6 computer exercise. Deadline: Monday week 7.
    Week 7 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 8
    Week 8 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 9.
    Project selection. Deadline Friday, Oct 20th 2023 at 23.59.
    Link to 2023 project selection sheet
    Project synopsis. Deadline: Monday week 9 at 23:59. The synopsis should be approximately half a page and maximum one page with a project title, motivation, background, milestones and references. It is important that the plan is realistic. The main purposes of the synopsis are to make sure the project size is well-calibrated and is concrete enough to start working from day one. The synopsis will not be used in the evaluation. The synopsis should be sent to your project supervisor.",0.6975386142730713,CoursePlan.txt,15.0,"Week 6 - Tricks of the trade and data science challenge

    Watch week 4 video lectures 

    02456week4 1 1 Initialization and gradient clipping 
    02456week4 1 2 batch normalization
    02456week4 2 1 regularization
    02456week4 2 2 regularization methods
    02456week4 2 3 data augmentation
    02456week4 2 4 ensemble methods and dropout
    02456week4 3 recap
    2017 37 reasons your nn working (part 1 of 2) Walk through of the 37 reasons why your neural network is not working blog post.
    2017 37 reasons you not working (part 2 of 2)
    2020 Recipe to training neural networks - become one with data (part 1 of 3).
    2020 Recipe to training neural networks - baselines (part 2 of 3).
    2020 Recipe to training neural networks - overfit, tune and tune some more (part 3 of 3).

and take notes for at least 3 questions to ask. Link to lecture slides 2016 lecture slides, 2017 blog post and 2020 lecture slides.",0.7144384384155273,CoursePlan.txt,16.0,"and take notes for at least 3 questions to ask. Link to lecture slides 2016 lecture slides, 2017 blog post and 2020 lecture slides.  

    Reading material Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapter 3 from section Overfitting and regularization and Chapter 5.
    Alternative textbook chapters on regularization, optimization, deep learning practice and applications from the deep learning book.  
    Additional material: Andrei Karpathy blogpost on how to approach a data science problem with deep learning, blogpost on things that can go wrong in neural network training and interactive initialization demo.
    Computer exercises week 6 using PyTorch on the Kaggle competition leaf classification. Hand in and peergrade on peergrade.io like in previous weeks.

Week 7 - Un- and semi-supervised learning

    Watch week 5 video lectures

    02456week5 1 1 unsupervised learning
    02456week5 1 2 unsupervised learning latent variables
    02456week5 2 1 autoencoders
    02456week5 2 2 autoencoders layerwise pretraining
    02456week5 3 1 variational autoencoders
    02456week5 3 2 semi-supervised variational autoencoders 
    2017 Generative adversarial networks
    2020 Flows
    2020 Self-supervised learning
    2020 Self-training/noisy student
    2020 Distribution Augmentation
    2020 Flat minima",0.7169805765151978,CoursePlan.txt,9.0,"During this week and the following two weeks read Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapters 1-3 (stop when reaching the section called Overfitting and regularization) and browse Chapter 4. Note that this is reading material for the first three weeks of the course. Also, in total six exercises of your own choice will be homework later in the course.
    Alternative textbooks: All topics are also covered in the deep learning book that may be read as a supplement. The book can also be bought from the DTU bookstore. You will get 10% discount with this link. Feed-forward neural networks are covered in this chapter. Chapter 1 gives an introduction to deep learning and Part II gives the necessary background on linear algebra, probability, numerical computation and machine learning. Alternative textbook 2: Chris Bishop, Pattern recognition and machine learning. If you need to up your game in mathematics, the book Mathematics for machine learning is an excellent resource and the note Mathematics for Machine Learning offers a concise and compressed collection of the mathematical concepts used in deep learning. These resources are freely available online and are very valuable sources of information.
    Install software on your laptop or go directly to Google CoLab (see above). Installation guide for laptop and cloud may be found here.",0.7523404359817505,notebook 5_3,1.0,"we will show you:', '* How to represent sequences of categorical variables', '* How to build and train an RNN in NumPy', '* How to build and train an LSTM network in NumPy', '* How to build and train an LSTM network in PyTorch']'",0.7616044878959656,CoursePlan.txt,8.0,"Detailed content

Links to individual video lectures and lecture slides are given below. Here is a link to all 2016 video lectures as a playlist and a Google doc folder with all the lecture slides. More videos have been added over the years. They are all linked below. A very good alternative video resource is Hugo Larochelle’s YouTube playlist.
Week 1 - Feed-forward neural networks - do it yourself pen and paper

    During this week and the following two weeks watch video lectures: 

    Part 0 Overview
    Part 1 Deep learning
    Part 2.1 Feed-forward neural networks
    Part 2.2 Feed-forward neural networks
    Part 3 Error Backpropagation
    Part 4 Optimization

and take notes for at least 3 questions to ask. Link to lecture slides is here.",0.7644559144973755,notebook 8_3,13.0,"'markdown' cell: '['## Exercises', '', 'Now it is your turn! Make sure you read and understand the code, then play around with it and try to make it learn better and faster.', '', 'Experiment with the:', '', '* number of episodes', '* discount factor', '* learning rate', '* network layers', '', '', '### Exercise 1 ', '', '*Describe any changes you made to the code and why you think they improve the agent. Are you able to get solutions consistently?*', '', '**Answer:**', '', '*Answer here...*', '', '### Exercise 2 ', '', '*Consider the following sequence of rewards produced by an agent interacting with an environment for 10 timesteps:*', '', '[0, 1, 1, 1, 0, 1, 1, 0, 0, 0]', '', '* *What is the total reward?*', '* *What is the total future reward in each timestep?*', '* *What is the discounted future reward in each timestep if $\\gamma = 0.9$?*', '', '*Hint: See introdution notebook.*', '', '**Answer:**', '', '*Answer here...*', '', '### Exercise 3', '', '*In the training output, you will sometimes observe the validation reward starts out lower than the training reward but as training progresses they cross over and the validation reward becomes higher than the training reward. How can you explain this behavior?*', '', '*Hint: Do we use the policy network in the same way during training and validation?*', '', '**Answer:**', '', '*Answer here...*', '', '### Exercise 4', '', '*How does the policy gradient method we have used address the",0.7719081044197083,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
What is CIFAR-10 dataset?,The images in CIFAR-10 are RGB images (3 channels) with size 32x32 (so they have size 3x32x32). There are 10 different classes.,notebook 4_2,notebook 4_2,0.0,"'markdown' cell: '['# CNN on CIFAR-10', '', 'In this notebook you need to put what you have learned into practice, and create your own convolutional classifier for the CIFAR-10 dataset.', '', 'The images in CIFAR-10 are RGB images (3 channels) with size 32x32 (so they have size 3x32x32). There are 10 different classes. See examples below.', '', '![cifar10](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/static_files/cifar10.png?raw=1)']'

 'markdown' cell: '['## Preliminaries']'",0.4697492718696594,notebook 4_2,2.0,"'code' cell: '['# The output of torchvision datasets are PIL images in the range [0, 1]. ', '# We transform them to PyTorch tensors and rescale them to be in the range [-1, 1].', 'transform = transforms.Compose(', '    [transforms.ToTensor(),', '     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # subtract 0.5 and divide by 0.5', '    ]', ')', '', 'batch_size = 64  # both for training and testing', '', '# Load datasets', ""train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)"", ""test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"", 'train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)', 'test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)', '', '# Map from class index to class name.', 'classes = {index: name for name, index in train_set.class_to_idx.items()}']'",0.6181260347366333,notebook 4_3,0.0,"'markdown' cell: '['# Transfer learning on the Caltech101 dataset', '', 'In this notebook, we will consider a more complex dataset than MNIST or CIFAR10. The images in Caltech101 are RGB images (3 channels) with variable size. There are 101 different classes. We will try a very common practice in computer vision nowadays: transfer learning from a pre-trained ImageNet model. ', '', 'Roadmap:', '- Modify the network from the previous exercise (CIFAR-10) to work with 224x224 images.', '- Train the model for a while on Caltech101 and see how far we can get.', '- Take a ResNet34 that was pre-trained on ImageNet-1k and fine-tune it to Caltech101.', '  - Consider both training only the head (the linear classifier at the end of the network) or the entire network.', '  - We should be able to reach better performance than our original network in fewer training steps.', '- Optional: play around with other pre-trained models from `timm` (see info [here](https://github.com/rwightman/pytorch-image-models)).']'

 'markdown' cell: '['## Preliminaries']'",0.6728724241256714,notebook 7_2,40.0,"'code' cell: '['num_epochs = 100', '', 'device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")', 'print(f"">> Using device: {device}"")', '', '# move the model to the device', 'vae = vae.to(device)', '', '# training..', 'while epoch < num_epochs:', '    epoch+= 1', '    training_epoch_data = defaultdict(list)', '    vae.train()', '    ', '    # Go through each batch in the training dataset using the loader', '    # Note that y is not necessarily known as it is here', '    for x, y in train_loader:', '        x = x.to(device)', '        ', '        # perform a forward pass through the model and compute the ELBO', '        loss, diagnostics, outputs = vi(vae, x)', '        ', '        optimizer.zero_grad()', '        loss.backward()', '        optimizer.step()', '        ', '        # gather data for the current bach', '        for k, v in diagnostics.items():', '            training_epoch_data[k] += [v.mean().item()]', '            ', '', '    # gather",0.6743332743644714,notebook 7_1,6.0,"'code' cell: '['import torch', 'cuda = torch.cuda.is_available()', '', 'from torch.utils.data import DataLoader', 'from torch.utils.data.sampler import SubsetRandomSampler', 'from torchvision.datasets import MNIST', 'from torchvision.transforms import ToTensor', '', '# Flatten the 2d-array image into a vector', 'flatten = lambda x: ToTensor()(x).view(28**2)', '', '# Define the train and test sets', 'dset_train = MNIST(""./"", train=True,  transform=flatten, download=True)', 'dset_test  = MNIST(""./"", train=False, transform=flatten)', '', '# The digit classes to use', 'classes = [3, 7]', '', 'def stratified_sampler(labels, classes):', '    """"""Sampler that only picks datapoints corresponding to the specified classes""""""', '    from functools import reduce', '    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))', '    indices = torch.from_numpy(indices)', '    return SubsetRandomSampler(indices)', '', '', '# The loaders perform the actual work', 'batch_size = 64', 'train_loader = DataLoader(dset_train, batch_size=batch_size,', '                          sampler=stratified_sampler(dset_train.targets, classes), pin_memory=cuda)', 'test_loader  = DataLoader(dset_test, batch_size=batch_size, ', '                          sampler=stratified_sampler(dset_test.targets, classes), pin_memory=cuda)']'",0.6771976947784424,notebook 3_4,12.0,"### Evaluate validation', '    val_preds, val_targs = [], []', '    for i in range(num_batches_valid):', '        slce = get_slice(i, batch_size)', '        ', '        output = net(x_valid[slce])', '        preds = torch.max(output, 1)[1]', '        val_targs += list(targets_valid[slce].numpy())', '        val_preds += list(preds.data.numpy())', '        ', '', '    train_acc_cur = accuracy_score(train_targs, train_preds)', '    valid_acc_cur = accuracy_score(val_targs, val_preds)', '    ', '    train_acc.append(train_acc_cur)', '    valid_acc.append(valid_acc_cur)', '    ', '    if epoch % 10 == 0:', '        print(""Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f"" % (', '                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))', '', 'epoch = np.arange(len(train_acc))', 'plt.figure()', ""plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')"", ""plt.legend(['Train Accucary','Validation Accuracy'])"", ""plt.xlabel('Updates'),",0.6864781379699707,notebook 7_2,41.0,"in diagnostics.items():', '            training_epoch_data[k] += [v.mean().item()]', '            ', '', '    # gather data for the full epoch', '    for k, v in training_epoch_data.items():', '        training_data[k] += [np.mean(training_epoch_data[k])]', '', '    # Evaluate on a single batch, do not propagate gradients', '    with torch.no_grad():', '        vae.eval()', '        ', '        # Just load a single batch from the test loader', '        x, y = next(iter(test_loader))', '        x = x.to(device)', '        ', '        # perform a forward pass through the model and compute the ELBO', '        loss, diagnostics, outputs = vi(vae, x)', '        ', '        # gather data for the validation step', '        for k, v in diagnostics.items():', '            validation_data[k] += [v.mean().item()]', '    ', '    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples', '",0.696873664855957,notebook 7_4,2.0,"'code' cell: '['import torch', 'cuda = torch.cuda.is_available()', 'device = ""cuda:0"" if cuda else ""cpu""', '', 'from torch.distributions import Normal', '', '_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)', 'n_samples = 5000', '', '# Define the base distribution p(z)', 'base = Normal(torch.zeros(2), torch.ones(2))', 'z = base.sample((n_samples,))', '', 'ax1.scatter(*z.t(), s=3)', 'ax1.set_title(r""Samples $z \\sim N(0, I)$"")', 'ax1.grid(True)', '', '# Naïvely transform samples', 'f = lambda z: (z/3)**3', 'x = f(z)', '', 'ax2.scatter(*x.t(), s=3)', 'ax2.set_title(r""$x = f(z)$"")', 'ax2.grid(True)', '', 'plt.show()']'",0.6979397535324097,notebook 7_3,1.0,"'code' cell: '['import torch', 'cuda = torch.cuda.is_available()', 'device = torch.device(""cuda:0"" if cuda else ""cpu"")', '', 'from torch.utils.data import DataLoader', 'from torch.utils.data.sampler import SubsetRandomSampler', 'from torchvision.datasets import MNIST', 'from torchvision.transforms import ToTensor', 'from functools import reduce', '', '# The digit classes to use, these need to be in order because', '# we are using one-hot representation', 'classes = np.arange(2)', '', 'def one_hot(labels):', '    y = torch.eye(len(classes)) ', '    return y[labels]', '', '# Define the train and test sets', 'dset_train = MNIST(""./"", train=True, download=True, transform=ToTensor(), target_transform=one_hot)', 'dset_test  = MNIST(""./"", train=False, transform=ToTensor(), target_transform=one_hot)', '', 'def stratified_sampler(labels):', '    """"""Sampler that only picks datapoints corresponding to the specified classes""""""', '    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))', '    indices = torch.from_numpy(indices)', '    return SubsetRandomSampler(indices)', '', '', 'batch_size = 64', '# The loaders perform the actual work', 'train_loader = DataLoader(dset_train, batch_size=batch_size,', '                          sampler=stratified_sampler(dset_train.train_labels),",0.7005460262298584,notebook 7_2,26.0,"'code' cell: '['from torch.utils.data import DataLoader', 'from torch.utils.data.sampler import SubsetRandomSampler', 'from torchvision.datasets import MNIST', 'from torchvision.transforms import ToTensor', 'from functools import reduce', '', '# Flatten the images into a vector', 'flatten = lambda x: ToTensor()(x).view(28**2)', '', '# Define the train and test sets', 'dset_train = MNIST(""./"", train=True,  transform=flatten, download=True)', 'dset_test  = MNIST(""./"", train=False, transform=flatten)', '', '# The digit classes to use', 'classes = [3, 7]', '', 'def stratified_sampler(labels):', '    """"""Sampler that only picks datapoints corresponding to the specified classes""""""', '    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))', '    indices = torch.from_numpy(indices)', '    return SubsetRandomSampler(indices)', '', '', 'batch_size = 64', 'eval_batch_size = 100', '# The loaders perform the actual work', 'train_loader = DataLoader(dset_train, batch_size=batch_size,', '                          sampler=stratified_sampler(dset_train.train_labels))', 'test_loader  = DataLoader(dset_test, batch_size=eval_batch_size, ', '                          sampler=stratified_sampler(dset_test.test_labels))']'",0.7064639329910278,1.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2.0
What are convolutional neural networks?,"The standard ConvNets are organised into layers. Each layer is parameterized by weights and biases. Each layer has an element-wise activation function, and there are no cycles in the connections. In ConvNets, each unit is only connected to a small subset of the input units, which is called the receptive field of the unit. ",notebook 4_1,notebook 4_1,0.0,"'markdown' cell: '['# Convolutional neural networks 101', '', 'Convolution neural networks are one of the most successful types of neural networks for image recognition and an integral part of reigniting the interest in neural networks. They are able to extract structural relations in the data, such as spatial in images or temporal in time series.', '', 'In this lab, we will experiment with inserting 2D-convolution layers in the fully connected neural networks introduced previously. We will also try to visualize the learned convolution filters and try to understand what kind of features they learn to recognize.', '', ""If you have not watched Jason Yosinski's [video on visualizing convolutional networks](https://www.youtube.com/watch?v=AgkfIQ4IGaM), you definitely should do so now. If you are unfamiliar with the convolution operation, [Vincent Dumoulin](https://github.com/vdumoulin/conv_arithmetic) has a nice visualization of different convolution variants. For a more in-depth tutorial, please see http://cs231n.github.io/convolutional-networks/ or http://neuralnetworksanddeeplearning.com/chap6.html.""]'",0.38064730167388916,CourseOutline.txt,0.0,"The purpose of this course is to give the student a detailed understanding of the deep artificial neural network models, their training, computational frameworks for deployment on fast graphical processing units, their limitations and how to formulate learning in a diverse range of settings. These settings include classification, regression, sequences and other types of structured input and outputs and for reasoning in complex environments.

The course outline is:
1. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part I do it yourself on pen and paper.
2. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part II do it yourself in NumPy.
3. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part III PyTorch.
4. Convolutional neural networks (CNN) + presentation of student projects.
5. Sequence modelling for text data with Transformers.
6. Tricks of the trade and data science with PyTorch + Start of student projects.
7. Variational learning and generative adversarial networks for unsupervised and semi-supervised learning.
8. Reinforcement learning - policy gradient and deep Q-learning.

Starting from week 6 and full time from week 9 and the rest of the term will be spent on tutored project work.",0.4196852147579193,notebook 4_1,1.0,"'markdown' cell: '['## Reminder: what are convolutional networks?', '', 'Standard ConvNets are, in many respects, very similar to the dense feedforward networks we saw previously:', ' * The network is still organized into layers.', ' * Each layer is parameterized by weights and biases.', ' * Each layer has an element-wise non-linear transformation (activation function).', ' * There are no cycles in the connections (more on this in later labs).', '', '*So what is the difference?*', 'The networks we saw previously are called *dense* because each unit receives input from all the units in the previous layer. This is not the case for ConvNets. In ConvNets each unit is only connected to a small subset of the input units. This is called the *receptive field* of the unit.', '', '#### Example', 'The input (green matrix) is a tensor of size `1x5x5` -- i.e. it has one ""channel"" (like a grayscale image), and the feature map has size `5x5`. Let us define a `1x3x3` kernel (yellow submatrix). The kernel weights are indicated in red at the bottom right of each element. The computation can be thought of as an elementwise multiplication followed by a sum. Here we use a *stride* of 1, as shown in this animation:', '', '<img src=""https://raw.githubusercontent.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/master/4_Convolutional/images/convolutions.gif"" style=""width: 400px;""/>', '', 'GIF courtesy of",0.489956259727478,notebook 4_1,15.0,"'markdown' cell: '['### Assignment 2', '', '1. Note the performance of the standard feedforward neural network. Add a [2D convolution layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) before the first layer. Insert the utility module `PrintSize` to check the size of the tensor at any point in `Sequential`, and notice that the size of the image reduces after the convolution. This can cause loss of information, and can be avoided by using adequate padding in the convolutional layer.', '  Does adding a convolutional layer increase the generalization performance of the network (try num_filters=32 and filter_size=5 as a starting point)?', '  ', '2. Can the performance be increases even further by stacking more convolution layers?', '', '3. We now have a deeper network than the initial simple feedforward network. What happens if we replace all convolutional layers with linear layers? Is this deep feedforward network performing as well as the convolutional one?', ' ', '4. Max-pooling is a technique for decreasing the spatial resolution of an image while retaining the important features. Effectively this gives a local translational invariance and reduces the computation by a factor of four. In the classification algorithm which is usually desirable. You can either: ', ' ', '   - add a maxpool layer (see the PyTorch docs, and try with kernel_size=2 and stride=2) after the convolution layer, or', '   - add stride=2 to the arguments of the convolution layer directly.', '     ', '  Verify that this decreases the spatial dimension of the image (insert a",0.5728058815002441,notebook 7_1,4.0,"using a neural network $g_{\\phi} : \\mathbf{x} \\rightarrow \\mathbf{z}$.', '2. *Decoding* the representation $\\mathbf{z}$ into a reconstruction $\\hat{\\mathbf{x}} = h_\\theta(\\mathbf{z}) \\in \\mathcal{R}^P$.', '', 'Because $M < P$, the encoding acts as an information bottleneck: only part of the information describing $\\mathbf{x}$ can be encoded into $\\mathbf{z}$ as long as $M$ is sufficiently small.', '', 'Learning the parameters of the autoencoder relies on two aspects:', '', '1. A distance in the observation space $d : \\mathcal{R}^{P} \\times \\mathcal{R}^{P} \\rightarrow \\mathcal{R}$ (e.g. MSE), measuring the reconstruction quality.', '2. Using backpropagation coupled with stochastic gradient descent (SGD) to optimize the parameters $\\{\\phi, \\theta\\}$ w.r.t $L := \\frac{1}{N} \\sum_i d(x_i, h_{\\theta}(g_{\\phi}(\\mathbf{x})))$.', '', '<img src=""static/autoencoder.png"" />', '', '*The exercises are found at the bottom of the notebook*']'",0.5809038877487183,LearningObjectives.txt,0.0,"Demonstrate knowledge of machine learning terminology such as likelihood function, maximum likelihood, Bayesian inference, feed-forward, convolutional and Transformer neural networks, and error back propagation.

Understand and explain the choices and limitations of a model for a given setting.

Apply and analyze results from deep learning models in exercises and own project work.

Plan, delimit and carry out an applied or methods-oriented project in collaboration with fellow students and project supervisor.

Assess and summarize the project results in relation to aims, methods and available data.

Carry out the project and interpret results by use of computational framework for GPU programming such as PyTorch.

Structure and write a final short technical report including problem formulation, description of methods, experiments, evaluation and conclusion.

Organize and present project results at the final project presentation and in report.

Read, evaluate and give feedback to work of other students.",0.5886116623878479,notebook 5_1,67.0,"\\right) = [ \\mathrm{Attention} \\left(\\mathbf{Q}^{1}, \\mathbf{K^{1}}, \\mathbf{V}^{1}, \\mathbf{P} \\right), ', '\\ldots', '\\mathrm{Attention} \\left(\\mathbf{Q}^{P}, \\mathbf{K^{P}}, \\mathbf{V}^{P}, \\mathbf{M} \\right)] \\ ,', '\\end{align}', '$$', 'where each set of vectors $\\mathbf{Q}^{i}, \\mathbf{Q}^{i}, \\mathbf{Q}^{i}$ corresponding to the head index $i$ is obtained using a separate linear transformation of the input sequence.', '', '**Feed-forward** The multi-head attention layer allows looking up multiple positions of the input sequence, but the output is only a linear combination of the value vector $\\mathbf{V}$. A multi-layer neural network (feed-forward layer) is applied **element-wise** to each element of the sequence of hidden states to allow modelling more complex non-linear dependencies.', '', '**Add & Norm** A Transformer is a deep neural network, and therefore might be difficult to optimize. Similarly deep neural networks in the image processing field, Transformer layer rely on two stabilizing components:', '1. [Residual connections](https://arxiv.org/abs/1512.03385) allow to bypass the attention layer as well as the feed-forward layer.', '2. [Layer normalization](https://arxiv.org/abs/1607.06450): allow enforcing that the output of a Transformer layer has values that are properly distributed', '',",0.5982919931411743,notebook 4_1,3.0,"'markdown' cell: '['# Assignment 1', '', '### Assignment 1.1: Manual calculations', '', 'Perform the following computation, and write the result below.', '', '![](https://raw.githubusercontent.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/master/4_Convolutional/images/conv_exe.png)', '', '1. Manually convolve the input, and compute the convolved features. No padding and stride of 1.', ' * **Answer:**', '2. Perform `2x2` max pooling on the convolved features. Stride of 2.', ' * **Answer:**', '', '### Assignment 1.2: Output dimensionality', '', 'Given the following 3D tensor input `(channel, height, width)`, a given amount (`channels_out`) of filters `(channels_in, filter_height, filter_width)`, stride `(height, width)` and padding `(height, width)`, calculate the output dimensionality if it is valid.', '', '1. input tensor with dimensionality (1, 28, 28) and 16 filters of size (1, 5, 5) with stride (1, 1) and padding (0, 0)', ' * **Answer:** ', '2. input tensor with dimensionality (2, 32, 32) and 24 filters of size (2, 3, 3) with stride (1, 1) and padding (0, 0)', ' * **Answer:** ', '3. input tensor with dimensionality (10, 32, 32) and 3 filters of size (10, 2, 2)",0.6033519506454468,CoursePlan.txt,9.0,"During this week and the following two weeks read Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapters 1-3 (stop when reaching the section called Overfitting and regularization) and browse Chapter 4. Note that this is reading material for the first three weeks of the course. Also, in total six exercises of your own choice will be homework later in the course.
    Alternative textbooks: All topics are also covered in the deep learning book that may be read as a supplement. The book can also be bought from the DTU bookstore. You will get 10% discount with this link. Feed-forward neural networks are covered in this chapter. Chapter 1 gives an introduction to deep learning and Part II gives the necessary background on linear algebra, probability, numerical computation and machine learning. Alternative textbook 2: Chris Bishop, Pattern recognition and machine learning. If you need to up your game in mathematics, the book Mathematics for machine learning is an excellent resource and the note Mathematics for Machine Learning offers a concise and compressed collection of the mathematical concepts used in deep learning. These resources are freely available online and are very valuable sources of information.
    Install software on your laptop or go directly to Google CoLab (see above). Installation guide for laptop and cloud may be found here.",0.6045852303504944,notebook 4_2,0.0,"'markdown' cell: '['# CNN on CIFAR-10', '', 'In this notebook you need to put what you have learned into practice, and create your own convolutional classifier for the CIFAR-10 dataset.', '', 'The images in CIFAR-10 are RGB images (3 channels) with size 32x32 (so they have size 3x32x32). There are 10 different classes. See examples below.', '', '![cifar10](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/static_files/cifar10.png?raw=1)']'

 'markdown' cell: '['## Preliminaries']'",0.6108084917068481,1.0,1.0,2.0,3.0,3.0,3.0,3.0,4.0,4.0,4.0
Can you provide some suggestions to improve the model’s performance?,"Tell us something like increase the depth of the network, modify the convolutional layer parameters(number of filters, filter sizes and strides), pooling layers, batch normalization,  change the learning rate, dropout or weight regularization.",notebook 4_2,notebook 4_2,13.0,"'markdown' cell: '['**Assignment 4:** ', '1. Go back and improve performance of the network. By using enough convolutional layers with enough channels (and by training for long enough), you should easily be able to get a test accuracy above 60%, but see how much further you can get it! Can you reach 70%?', '', '2. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.', 'Did anything surprise you during the exercise?', 'What were the changes that seemed to improve performance the most?', '', '3. Write down key lessons/insights you got during this exercise.', '', '**Answer:**']'

 'markdown' cell: '['# Training on GPU', '', '**Optional Assignment:**', 'If you have a GPU, we suggest that you try training your model on GPU. For this, you need to move the model to GPU after defining it, which will recursively go over all modules and convert their parameters and buffers to CUDA tensors. You also need to transfer both the inputs and targets to GPU at each training step, before performing the forward pass.', '', ""The code for this is already in place: notice the `.to(device)` statements. The only thing left to do is change the definition of `device` from `'cpu'` to `'cuda'`."", '', ""If you don't have a GPU, you can do this on [Google Colab](https://research.google.com/colaboratory/)."", '', 'Use the code below to check if any GPU is avaiable in your current setup. This should print the models of all available GPUs.']'",0.5772087574005127,notebook 4_1,16.0,"and stride=2) after the convolution layer, or', '   - add stride=2 to the arguments of the convolution layer directly.', '     ', '  Verify that this decreases the spatial dimension of the image (insert a `PrintSize` module in the `Sequential`). Does this increase the performance of the network? Note that, to increase performance, you may need to stack multiple layers, increase the number of filters, or tune the learning rate.', '', '5. Dropout is a very useful technique for preventing overfitting. Try to add a DropoutLayer after some of the convolution layers. You may observe a higher validation accuracy but lower train accuracy. Can you explain why this might be the case?', ' ', '6. Batch normalization may help convergence in larger networks as well as generalization performance. Try to insert batch normalization layers into the network.']'",0.5868638753890991,notebook 4_3,18.0,"'print(model)', 'print(""Number of model parameters:"", data[""num_model_parameters""])', 'print(""Number of trainable parameters:"", data[""num_trainable_model_parameters""])', '', ""device = torch.device('cuda')  # use cuda or cpu"", 'model = model.to(device)']'",0.6236862540245056,notebook 7_2,43.0,"'markdown' cell: '['# Analyzing the VAE', '', '## Mandatory Exercises', '', '### Exercise 1.', '', '1. Implement the class `ReparameterizedDiagonalGaussian` (`log_prob()` and `rsample()`).', '2. Import the class `Bernoulli`', '3. Implement the class `VariationalInference` (computation of the `elbo` and `beta_elbo`).', '', '### Exercise 2.', '', '**Trainnig and Evaluating a VAE model**', '', '1. Why do we use the reparameterization trick?', '2. What available metric can you use to estimate the marginal likelihood ($p_\\theta(\\mathbf{x})$) ?', '3. In the above plots, we display numerous model samples. If you had to pick one plot, which one would you pick to evaluate the quality of a VAE (i.e. using posterior samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$ or prior samples $\\mathbf{z} \\sim p(\\mathbf{z})$) ? Why?.', '4. How could you exploit the VAE model for classification?', '', '**Answers**:', '', '`[...]`', '', '### Exercise 3.', '', '**Experiment with the VAE model.**', '', '1. Experiment with the number of layers and activation functions in order to improve the reconstructions and latent representation. What solution did you find the best and why?', '2. Try to increase the number of digit classes in the training set and analyze the learning curves,",0.6255226135253906,notebook 4_3,19.0,"'markdown' cell: '['**Assignment 3:** ', '', '1. Train the linear classifier on top of the pre-trained network, and observe how quickly you can get pretty good results, compared to training a smaller network from scratch as above.', '', '2. Go back and change argument to finetune entire network, maybe adjust learning rate, see if you can get better performance than before and if you run into any issues.', '', '3. Optional: experiment with `timm`: try smaller or larger models, including state-of-the-art models, e.g. based on vision transformers (ViT) or MLP-Mixers.', '', '4. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.', 'Did anything surprise you during the exercise?', '', '5. Write down key lessons/insights you got during this exercise.']'

 'code' cell: '[]'",0.6302786469459534,notebook 4_2,14.0,"'code' cell: '['# Check if we have GPUs available', 'print(""Available CUDA devices:"", [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])']'

 'markdown' cell: '['You may not notice any significant speed-up from using a GPU. This is probably because your network is really small. Try increasing the width of your network (number of channels in the convolutional layers) and see if you observe any speed-up on GPU compared to CPU.']'

 'markdown' cell: '[""# Exercise from Michael Nielsen's book"", '', ""**Assignment 5:** Pick an exercise of your own choice from [Michael Nielsen's book](http://neuralnetworksanddeeplearning.com/)."", '', '**Answer:**', '', '', '']'

 'code' cell: '[]'",0.6348700523376465,notebook 3_4,6.0,"appropriate if the targets are binary.', '4. Tanh is similar to the sigmoid, but squashes to [-1, 1]. Rarely used any more.', '4. Softmax normalizes the output to 1, usrful if you have a multi-class classification problem.', '', 'See the plot below.', '', '__Optimizer and learning rate:__', '1. SGD + Momentum: learning rate 0.01 - 0.1 ', '2. ADAM: learning rate 3e-4 - 1e-5', '3. RMSPROP: somewhere between SGD and ADAM']'",0.6376090049743652,notebook 4_3,11.0,"'                for inputs, targets in test_loader:', '                    inputs, targets = inputs.to(device), targets.to(device)', '                    output = model(inputs)', '                    loss = loss_fn(output, targets)', '', '                    predictions = output.max(1)[1]', '', '                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).', '                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))', '', '                model.train()', '                ', '            # Append average validation accuracy to list.', '            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))', '     ', '            print(f""Step {step:<5}   training accuracy: {train_accuracies[-1]}"")', '",0.6377111077308655,notebook 7_1,9.0,"# output layer, projecting back to image size', '            nn.Linear(in_features=hidden_units, out_features=num_features)', '        )', '', '    def forward(self, x): ', '        outputs = {}', ""        # we don't apply an activation to the bottleneck layer"", '        z = self.encoder(x)', '        ', '        # apply sigmoid to output to get pixel intensities between 0 and 1', '        x_hat = torch.sigmoid(self.decoder(z))', '        ', '        return {', ""            'z': z,"", ""            'x_hat': x_hat"", '        }', '', '', '# Choose the shape of the autoencoder', 'net = AutoEncoder(hidden_units=128, latent_features=2)', '', 'if cuda:', '    net = net.cuda()', '', 'print(net)']'",0.643200159072876,notebook 4_3,17.0,"layer = model.head.l', '        elif hasattr(model, ""fc""):', '            if isinstance(model.fc, nn.Linear):', '                layer = model.fc', '        if layer is None:', '            raise ValueError(f""Couldn\'t automatically find last layer of model."")', '        ', '        # Make the last layer trainable.', '        layer.weight.requires_grad_()', '        layer.bias.requires_grad_()', '', '    num_trainable_model_parameters = 0', '    for p in model.parameters():', '        if p.requires_grad:', '            num_trainable_model_parameters += p.numel()', '', '    return model, {', '        ""num_model_parameters"": num_model_parameters,', '        ""num_trainable_model_parameters"": num_trainable_model_parameters,', '    }', '', ""model, data = initialize_model('resnet34d', num_classes=len(np.unique(labels)), finetune_entire_model=False)"", '', 'print(model)', 'print(""Number of model parameters:"", data[""num_model_parameters""])', 'print(""Number of trainable parameters:"", data[""num_trainable_model_parameters""])', '', ""device = torch.device('cuda')  # use cuda or",0.6456092000007629,1.0,1.0,1.0,1.0,1.0,2.0,2.0,2.0,2.0,2.0
What do RNN and LSTM stand for?,RNN stands for Reccurent Neural Network and LSTM stands for Long Short-Term Memory (unit).,notebook 5_1,notebook 5_2,1.0,"How to build and train an RNN in Nanograd', '* How to build and train an LSTM network in Nanograd', '* How to build and train an LSTM network in PyTorch', '', '', '[Numpy version of the Notebook (previous version)](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/5_Recurrent/OLD-5.1-Numpy-Recurrent-Neural-Networks.ipynb)']'",0.5521618127822876,notebook 5_3,75.0,"'code' cell: '['import torch', 'import torch.nn as nn', 'import torch.nn.functional as F', '', 'class MyRecurrentNet(nn.Module):', '    def __init__(self):', '        super(MyRecurrentNet, self).__init__()', '        ', '        # Recurrent layer', '        # YOUR CODE HERE!', '        self.lstm = ', '        ', '        # Output layer', '        self.l_out = nn.Linear(in_features=50,', '                            out_features=vocab_size,', '                            bias=False)', '        ', '    def forward(self, x):', '        # RNN returns output and last hidden state', '        x, (h, c) = self.lstm(x)', '        ', '        # Flatten output for feed-forward layer', '        x = x.view(-1, self.lstm.hidden_size)', '        ', '        # Output layer', '        x = self.l_out(x)', '        ', '        return x', '', 'net = MyRecurrentNet()', 'print(net)']'

 'markdown' cell: '['## Exercise K:', '', 'Define an LSTM for our recurrent neural network `MyRecurrentNet` above. A single LSTM layer is sufficient. What should the input size and hidden size be? Hint: use the PyTorch documentation.']'

 'markdown' cell: '['### Training loop']'",0.5705484747886658,notebook 5_3,46.0,"'markdown' cell: '['## Exercise F:', '', 'How well does your RNN extrapolate -- does it work as expected? Are there any imperfections? If yes, why could that be?']'

 'markdown' cell: '['## Exercise G (optional):']'

 'markdown' cell: '['Alter the forward pass, backward pass and training loop to handle batches of samples. You will see great improvements!']'

 'markdown' cell: '['# Introduction to the Long Short-Term Memory (LSTM) Cell']'

 'markdown' cell: '[""Reading material: [Christopher Olah's walk-through](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."", '', '___', '', '', 'A vanilla RNN suffers from [the vanishing gradients problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem) which gives challenges in saving memory over longer sequences. To combat these issues the gated hidden units were created. The two most prominent gated hidden units are the Long Short-Term Memory (LSTM) cell and the Gated Recurrent Unit (GRU), both of which have shown increased performance in saving and reusing memory in later timesteps. In this exercise, we will focus on LSTM but you would easily be able to go ahead and implement the GRU as well based on the principles that you learn here.', '', 'Below is a figure of the LSTM cell:']'",0.571717381477356,notebook 5_3,74.0,"'markdown' cell: '['## Exercise J:']'

 'markdown' cell: '['Run the training loop above. Compare your LSTM learning curve (NLL and number of epochs) to the vanilla RNN from earlier. Do you observe any improvements? Motivate your answer.']'

 'markdown' cell: '['## PyTorch implementation of the LSTM', '', ""Now that we know how the LSTM cell works, let's see how easy it is to use in PyTorch!""]'

 'markdown' cell: '['Definition of our LSTM network. We define a LSTM layer using the [nn.LSTM](https://pytorch.org/docs/stable/nn.html#lstm) class. The LSTM layer takes as argument the size of the input and the size of the hidden state like in our numpy implementation.']'",0.5724411010742188,notebook 5_1,25.0,"space using a projection matrix $\\mathbf{F} \\in \\mathcal{R}^{V \\times d_h}$. This results in parameterizing the transition distribution as', '$$', 'p_\\theta(\\cdot \\mid \\mathbf{w}_{<t}) = \\mathrm{Softmax}( \\mathbf{h}_t \\mathbf{F}^T)', '$$', 'In the above figure, we showcase how a standard RNN can be applied to implement a left-to-right language model, and annotated the diagramm with the function $h_\\theta(\\mathbf{w}_{t}, \\mathbf{h}_{t-1)})$ and the projection matrix $\\mathbf{F}$.', '', '**Long Short-Term Memory (LSTM) networks** A standard RNN suffers from [the vanishing gradients problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem) which gives challenges in saving memory over longer sequences. To combat these issues the gated hidden units were created. The two most prominent gated hidden units are the [Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber. (1997))](https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735) cell and the Gated Recurrent Unit (GRU), both of which have shown increased performance in saving and reusing memory in later timesteps. RNNs coupled with gated mechanisms are less prone to the problem of vanishing gradients, and can therefore model dependencies over longer number of steps.']'",0.5858341455459595,notebook 5_1,29.0,"'code' cell: '['class RNNLM(torch.nn.Module):', '    """"""A simple implementation of a language model using RNNs.""""""', '    def __init__(self, vectors:torch.Tensor):', '        super().__init__()', '        # register the embeddings', '        self.embeddings = torch.nn.Embedding(*glove_vectors.shape)', '        self.embeddings.weight.data = glove_vectors', '', '        # register the LSTM', '        self.rnn = torch.nn.LSTM(', '            input_size=glove_vectors.shape[1],', '            hidden_size=glove_vectors.shape[1],', '            num_layers=1,', '            batch_first=True,', '        )', '', '        # project the output of the LSTM (hidden state) back to the vocabulary space', '        self.proj = nn.Linear(glove_vectors.shape[1], glove_vectors.shape[0], bias=False)', '        # init the projection using the embeddings weights', '        self.proj.weight.data = glove_vectors', '', '    def forward(self, token_ids: torch.Tensor, retain_ws:bool=False) -> torch.Tensor:', '",0.5897312164306641,notebook 5_2,68.0,"'markdown' cell: '['# It works, now what?']'

 'markdown' cell: '['In this notebook you have learned how to use embeddings, recurrent neural networks, and the LSTM cell in particular.', '', ""As we have already seen, RNNs are excellent for sequential data such as language. But what do we do if we're modelling data with strong dependency in both directions? Like in many things deep learning, we can build powerful models by stacking layers on top of each other; *bi-directional* RNNs consist of two LSTM cells, one for each direction. A sequence is first fed into the forward LSTM cell and the reversed sequence is then used as input to the backward LSTM cell together with the last hidden state from the forward LSTM cell. Follow [this link](https://pdfs.semanticscholar.org/4b80/89bc9b49f84de43acc2eb8900035f7d492b2.pdf) for the original paper from 1997(!)."", '', 'For even deeper representations, multiple layers of both uni-directional and bi-directional RNNs can be stacked ontop of each other, just like feed-forward and convolutional layers. For more information on this, check out the [LSTM PyTorch documentation](https://pytorch.org/docs/stable/nn.html#lstm). Next week we will also explore ways to combine RNNs with other types of layers for even more expressive function approximators.']'",0.5898075103759766,notebook 5_3,82.0,"'markdown' cell: '['# It works, now what?']'

 'markdown' cell: '['In this notebook you have learned how to use embeddings, recurrent neural networks, and the LSTM cell in particular.', '', ""As we have already seen, RNNs are excellent for sequential data such as language. But what do we do if we're modelling data with strong dependency in both directions? Like in many things deep learning, we can build powerful models by stacking layers on top of each other; *bi-directional* RNNs consist of two LSTM cells, one for each direction. A sequence is first fed into the forward LSTM cell and the reversed sequence is then used as input to the backward LSTM cell together with the last hidden state from the forward LSTM cell. Follow [this link](https://pdfs.semanticscholar.org/4b80/89bc9b49f84de43acc2eb8900035f7d492b2.pdf) for the original paper from 1997(!)."", '', 'For even deeper representations, multiple layers of both uni-directional and bi-directional RNNs can be stacked ontop of each other, just like feed-forward and convolutional layers. For more information on this, check out the [LSTM PyTorch documentation](https://pytorch.org/docs/stable/nn.html#lstm). Next week we will also explore ways to combine RNNs with other types of layers for even more expressive function approximators.']'",0.5898075103759766,notebook 5_3,61.0,"'markdown' cell: '['## Exercise I:']'

 'markdown' cell: '[""Complete the implementation of the LSTM forward pass above. Refer to the equations and figures further up if you're in doubt.""]'

 'markdown' cell: '['## Backward pass']'

 'markdown' cell: '['Similar to the RNN in numpy we also need to specify a backward pass. Fortunately, we have already done the work for you here :-)', '', 'Feel free to dive into the code to get a better intuition of what is going on -- otherwise you can jump straight to the training loop.']'",0.592962920665741,notebook 5_3,18.0,"'markdown' cell: '['## Implementing an RNN']'

 'markdown' cell: '['We will implement the forward pass, backward pass, optimization and training loop for an RNN in numpy so that you can get familiar with the recurrent nature of RNNs. Later, we will go back to PyTorch and appreciate how convenient the implementation becomes!']'

 'markdown' cell: '[""Let's first define the necessary model parameters. Recall that an $n \\times m$ weight matrix maps $\\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$.""]'",0.593086302280426,0.0,0.0,0.0,0.0,1.0,2.0,2.0,2.0,2.0,2.0
How can I give text as input to my network?,"Before text can be used as input for a neural network, it needs to be represented as a vector. This can be done by tokenizing the text, and then looking up the embedding vector for each token. Tokenization transforms characters, words, or parts of words into tokens, which can be numbers.",notebook 5_1,notebook 5_2,34.0,"'markdown' cell: '['Now we can define a network and pass some data through it.']'

 'code' cell: '['NN = [', '    RNNLayer(1, 5, lambda x: x.tanh()),', '    DenseLayer(5, 1, lambda x: x.identity())', ']', '', 'def forward_batch(input: Sequence[Sequence[Sequence[Var]]], network, use_stored_hid=False):', '  ', '  def forward_single_sequence(x, network, use_stored_hid):', '    for layer in network:', '        if isinstance(layer, RNNLayer):', '            x = layer.forward_sequence(x, use_stored_hid) ', '        else:', '            x = layer.forward_sequence(x)', '    return x', '', '  output = [ forward_single_sequence(input[n], network, use_stored_hid) for n in range(len(input))]', '  return output', '', 'print(NN[0])', 'x_train =[', '          [[Var(1.0)], [Var(2.0)], [Var(3.0)]],', '          [[Var(1.0)], [Var(2.0)], [Var(3.0)]]', '          ]', '', 'output_train = forward_batch(x_train, NN)          ', 'output_train[0][0][0].backward()', '', 'print(output_train)']'",0.5785624980926514,notebook 5_2,47.0,"'code' cell: '[""def freestyle(NN, sentence='', num_generate=10):"", '    """"""', '    Takes in a sentence as a string and outputs a sequence', '    based on the predictions of the RNN.', '    ', '    Args:', '     `params`: the parameters of the network', '     `sentence`: string with whitespace-separated tokens', '     `num_generate`: the number of tokens to generate', '    """"""', ""    sentence = sentence.split(' ')"", '    output_sentence = sentence', '    sentence_one_hot = one_hot_encode_sequence(sentence, vocab_size)', '', '    # Begin predicting', '    outputs = forward_batch([sentence_one_hot], NN, use_stored_hid=False)', '    output_words = [idx_to_word[np.argmax(output)] for output in Var_to_nparray(outputs[0])]', '    word = output_words[-1]', '', '    # Append first prediction', '    output_sentence.append(word)', '', '    # Forward pass - Insert code here!', ""    if word != 'EOS':"", '      for i in range(num_generate-1):', '          sentence_one_hot = ', '          outputs = ', '          output_words = ', '          word = ', '",0.5903313755989075,notebook 5_3,43.0,"'code' cell: '[""def freestyle(params, sentence='', num_generate=10):"", '    """"""', '    Takes in a sentence as a string and outputs a sequence', '    based on the predictions of the RNN.', '    ', '    Args:', '     `params`: the parameters of the network', '     `sentence`: string with whitespace-separated tokens', '     `num_generate`: the number of tokens to generate', '    """"""', ""    sentence = sentence.split(' ')"", '    ', '    sentence_one_hot = one_hot_encode_sequence(sentence, vocab_size)', '    ', '    # Initialize hidden state as zeros', '    hidden_state = np.zeros((hidden_size, 1))', '', '    # Generate hidden state for sentence', '    outputs, hidden_states = forward_pass(sentence_one_hot, hidden_state, params)', '    ', '    # Output sentence', '    output_sentence = sentence', '    ', '    # Append first prediction', '    word = idx_to_word[np.argmax(outputs[-1])]    ', '    output_sentence.append(word)', '    ', '    # Forward pass', '    for i in range(num_generate):', '', '        # Get the latest prediction and latest hidden state', '        output = outputs[-1]', '        hidden_state =",0.5971213579177856,notebook 4_3,8.0,"'code' cell: '['loss_fn = None  # Your code here!', 'optimizer = None  # Your code here!']'

 'markdown' cell: '['## Train the network']'

 'code' cell: '['# Test the forward pass with dummy data', 'out = model(torch.randn(2, 3, 224, 224).to(device))', 'print(""Output shape:"", out.size())']'",0.6070430874824524,notebook 4_2,9.0,"'markdown' cell: '['## Test the network', '', 'Now we show a batch of test images and generate a table below with the true and predicted class for each of these images.']'

 'code' cell: '['inputs, targets = next(iter(test_loader))', 'inputs, targets = inputs.to(device), targets.to(device)', 'show_image(make_grid(inputs))', 'plt.show()', '', 'outputs = model(inputs)', '_, predicted = torch.max(outputs.data, 1)', '', 'print(""    TRUE        PREDICTED"")', 'print(""-----------------------------"")', 'for target, pred in zip(targets, predicted):', '    print(f""{classes[target.item()]:^13} {classes[pred.item()]:^13}"")']'

 'markdown' cell: '['We now evaluate the network as above, but on the entire test set.']'",0.6424118876457214,notebook 5_1,7.0,"'code' cell: '['# Example sentence with rare English words and non-english words', 'sentence = ""It is jubilating to see how élégant my horse has became""', 'rich.print(f""Input sentence: [bold blue]`{sentence}`"")', '', '# Define multiple tokenizers', 'tokenizer_ids = {', '    ""Word-level"": glove_tokenizer,', '    ""WordPiece"": ""bert-base-cased"",', '    ""BPE"": ""distilgpt2"",', '    ""Character-level"":  ""google/byt5-small"",', '    }', '', '# iterate through the tokenizers and decode the input sentences', 'for tokenizer_name, tokenizer in tokenizer_ids.items():', '    # intialize the tokenizer (either)', '    if isinstance(tokenizer, str):', '        # init a `transformers.PreTrainedTokenizerFast`', '        tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer)', '        vocab_size = tokenizer.vocab_size', '    else:', '        # use the provided `tokenizers.Tokenizer``', '        vocab_size = tokenizer.get_vocab_size()', '', '    # Tokenize', '    token_ids = tokenizer.encode(sentence, add_special_tokens=False)', '    if isinstance(token_ids, tokenizers.Encoding):', '        token_ids = token_ids.ids', '', '    # Report', '",0.6506288051605225,notebook 7_3,16.0,"'            nn.Conv2d(128, 256, kernel_size=4, stride=2),', '            nn.BatchNorm2d(256),', '            nn.LeakyReLU(0.2),', '            Flatten(),', '            nn.Linear(256, 1),', '            nn.Sigmoid()', '        )', '        ', '    def forward(self, x, y):', '        # Merge information and send through network.', '        x = torch.cat([?, ?], dim=1)', '        x = self.model(x)', '        return x', '', 'loss = nn.BCELoss()', 'print(""Using device:"", device)', '', 'generator = ConditionalGenerator().to(device)', 'discriminator = ConditionalDiscriminator().to(device)', '', 'generator_optim = torch.optim.Adam(generator.parameters(), 2e-4, betas=(0.5, 0.999))', 'discriminator_optim = torch.optim.Adam(discriminator.parameters(), 2e-4, betas=(0.5, 0.999))']'",0.6560571789741516,notebook 5_2,46.0,"'code' cell: '['# Get first sentence in test set', 'inputs, targets = test_set[0]', '', '# One-hot encode input and target sequence', 'inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)', 'targets_one_hot = one_hot_encode_sequence(targets, vocab_size)', '', '# Forward pass', 'outputs = forward_batch(encoded_test_set_x[:1], NN)', '', 'output_sentence = [idx_to_word[np.argmax(output)] for output in Var_to_nparray(outputs[0])]', '', ""print('Input sentence:')"", 'print(inputs)', '', ""print('\\nTarget sequence:')"", 'print(targets)', '', ""print('\\nPredicted sequence:')"", 'print([idx_to_word[np.argmax(output)] for output in Var_to_nparray(outputs[0])])']'

 'markdown' cell: '['## Exercise g) Extrapolation', '', ""Now that we have trained an RNN, it's time to put it to test. We will provide the network with a starting sentence and let it `freestyle` from there!"", '', 'How well does your RNN extrapolate -- does it work as expected? Are there any imperfections? If yes, why could that be?']'",0.6571481823921204,notebook 5_3,70.0,"'code' cell: '['# Hyper-parameters', 'num_epochs = 200', '', '# Initialize a new network', 'z_size = hidden_size + vocab_size # Size of concatenated hidden + input vector', 'params = init_lstm(hidden_size=hidden_size, vocab_size=vocab_size, z_size=z_size)', '', '# Initialize hidden state as zeros', 'hidden_state = np.zeros((hidden_size, 1))', '', '# Track loss', 'training_loss, validation_loss = [], []', '', '# For each epoch', 'for i in range(num_epochs):', '    ', '    # Track loss', '    epoch_training_loss = 0', '    epoch_validation_loss = 0', '    ', '    # For each sentence in validation set', '    for inputs, targets in validation_set:', '        ', '        # One-hot encode input and target sequence', '        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)', '        targets_one_hot = one_hot_encode_sequence(targets, vocab_size)', '', '        # Initialize hidden state and cell state as zeros', '        h = np.zeros((hidden_size, 1))', '        c = np.zeros((hidden_size, 1))', '', '        # Forward pass', '        z_s, f_s, i_s, g_s, C_s, o_s,",0.6585369110107422,notebook 4_3,13.0,"'markdown' cell: '['## Test the network on the test data']'

 'code' cell: '['# Evaluate test set', 'with torch.no_grad():', '    model.eval()', '    test_accuracies = []', '    for inputs, targets in test_loader:', '        inputs, targets = inputs.to(device), targets.to(device)', '        output = model(inputs)', '        loss = loss_fn(output, targets)', '', '        predictions = output.max(1)[1]', '', '        # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=True).', '        test_accuracies.append(accuracy(targets, predictions) * len(inputs))', '', '    test_accuracy = np.sum(test_accuracies) / len(test_set)', '    print(f""Test accuracy: {test_accuracy:.3f}"")', '    ', '    model.train()']'",0.6611505746841431,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0
What variables are used in the attention function?,"The attention mechanism is defined using the query Q, the keys K, the values V, and the scaling parameter tau.",notebook 5_1,notebook 5_1,41.0,"', '', 'We define three variables:', '1. The query $\\mathbf{Q} = [\\mathbf{q}_i, \\ldots \\mathbf{q}_{T_\\mathbf{Q}}] \\in \\mathcal{R}^{T_\\mathbf{Q} \\times h_i}$, a sequence of vectors of length $T_\\mathbf{Q}$ and vector dimension $h_i$.', '1. The keys $\\mathbf{K} = [\\mathbf{k}_1, \\ldots \\mathbf{k}_{T_{\\mathbf{K}\\mathbf{V}}}] \\in \\mathcal{R}^{T_{\\mathbf{K}\\mathbf{V}} \\times h_i}$, a sequence of vectors of length $T_{\\mathbf{K}\\mathbf{V}}$ and vector dimension $h_i$.', '1. The values $\\mathbf{V} = [\\mathbf{v}_1, \\ldots \\mathbf{v}_{T_{\\mathbf{K}\\mathbf{V}}}] \\in \\mathcal{R}^{T_{\\mathbf{K}\\mathbf{V}} \\times h_o}$, a sequence of vectors of length $T_{\\mathbf{K}\\mathbf{V}}$ and of another dimension $h_o$, although in general we choose $h_i = h_o$.', '', 'For each query, the attention mechanism returns a convex combinations of the values $\\mathbf{V}$. The attention mechanism is defined as', '$$',",0.4254906475543976,notebook 5_1,52.0,"for i in range(len(queries_labels)):', '            for j in range(len(keys_labels)):', '                text = ax.text(j, i, f""{attention_map[i, j]:.2f}"",', '                            ha=""center"", va=""center"", color=""w"")', '', '    if color_bar:', '      fig.colorbar(im, fraction=0.02, pad=0.04)', '    fig.tight_layout()', '', 'def attention(Q, K, V, tau=None):', '    """"""A simple parallelized attention layer""""""', '    if tau is None:', '        tau = math.sqrt(float(Q.shape[-1]))', '    assert Q.shape[-1] == K.shape[-1]', '    assert K.shape[0] == V.shape[0]', '    attention_map = Q @ K.T / tau', '    attention_weights = attention_map.softmax(dim=1)', '    return torch.einsum(""qk, kh -> qh"", attention_weights, V), attention_weights', '', '# return the output of the attention', 'output, attention_weights = attention(Q, K, V, tau=1.)', '', '# plot the attention weights', 'plot_attention_map(attention_weights, queries_labels, keys_labels, print_values=True)']'",0.456895112991333,notebook 5_1,42.0,"$h_o$, although in general we choose $h_i = h_o$.', '', 'For each query, the attention mechanism returns a convex combinations of the values $\\mathbf{V}$. The attention mechanism is defined as', '$$', '\\mathrm{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\mathrm{Softmax}\\left( \\frac{\\mathbf{Q} \\mathbf{K}^T}{\\tau} \\right) \\mathbf{V} \\ ,', '$$', 'where $\\tau$ is a scaling parameter, set to $\\sqrt{h_i}$ in ([""Attention is All You Need"", Wasrani et al. (2016)](https://arxiv.org/abs/1706.03762)). $\\mathrm{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})$ is a sequence of $T_\\mathbf{Q}$ vectors, each of dimension $h_o$.', '', 'The above expresson is equivalent to applying attention to each query vector $\\mathbf{q}$ separately. The output for each vector $\\mathbf{q}$ depends on the vector of weights $\\mathbf{\\Lambda} = \\mathrm{Softmax}\\left( \\frac{\\mathbf{q} \\mathbf{K}^T}{\\tau} \\right)$ with values $[\\lambda_1, \\ldots \\lambda_{T_{\\mathbf{K}\\mathbf{V}}}]$. The vector of weights $\\Lambda$ is a function of the inner-product",0.464030385017395,notebook 5_1,46.0,"=  \\mathrm{vec(""Query\\,country"")} + \\underbrace{\\mathrm{Attention}(\\mathrm{vec(""Query\\,country"")}, \\mathbf{K}, \\mathbf{V})}_{\\sum_{i=1}^{T_{\\mathbf{K}\\mathbf{V}}} \\mathbf{\\lambda}_i \\mathbf{v}_i}', '$$', '', ""First, let's investigate the attention weights $\\lambda_1, \\ldots, \\lambda_{T_{\\mathbf{K}\\mathbf{V}}}$ for each query $\\mathbf{q}_1, \\ldots, \\mathbf{q}_\\mathbf{Q} $ separately:""]'",0.49596989154815674,notebook 5_1,50.0,"'markdown' cell: '['**Exercise 4**:  In this example, what is the value of $T_{\\mathbf{K}\\mathbf{V}}$, $T_{\\mathbf{Q}}$, $h_i$, $h_o$ and $\\tau$ ?', '', '> * $T_{\\mathbf{K}\\mathbf{V}} = ...$ ', '> * $T_{\\mathbf{Q}} = ...$ ', '> * $h_i = ...$', '> * $h_o = ...$', '> * $\\tau = ...$']'

 'markdown' cell: '[""**Implementing the attention function** We obtained a set of attention weights for each query, concatenating them results in a 2D matrix that will display bellow. Let's implement the `attention` function in the cell bellow using the inputs vectors $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ and visualize the output vector. we use [`torch.einsum`](https://pytorch.org/docs/stable/generated/torch.einsum.html) to implement the sum $\\sum_{i=1}^{T_{\\mathbf{K}\\mathbf{V}}} \\mathbf{\\lambda}_i \\mathbf{v}_i$.""]'",0.5348336696624756,notebook 5_1,69.0,"'code' cell: '['def attention(query, key, value, mask=None, dropout=None):', '    ""Compute \'Scaled Dot Product Attention\'""', '    d_k = query.size(-1)', '    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)', '    if mask is not None:', '        scores = scores.masked_fill(mask == 0, -math.inf)', '    p_attn = F.softmax(scores, dim = -1)', '    if dropout is not None:', '        p_attn = dropout(p_attn)', '    return torch.matmul(p_attn, value), p_attn']'",0.5371274948120117,notebook 5_1,60.0,"'        plot_attention_map(attention_map_masked.log(), tokens, tokens, ax=ax, color_bar=False)', '    ax.set_title(f""Attention map {mask_name}"")', 'plt.tight_layout()', 'plt.show()']'",0.5465043187141418,notebook 5_1,48.0,"= attention_weights[k]', '        cols = [colors(normalize(x, attention_weights).detach().item()) for x in attention_weights_k]', '        ax.bar(keys_labels, attention_weights_k, color=cols)', '        plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"", rotation_mode=""anchor"")', '        ax.set_title(f""Query={query_label}"")', '        if k % ncols == 0 :', '            ax.set_ylabel(""Attention score"")', '        if k >= ncols:', '            ax.set_xlabel(""$\\mathbf{K}$"")', '    fig.tight_layout()', '    plt.show()', '', '# Define keys, queries and values', 'queries_labels = [""Italy"", ""Korea"", ""Nicaragua"", ""Canada"", ""Algeria"", ""India""]', 'keys_labels = [""China"", ""Russia"", ""Turkey"", ""Japan"", ""Thailand"", ""Germany"", ""France"", ""Sweden"", ""Poland"", ""Nigeria"", ""Morocco"", ""Colombia"", ""Chile"", ""USA"", ""Pakistan""]', 'keys_cities_labels = [""Beijing"", ""Moscow"", ""Ankara"", ""Tokyo"", ""Bangkok"", ""Berlin"", ""Paris"", ""Stockholm"", ""Warsaw"", ""Abuja"", ""Rabat"", ""Bogota"", ""Santiago"",",0.5488458275794983,notebook 5_1,58.0,"'code' cell: '['def masked_attention(Q, K, V, tau=None, mask=None):', '    """"""A simple masked attention layer""""""', '    if tau is None:', '        tau = math.sqrt(float(Q.shape[-1]))', '    assert Q.shape[-1] == K.shape[-1]', '    assert K.shape[0] == V.shape[0]', '    attention_map = Q @ K.T / tau', '    if mask is not None:', '        attention_map = mask + attention_map', '    attention_weights = attention_map.softmax(dim=1)', '    return torch.einsum(""qk, kh -> qh"", attention_weights, V), attention_weights', '', '# get a more natural sentence', 'sentence = ""Masked attention allows implementing dependency constrains between inputs and outputs.""', 'token_ids = torch.tensor(glove_tokenizer.encode(sentence).ids)', 'tokens = [glove_vocabulary[x] for x in token_ids]', 'vectors = embeddings(token_ids)', '', '# EXERCISE: Implement the masks corresponding to each factorization', '# Hint: check the documentation for `torch.diag()`, `torch.triu()` and `torch.tril()`', 'T = len(token_ids)', 'masks = {', '    ""left-to-right"": ..., # <- YOUR CODE HERE', '    ""bidirectional"": ..., # <- YOUR CODE HERE', '    ""right-to-left"": ..., # <- YOUR CODE HERE', '}', 'for key in masks.keys():', '",0.5516355633735657,notebook 5_1,53.0,"'markdown' cell: '['**Exercise 5**:  What effect has the parameter $\\tau$ on the attention mechanism? What is happening when using a large value for $\\tau$? using a small value for $\\tau$? In the limits $\\tau \\rightarrow 0$ and $\\tau \\rightarrow \\infty$', '', '> *Insert your answer here*', '', '', '**Visualizing the output word vector** In the code below we use the code from the previous word2vec experiment to generate the nearest neighbour corresponding to the analogy: ', '', '$$\\mathrm{vec(""Capital\\,city\\,of\\,the\\,query\\,country"")} = \\mathrm{vec(""Query\\,country"")} + \\mathrm{Attention}(\\mathrm{vec(""Query\\,country"")}, \\mathbf{K}, \\mathbf{V})$$']'

 'code' cell: '['# Report of the nearest neighbors of the vector `query + Attention(query, keys, values)``', 'for i, attn_output in enumerate(output):', '    # z = query + Attention(qeury, keys, values)', '    z = Q[i] + attn_output', '    rich.print(f""Nearest neighbors of [red]{queries_labels[i]}[/red] + [blue]Attention({queries_labels[i]}, K, V)"")', '    rich.print(vec2words(z, k=5, **glove_args, exclude_vecs=[word2vec(queries_labels[i], **glove_args)]))']'",0.560402512550354,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0
What is sampling for a language model?,"Sampling text means that the language model is generating text. This corresponds to the inference process of the network, as opposed to training.",notebook 5_1,notebook 5_1,20.0,"'markdown' cell: '['**Sampling** At each step $t$, the $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ implements a categorical distribution defined on the vocabulary $\\mathcal{V}$. Sampling or *generating* text can by iteratively sampling tokens, as showed in the pseudo-code bellow:', '```python', 'ws = [] # placeholder for all the samples w_t', 'for t in range(T):', '    wt_logits = f(ws, theta) # logits of p(w_t | w_{<t})', '    wt = Categorical(logits=wt_logits).sample() # sampled w_t', '    ws.append(wt) # append the new sample to be used as input in the next step', '```', 'Sampling is often expensive, as it requires one evaluation of the function $f_\\theta$ for each step and cannot be trivially parallelized.', '', '**Training** As long as the transition function $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ is differentiable (i.e., using neural networks), a language model can be  trained via maximum likelihood, e.g. maximizing the log-likelihood with the loss:', '$$', 'L = - \\log p_\\theta(\\mathbf{w}_{1:T}) = - \\sum_{t=1}^T \\log p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})', '$$', 'Each term",0.37605229020118713,notebook 5_1,16.0,"'markdown' cell: '['**Beyond word2vec**  The Skip-Gram model allows us to learn meaningful word representations and arithmetic in the resulting vector space, allowing us to manipulate concepts. Ultimately, we are interested in learning representations that represent larger text fragments such as sentences, paragraphs or documents. Doing so requires combining multiple vectors, which can be done by exploiting arithmetic in the vector space or by combining word-vectors using deep neural networks, such as Transformers!', '', '___', '## II. Language models', '', 'We have seen how to encode text into sequences of tokens, seen how to convert tokens into vectors using a set of embeddings and experimented with a GloVe word vector space. In this section we will see how to model text at the sentence, pragraph or even document level using language models.', '', '### II.a Language Modelling', '', '*Figure: Left-to-right language models*', '![Autoregressive left-to-right language model](images/ar-lm.png)', '', '**Autoregressive factorization** Language models aim at grasping the underlying linguistic structure of a text fragment: whereas word vectors model words independently of each others, a language model tracks the grammatical and semantic relationships between word tokens. Given a piece of text encoded into tokens $\\mathbf{w}_{1:T} = [\\mathbf{w_1}, \\ldots, \\mathbf{w}_T]$ a *left-to-right* language model describes $\\mathbf{w}_{1:T}$ with the following factorization:', '$$', ' p_\\theta(\\mathbf{w}_{1:T}) = \\prod_{t=1}^T",0.49271121621131897,notebook 5_1,73.0,"that encodes the source text and a language model decoder conditioned on the encoded source text. Transformer-based language models consist in a single decoder component (without conditional attention). The two main alternatives for language modelling are:', '', '* [Generative Pre-trained Transformers (GPT)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf): autoregressive left-to-right language models implemented using a Transformer.', '', '* [Bidirectional Encoder Representations from Transformers (BERT)](https://arxiv.org/abs/1810.04805): a masked language model trained to predict tokens that are randomly masked out (masked language model) and trained to predict whether two sentences are related or not (next-sentence prediction (NSP) task).']'",0.5287131071090698,notebook 5_1,77.0,"'markdown' cell: '['**Exercise 9**: Play with the temperature parameter, what is it controlling? how can you related it to the definition of a language model?', '', '> *Insert your your answer here*']'

 'markdown' cell: '['### V.b. Prompt-based learning', '', 'Prompt-based learning consists in framing a problem into a *prompt* which completion by a language model corresponds to the answer. In other words, it consists in using natural language to interface with language models.', '', '**Experiment** Do language models know what hygge is?', '', '**Exercise 10**: Write a prompt that triggers a language model to define ""hygge"".', '', '> *write your prompt here and test it using GPT-2 and GPT-3*']'

 'code' cell: '['prompt = ""Q: What is Hygge? A:""', ""input_ids = tokenizer.encode(prompt, return_tensors='pt')"", 'input_ids = input_ids.repeat(5, 1)', 'output = model.generate(input_ids, do_sample=True, temperature=1, max_length=50)', 'decoded = tokenizer.batch_decode(output)', 'for txt in decoded:', '    txt = txt.replace(""\\n"", """")', '    print(txt)']'

 'markdown' cell: '['**Exercise 11**: Test your prompt [with GPT-3](https://beta.openai.com/playground)', '', '> *Insert your prompt and the GPT-3 completion here*', '']'",0.5314544439315796,notebook 5_1,19.0,"model $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$.', '', '**Language models learn contextual representations** Assigning a probability to each word in the vocabulary requires learning abstract representations of the context $\\mathbf{w}_{<t}$. For instance, in the horse example, predicting the word ""fast"" will be easier to predict if some knowledge of the grammatical rules and common sense is acquired. In this example example, the model needs to learn that $\\mathbf{w}_4$ must be an adjective, and that this adjective can be attributed to a horse. Therefore, the function $f_\\theta$ must acquire a non-trivial representation of the context to make sensible token predictions$.']'",0.532628059387207,notebook 5_1,17.0,"language model describes $\\mathbf{w}_{1:T}$ with the following factorization:', '$$', ' p_\\theta(\\mathbf{w}_{1:T}) = \\prod_{t=1}^T p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t}) \\ ,', '$$', 'where $\\theta$ is a model parameter. The above *autoregressive* factorization describes a *recursive* function $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$, which is shared across all the time steps. In the above figure, we represent a left-to-right language model with dependencies represented by arrows for fixed steps $t=3$ and $t=4$. Because of this choice of factorization, a language model defines a graphical model where each step $t$ depends on all the previous steps $<t$ and the conditional $p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$ models the dependendies between the context $\\mathbf{w}_{<t}$ and the variable $\\mathbf{w}_t$.', '', '**Other factorizations** Autoregressive models are not required to adopt a left-to-right factorization and other forms of factorizations can be implemented (right-to-left or arbitrary permutations). See [""XLNet: Generalized Autoregressive Pretraining for Language Understanding"", Yang et al. (2019)](https://arxiv.org/abs/1906.08237) for an example.']'",0.5547282695770264,notebook 5_1,75.0,"we will us the blackbox [`Pipeline`](https://huggingface.co/docs/transformers/v4.22.1/en/main_classes/pipelines#transformers.pipeline) object. If you want to apply Transformers without modifying any of the components, the `Pipeline` can be used to perform complex tasks in one line of code, as showed here with the translation task.', '', 'The OpenAI API gives access to GPT-3 ([""Language Models are Few-Shot Learners"", Brown et al. (2020)](https://arxiv.org/abs/2005.14165)) through a [playground](https://beta.openai.com/playground), where you can test the text completion capabilities of these models. GPT-3 is a large language model (up to 175 billion parameters) which has acquired impressive language understanding capabilities. It can be applied to solve new tasks without task-specific fine-tuning. [OpenAI gives you $18 to of API credits, but careful with the number of calls: running the largest version of GPT-3 (´davinci´) can be expensive](https://openai.com/api/pricing/).', '', '### V.a Language generation', '', ""Let's experiment with GPT-2 (in the notebook, we use the smaller [`distilgpt2`](https://huggingface.co/distilgpt2), but feel free to use the original `gpt2` if you have enough compute)"", '', '**Experiment** Generate text using GPT-2:']'",0.5561829805374146,notebook 5_1,57.0,"'markdown' cell: '[""**Exercise 8**:  Let's implement an attention mask corresponding to:"", '1. Left-to-right language model $p(\\mathbf{w}_t \\mid \\mathbf{w}_{<t})$', '1. bidirection language model $p(\\mathbf{w}_t \\mid \\mathbf{w}_{-t})$', '1. right-to-left language model $p(\\mathbf{w}_t \\mid \\mathbf{w}_{t>})$', '', '> Answer in the code below.', '', '**NB** In the visualization bellow, we re-use the gradient map extracted from the left-to-right RNN language model.']'",0.5565862655639648,notebook 5_1,72.0,"'markdown' cell: '['### IV.c Conditional layers', '', ""In the description of the base layer, we have applied attention to the input sequence (*self-attention*). Machine translation is a sequence-to-sequence task, which requires and *encoder* component, that encodes a source text into a sequence of hidden states $\\mathbf{g_{1:T'}}$. The base layer can be modified with an additional attention layer that is conditionned on the source text. Given a sequence of hidden states $\\mathbf{h}_{1:T}$, the conditional attention layer is:"", '$$', ""\\mathrm{Attention}(\\mathbf{Q}(\\mathbf{h}_{1:T}), \\mathbf{K}(\\mathbf{g}_{1:T'}), \\mathbf{V}(\\mathbf{g}_{1:T'})) \\ ."", '$$', 'Conditional attention layers are place right after the self-attention layers, before the feed-forward layer (see diagram).', '', '### IV.d Pre-training as language models', '', 'Transformers (the decoder component) are language models can therefore be pre-train on vast amount of unlabelled text via maximum likelihood or pseudo likelihood. They allow obtaining contextualized text representations, that can be applied to a multitude of downstream tasks (question-answering, classification, ...). While the original Transformer architecture was applied to a sequence-to-sequence problem with a component that encodes the source text and a language model decoder conditioned on the encoded source text. Transformer-based language models consist in a single decoder component (without conditional attention). The two main alternatives for language modelling are:', '', '* [Generative Pre-trained Transformers",0.5609427690505981,notebook 5_1,22.0,"'markdown' cell: '['*Figure: Bidirectional language models*', '![Masked language model](images/masked-lm.png)', '', '**Bidirectional and masked language models** Autoregressive language models learn to predict a token $\\mathbf{w}_t$ given the context up to the step $t-1$. One can also use a [pseudo likelihood](https://en.wikipedia.org/wiki/Pseudolikelihood), where $\\mathbf{w}_t$ is not only conditioned on the preceeding tokens $\\mathbf{w}_{<t}$, but also on the next tokens $\\mathbf{w}_{>t}$. This defines a bidirectional language model, which factorizes as', '$$', 'L_\\theta(\\mathbf{w}_{1:T}) = \\prod_{t=1}^T p_\\theta(\\mathbf{w}_t \\mid \\mathbf{w}_{-t}) \\ ,', '$$', 'where $\\mathbf{w}_{-t}$ represent the set of tokens $\\mathbf{w}_{1:T} \\backslash \\{ \\mathbf{w}_t \\}$. We call it pseudo because this likelihood is not forming a valid distribution (because the graph formed by $\\mathbf{w}_{1:T}$ is not a directed acyclic graph (a DAG)).  Bidirectional language models such as [ELMo (""Deep contextualized word representations"", Peters et al. (2018))](https://arxiv.org/abs/1802.05365), learn token representation contextualized on the whole context.', '', 'In the case, of bidirectional language",0.5618789196014404,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0
What is a rnn?,"A recurrent neural network (RNN) is a type of neural network that has been succesful in modelling sequential data, e.g. language, speech, protein sequences, etc. A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence. The idea is that the network should be able to use the previous computations as some form of memory and apply this to future computations"," notebook 5_2, notebook 5_3",notebook 5_2,24.0,"'markdown' cell: '['# Introduction to Recurrent Neural Networks (RNN)', '', 'Reading material: [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and (optionally) [this lecture](https://www.youtube.com/watch?v=iWea12EAu6U&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z).', '', '___', '', 'A recurrent neural network (RNN) is a type of neural network that has been succesful in modelling sequential data, e.g. language, speech, protein sequences, etc.', '', 'A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.', 'The idea is that the network should be able to use the previous computations as some form of memory and apply this to future computations.', 'An image may best explain how this is to be understood,', '', '![rnn-unroll image](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/static_files/rnn-unfold.png?raw=1)', '', '', 'where it the network contains the following elements:', '', '- $x$ is the input sequence of samples, ', '- $U$ is a weight matrix applied to the given input sample,', '- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,', '- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),', ""- $h$ is the hidden state (the",0.6269567012786865,notebook 5_3,16.0,"'markdown' cell: '['# Introduction to Recurrent Neural Networks (RNN)', '', 'Reading material: [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and (optionally) [this lecture](https://www.youtube.com/watch?v=iWea12EAu6U&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z).', '', '___', '', 'A recurrent neural network (RNN) is a type of neural network that has been succesful in modelling sequential data, e.g. language, speech, protein sequences, etc.', '', 'A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.', 'The idea is that the network should be able to use the previous computations as some form of memory and apply this to future computations.', 'An image may best explain how this is to be understood,', '', '![rnn-unroll image](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/static_files/rnn-unfold.png?raw=1)', '', '', 'where it the network contains the following elements:', '', '- $x$ is the input sequence of samples, ', '- $U$ is a weight matrix applied to the given input sample,', '- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,', '- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),', ""- $h$ is the hidden state (the",0.6269567012786865,notebook 5_1,39.0,"'code' cell: '['# sample the RNN language model', 'with torch.no_grad():', '    sample = rnn.sample(num_steps=10, batch_size=10, temperature=0.5, prevent_repetitions=True)', '    rich.print(glove_tokenizer.decode_batch(sample.detach().cpu().numpy()))']'

 'code' cell: '['_ = rnn.cpu()', '# free-up memory if needed: delete the RNN model', '# del rnn']'

 'markdown' cell: '['**Exercise 3**: What would happen if the inputs of the RNN were not shifted to the right (in sample in the RNNLM class)?  ', '', '> *Insert your answer here.*']'",0.6743946075439453,notebook 5_3,18.0,"'markdown' cell: '['## Implementing an RNN']'

 'markdown' cell: '['We will implement the forward pass, backward pass, optimization and training loop for an RNN in numpy so that you can get familiar with the recurrent nature of RNNs. Later, we will go back to PyTorch and appreciate how convenient the implementation becomes!']'

 'markdown' cell: '[""Let's first define the necessary model parameters. Recall that an $n \\times m$ weight matrix maps $\\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{n}$.""]'",0.6760581135749817,notebook 5_2,26.0,"'markdown' cell: '['## Implementing an RNN', '', 'We will implement the forward pass, backward pass, optimization and training loop for an RNN in Nanograd so that you can get familiar with the recurrent nature of RNNs. Later, we will go back to PyTorch.']'

 'markdown' cell: '['We define the Nanograd DenseLayer class from [lab 2](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_Python/2.1-EXE-FNN-AutoDif-Nanograd.ipynb) with a few additions:', '* the option use_bias to define a layer without bias. This is useful when we define the recurrent layer and', '* a method forward_sequence which is useful when a DenseLayer is used as part of a recurrent neural network']'",0.6919266581535339,notebook 5_2,1.0,"How to build and train an RNN in Nanograd', '* How to build and train an LSTM network in Nanograd', '* How to build and train an LSTM network in PyTorch', '', '', '[Numpy version of the Notebook (previous version)](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/5_Recurrent/OLD-5.1-Numpy-Recurrent-Neural-Networks.ipynb)']'",0.6929807662963867,CoursePlan.txt,13.0,"Week 5 - Transformers and recurrent neural networks

    Watch week 3 video lectures

    02456week3 1 RNN (PART 1 of 3)
    02456week3 1 RNN (PART 2 of 3)
    02456week3 1 RNN (PART 3 of 3)
    02456week3.2_RNN_training (PART 1 of 3)
    02456week3.2_RNN_training (PART 2 of 3)
    02456week3 2 RNN training (PART 3 of 3)
    02456week3 3 Attention (PART 1 of 2)
    02456week3 3 Attention (PART 2 of 2)
    02456week3 4 Supervised learning recap
    2017 Quasi RNN
    2017 Non-recurrent sequence to sequence models
    2017 Text summarization
    2020 Transformers (PART 1 of 2)
    2020 Transformers (PART 2 of 2)
    2020 Language modelling - GPT-2 and 3
    2020 BERT

and take notes for at least 3 questions to ask. Link to: 2016 lectures, 2017 lecture updates and 2020 lecture updates.",0.6956328749656677,notebook 5_1,25.0,"space using a projection matrix $\\mathbf{F} \\in \\mathcal{R}^{V \\times d_h}$. This results in parameterizing the transition distribution as', '$$', 'p_\\theta(\\cdot \\mid \\mathbf{w}_{<t}) = \\mathrm{Softmax}( \\mathbf{h}_t \\mathbf{F}^T)', '$$', 'In the above figure, we showcase how a standard RNN can be applied to implement a left-to-right language model, and annotated the diagramm with the function $h_\\theta(\\mathbf{w}_{t}, \\mathbf{h}_{t-1)})$ and the projection matrix $\\mathbf{F}$.', '', '**Long Short-Term Memory (LSTM) networks** A standard RNN suffers from [the vanishing gradients problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem) which gives challenges in saving memory over longer sequences. To combat these issues the gated hidden units were created. The two most prominent gated hidden units are the [Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber. (1997))](https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735) cell and the Gated Recurrent Unit (GRU), both of which have shown increased performance in saving and reusing memory in later timesteps. RNNs coupled with gated mechanisms are less prone to the problem of vanishing gradients, and can therefore model dependencies over longer number of steps.']'",0.7600182294845581,notebook 5_1,35.0,"'markdown' cell: '['**Exercise 1**: Explain in your own words what the plot shows. How would it look if we had a leakage of information from the future to the present?', '', '> *Insert your answer here.*']'

 'markdown' cell: '['**Exercise 2**: Implement the loss of the RNN language model.', '', '> answer in the code below.']'",0.7645207643508911,notebook 5_3,25.0,"'code' cell: '['def forward_pass(inputs, hidden_state, params):', '    """"""', '    Computes the forward pass of a vanilla RNN.', '    ', '    Args:', '     `inputs`: sequence of inputs to be processed', '     `hidden_state`: an already initialized hidden state', '     `params`: the parameters of the RNN', '    """"""', '    # First we unpack our parameters', '    U, V, W, b_hidden, b_out = params', '    ', '    # Create a list to store outputs and hidden states', '    outputs, hidden_states = [], []', '    ', '    # For each element in input sequence', '    for t in range(len(inputs)):', '', '        # Compute new hidden state', '        # YOUR CODE HERE!', '        hidden_state = ', '', '        # Compute output', '        # YOUR CODE HERE!', '        out = ', '        ', '        # Save results and continue', '        outputs.append(out)', '        hidden_states.append(hidden_state.copy())', '    ', '    return outputs, hidden_states', '', '', '# Get first sequence in training set', 'test_input_sequence,",0.7795169949531555,1.0,2.0,2.0,3.0,4.0,5.0,5.0,5.0,5.0,6.0
What topics are covered in the first three weeks of the course?,". Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part I do it yourself on pen and paper. 2. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part II do it yourself in NumPy. 3. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part III PyTorch.", CourseOutline.txt,CoursePlan.txt,5.0,"Week 1 computer exercise. Deadline: Monday week 2.
    Week 2 computer exercise. Deadline: Monday week 3.
    Week 3 computer exercise and 1 exercise of your own choice from course material week 1. Deadline: Monday week 4
    Week 4 computer exercise  and 1 exercise of your own choice from course material week 1-2. Deadline: Monday week 5.
    Week 5 computer exercise. Deadline: Monday week 6.
    Week 6 computer exercise. Deadline: Monday week 7.
    Week 7 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 8
    Week 8 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 9.
    Project selection. Deadline Friday, Oct 20th 2023 at 23.59.
    Link to 2023 project selection sheet
    Project synopsis. Deadline: Monday week 9 at 23:59. The synopsis should be approximately half a page and maximum one page with a project title, motivation, background, milestones and references. It is important that the plan is realistic. The main purposes of the synopsis are to make sure the project size is well-calibrated and is concrete enough to start working from day one. The synopsis will not be used in the evaluation. The synopsis should be sent to your project supervisor.",0.6422405242919922,CoursePlan.txt,12.0,"and take notes for at least 3 questions to ask. Link to lecture slides is here and here for 2017 updates.

    Reading material Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapter 6 (stop when reaching section called Other approaches to deep neural nets).
    Alternative textbook chapter in the deep learning book.
    One exercise from the book chapters.
    Carry out computer exercises week 4.
    Hand in the notebook marked with EXE on peergrade.io.
    Peergrade exercise from three other students through peergrade.io. You will receive instructions about this from peergrade.io.

Week 5 - Transformers and recurrent neural networks

    Watch week 3 video lectures",0.6609260439872742,CoursePlan.txt,16.0,"and take notes for at least 3 questions to ask. Link to lecture slides 2016 lecture slides, 2017 blog post and 2020 lecture slides.  

    Reading material Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapter 3 from section Overfitting and regularization and Chapter 5.
    Alternative textbook chapters on regularization, optimization, deep learning practice and applications from the deep learning book.  
    Additional material: Andrei Karpathy blogpost on how to approach a data science problem with deep learning, blogpost on things that can go wrong in neural network training and interactive initialization demo.
    Computer exercises week 6 using PyTorch on the Kaggle competition leaf classification. Hand in and peergrade on peergrade.io like in previous weeks.

Week 7 - Un- and semi-supervised learning

    Watch week 5 video lectures

    02456week5 1 1 unsupervised learning
    02456week5 1 2 unsupervised learning latent variables
    02456week5 2 1 autoencoders
    02456week5 2 2 autoencoders layerwise pretraining
    02456week5 3 1 variational autoencoders
    02456week5 3 2 semi-supervised variational autoencoders 
    2017 Generative adversarial networks
    2020 Flows
    2020 Self-supervised learning
    2020 Self-training/noisy student
    2020 Distribution Augmentation
    2020 Flat minima",0.704420804977417,CoursePlan.txt,17.0,"and take notes for at least 3 questions to ask. Link to lecture slides 2016 slides and 2017 slides and 2020 slides.

    Reading material DL Chapter 14 and 20.10.3. (Further learning a course dedicated to generative modelling.)
    One exercise from the book chapters.
    Carry out computer exercises week 7 on autoencoder un- and semi-supervised. Hand in and peergrade on peergrade.io like in previous weeks.
    Project selection deadline is this week (see above).

Week 8 - Reinforcement learning 

    Watch week 6 video lectures 

    02456week6 1 1 reinforcement learning
    02456week6 1 2 reinforcement learning approaches
    02456week6 2 1 AlphaGo policy and value networks
    02456week6 2 2 AlphaGo steps 1 to 4
    02456week6 3 policy gradients
    02456week6 4 a few last words
    2017 Deep Q learning
    2017 Evolutionary strategies

and take notes for at least 3 questions to ask. Link to lectures here and here for 2017 update.

    Reading: another nice blog post by Andrei Karpathy. Optional reading material on the connection between variational and reinforcement learning.
    One exercise from the book chapters. 
    Computer exercises on reinforcement learning methods (policy gradient, deep Q learning, evolutionary strategies) in the openAI Gym. Carry out exercises week 8. Hand in and peergrade on peergrade.io like in previous weeks.
    Project work.",0.7113507986068726,notebook 5_1,0.0,"'markdown' cell: '['# Week 5 - Deep learning with Transformers']'

 'markdown' cell: '['Some preliminary set-up.']'",0.7255055904388428,CoursePlan.txt,0.0,"02456 Deep learning 2023 - course plan and information

Time: Mondays at 13:00-17:00 (first session is August 28th, 2023)

Locations: We will use the following rooms - building/room - (Campus map):

B303A-A042

B303A-046

B303A-047

B303A-048

B303A-HOEST

Zoom (You need to sign-in with you DTU account)

We use flipped classroom teaching. During the weeks with labs, the teachers and teaching assistants will circulate between the rooms so there will be opportunity to meet all. Any short lectures/instructions will be repeated in all rooms. You are free to choose whatever room you prefer of course respecting the limits on room capacity. During the weeks with project work each room will cover specific topics.

If you are not able to be on campus or prefer to work remotely you will be able to participate through Zoom. One teaching assistant will be dedicated to the Zoom channel: Zoom.

We also use Slack for communication: We will make dedicated channels for labs and projects. Here is a Slack invite link. (In Slack you can add channels from the list of channels by clicking the “+” next to Channels in the left panel and click “Browse channels” to choose.)

Bring a laptop.

The first eight weeks of the course will be dedicated to lab work. There will be a brief introduction to the course at the first session and a number of dedicated meetings online or in person with project supervisors.

Teachers

    Ole Winther
    Jes Frellsen

Teaching assistants",0.7360115051269531,CoursePlan.txt,8.0,"Detailed content

Links to individual video lectures and lecture slides are given below. Here is a link to all 2016 video lectures as a playlist and a Google doc folder with all the lecture slides. More videos have been added over the years. They are all linked below. A very good alternative video resource is Hugo Larochelle’s YouTube playlist.
Week 1 - Feed-forward neural networks - do it yourself pen and paper

    During this week and the following two weeks watch video lectures: 

    Part 0 Overview
    Part 1 Deep learning
    Part 2.1 Feed-forward neural networks
    Part 2.2 Feed-forward neural networks
    Part 3 Error Backpropagation
    Part 4 Optimization

and take notes for at least 3 questions to ask. Link to lecture slides is here.",0.7562510371208191,CoursePlan.txt,14.0,"and take notes for at least 3 questions to ask. Link to: 2016 lectures, 2017 lecture updates and 2020 lecture updates.

    Reading material Alex Graves book, Supervised Sequence Labelling with Recurrent Neural Networks Chapters 3.1, 3.2 and 4. Browse Michael Nielsen, Neural networks and deep learning Chapter 6 section Other approaches to deep neural nets) and onwards. A good introduction to Transformers is The Illustrated Transformer. New tutorial on Transformers https://aman.ai/primers/ai/transformers/#one-hot-encoding
    Alternative textbook chapter in the deep learning book. Andrej Karpathy has a nice blogpost that gives a good flavour of the whats and hows of RNNs.
    Carry out computer exercises week 5
    Hand in and peergrade on peergrade.io like in previous week.

Week 6 - Tricks of the trade and data science challenge

    Watch week 4 video lectures",0.766103982925415,CoursePlan.txt,9.0,"During this week and the following two weeks read Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapters 1-3 (stop when reaching the section called Overfitting and regularization) and browse Chapter 4. Note that this is reading material for the first three weeks of the course. Also, in total six exercises of your own choice will be homework later in the course.
    Alternative textbooks: All topics are also covered in the deep learning book that may be read as a supplement. The book can also be bought from the DTU bookstore. You will get 10% discount with this link. Feed-forward neural networks are covered in this chapter. Chapter 1 gives an introduction to deep learning and Part II gives the necessary background on linear algebra, probability, numerical computation and machine learning. Alternative textbook 2: Chris Bishop, Pattern recognition and machine learning. If you need to up your game in mathematics, the book Mathematics for machine learning is an excellent resource and the note Mathematics for Machine Learning offers a concise and compressed collection of the mathematical concepts used in deep learning. These resources are freely available online and are very valuable sources of information.
    Install software on your laptop or go directly to Google CoLab (see above). Installation guide for laptop and cloud may be found here.",0.7680788040161133,notebook 5_3,1.0,"we will show you:', '* How to represent sequences of categorical variables', '* How to build and train an RNN in NumPy', '* How to build and train an LSTM network in NumPy', '* How to build and train an LSTM network in PyTorch']'",0.7841282486915588,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
When does project work start?,Starting from week 6 and full time from week 9 and the rest of the term will be spent on tutored project work., CourseOutline.txt,CoursePlan.txt,5.0,"Week 1 computer exercise. Deadline: Monday week 2.
    Week 2 computer exercise. Deadline: Monday week 3.
    Week 3 computer exercise and 1 exercise of your own choice from course material week 1. Deadline: Monday week 4
    Week 4 computer exercise  and 1 exercise of your own choice from course material week 1-2. Deadline: Monday week 5.
    Week 5 computer exercise. Deadline: Monday week 6.
    Week 6 computer exercise. Deadline: Monday week 7.
    Week 7 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 8
    Week 8 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 9.
    Project selection. Deadline Friday, Oct 20th 2023 at 23.59.
    Link to 2023 project selection sheet
    Project synopsis. Deadline: Monday week 9 at 23:59. The synopsis should be approximately half a page and maximum one page with a project title, motivation, background, milestones and references. It is important that the plan is realistic. The main purposes of the synopsis are to make sure the project size is well-calibrated and is concrete enough to start working from day one. The synopsis will not be used in the evaluation. The synopsis should be sent to your project supervisor.",0.729928731918335,CoursePlan.txt,3.0,"Week 9-13 will be only project work

In the seven project weeks we will still meet on Mondays for project work and supervision.
Evaluation and peer grading during the course

​​Evaluation:

    The course is graded using the 7-step scale.
    The final grade is based solely on the evaluation of the final project, which starts in the 7th week of the course. The project group should consist of 3-4 students. In special circumstances we can also accept groups of 1 or 2 students. (In the course catalogue it says 1-3 students. We will correct that for next year but cannot change it now.)
    The evaluation of the final project is based on two parts, both of which are done in groups but evaluated individually:

    a poster exam presentation, where the project groups document the results of their project in a poster and present to two or more teachers acting as examiners and
    a report in which the project groups document their solution. The report should be a maximum of 6 pages plus references using this conference paper format.

More details are given below.

    The student gains access to the final project by passing 6 out of 8 lab sessions that precede it.
    A lab session is passed by:

    grading the reports from lab sessions of 3 other students on Peergrade and
    passing the lab as judged by the teacher. More details given below.",0.7945431470870972,CoursePlan.txt,7.0,"Final report deadline December 21st at 23:59. [Note this was earlier set to a later date but according to DTU rules, the latest allowed deadline is December 21st.] The report should be a maximum 6 pages plus references using this conference paper format. The report should also contain a link to your project code Github repository. Among the files in the repository should be a jupyter notebook that ideally should recreate the main results of your report. If some of your data is confidential then use some shareable data instead. For MSc students, please also include your poster in the submission.",0.8089946508407593,LearningObjectives.txt,0.0,"Demonstrate knowledge of machine learning terminology such as likelihood function, maximum likelihood, Bayesian inference, feed-forward, convolutional and Transformer neural networks, and error back propagation.

Understand and explain the choices and limitations of a model for a given setting.

Apply and analyze results from deep learning models in exercises and own project work.

Plan, delimit and carry out an applied or methods-oriented project in collaboration with fellow students and project supervisor.

Assess and summarize the project results in relation to aims, methods and available data.

Carry out the project and interpret results by use of computational framework for GPU programming such as PyTorch.

Structure and write a final short technical report including problem formulation, description of methods, experiments, evaluation and conclusion.

Organize and present project results at the final project presentation and in report.

Read, evaluate and give feedback to work of other students.",0.8135966062545776,notebook 8_2_Prerequisites_ipynb,2.0,"'markdown' cell: '['## Running an environment', '', 'Here is a bare minimum example of running a Gym environment. This creates an instance of the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) environment and runs until the rollout is done, taking random actions and rendering the environment at each step. With Gym installed, you should be able to see a small animation of the environment below.']'

 'code' cell: '['import gym', 'from gym import wrappers', 'from colabgymrender.recorder import Recorder', '', ""env = gym.make('CartPole-v0') # Create the environment"", ""directory = './video' # Directory for the recorded video"", 'env = Recorder(env, directory)', 'env.reset() # Reset environment', '', '# Run environment', 'while True:', '    action = env.action_space.sample() # Get a random action', '    _, _, done, _ = env.step(action) # Take a step', '    if done: break # Break if environment is done', '', 'env.close() # Close environment', 'env.play() # Show the video']'

 'markdown' cell: '['Hooray! You now have a working `Gym` environment that we can take actions in and render.']'",0.8682719469070435,CoursePlan.txt,10.0,"Install software on your laptop or go directly to Google CoLab (see above). Installation guide for laptop and cloud may be found here.
    Carry out computer exercises week 1. It is encouraged to work together with other students. Type in everything yourself. Code answers are fine not to differ much within the group and text answers should be in your own words. Note that the computer exercises may experience minor change up to 3 days before the actual session. The hand-in is the notebook with your modifications. It is only allowed to hand in .ipynb files. Each week you should only hand in one file. It is the file with EXE in its name. You hand in on peergrade.io. In order to be able to hand in on peergrade you can use this invitation link https://app.peergrade.io/join/JFFJBF or login into peergrade and use the invitation code JFFJBF. There you will receive information on handing in exercises and deadlines for activities. Some students have previously by accident signed up twice with different emails or forgotten the “student” in their DTU student mail address. If you then submit and check with different email addresses it will look as though you have not handed in.
    Peergrade exercise from three other students through peergrade.io.",0.8798943758010864,notebook 8_3,4.0,"'markdown' cell: '['First we create the environment:']'

 'code' cell: '[""env = gym.make('CartPole-v0') # Create environment""]'

 'markdown' cell: '['A state in this environment is four numbers describing the position and speed of the cart along with the angle and angular speed of the pole.', '', 'There are two available actions: push the cart *left* or *right* encoded as 0 and 1.']'

 'code' cell: '['s = env.reset()', 'a = env.action_space.sample()', ""print('sample state:', s)"", ""print('sample action:', a )""]'

 'markdown' cell: '['Let us see how the environment looks when we just take random actions. Note that the episode ends when the pole either 1) is more than 15 degrees from vertical, 2) more outside of the frame or 3) the pole is successfully balanced for some fixed duration.']'

 'code' cell: '[""env = gym.make('CartPole-v0') # Create environment"", 'env = Recorder(env, ""./video"") # To display environment in Colab', 'env.reset() # Reset environment', '', '# Run environment', 'while True:', '    action = env.action_space.sample() # Get a random action', '    _, _, done, _ = env.step(action) # Take a step', '    if done: break # Break if environment is done', '', 'env.close() # Close environment', 'env.play()']'",0.8814290761947632,notebook 4_3,15.0,"'code' cell: '['def initialize_model(model_name: str, *, num_classes: int, finetune_entire_model: bool = False):', '    """"""Returns a pretrained model with a new last layer, and a dict with additional info.', '', '    The dict now contains the number of model parameters, computed as the number of', '    trainable parameters as soon as the model is loaded.', '    """"""', '', '    print(', '        f""Loading model \'{model_name}\', with ""', '        f""finetune_entire_model={finetune_entire_model}, changing the ""', '        f""last layer to output {num_classes} logits.""', '    )', '    model = timm.create_model(', '        model_name, pretrained=True, num_classes=num_classes', '    )', '', '    num_model_parameters = 0', '    for p in model.parameters():', '        if p.requires_grad:', '            num_model_parameters += p.numel()', '', '    if not finetune_entire_model:', '        for name, param in model.named_parameters():', '            param.requires_grad = False', '', '        # Layer names are not consistent, so we have to consider a few cases. This might ', '",0.8828926682472229,notebook 5_1,38.0,"<- YOUR CODE HERE ', '', '            # backward and optimize', '            loss.backward()', '            optimiser.step()', '            step += 1', '            pbar.update(1)', '', '            # Report', '            if step % 5 ==0 :', '                loss = loss.detach().cpu()', '                pbar.set_description(f""epoch={epoch}, step={step}, loss={loss:.1f}"")', '', '            # save checkpoint', '            if step % 50 ==0 :', '                torch.save(rnn.state_dict(), checkpoint_file)', '            if step >= num_steps:', '                break', '        epoch += 1']'",0.9023239612579346,CoursePlan.txt,0.0,"02456 Deep learning 2023 - course plan and information

Time: Mondays at 13:00-17:00 (first session is August 28th, 2023)

Locations: We will use the following rooms - building/room - (Campus map):

B303A-A042

B303A-046

B303A-047

B303A-048

B303A-HOEST

Zoom (You need to sign-in with you DTU account)

We use flipped classroom teaching. During the weeks with labs, the teachers and teaching assistants will circulate between the rooms so there will be opportunity to meet all. Any short lectures/instructions will be repeated in all rooms. You are free to choose whatever room you prefer of course respecting the limits on room capacity. During the weeks with project work each room will cover specific topics.

If you are not able to be on campus or prefer to work remotely you will be able to participate through Zoom. One teaching assistant will be dedicated to the Zoom channel: Zoom.

We also use Slack for communication: We will make dedicated channels for labs and projects. Here is a Slack invite link. (In Slack you can add channels from the list of channels by clicking the “+” next to Channels in the left panel and click “Browse channels” to choose.)

Bring a laptop.

The first eight weeks of the course will be dedicated to lab work. There will be a brief introduction to the course at the first session and a number of dedicated meetings online or in person with project supervisors.

Teachers

    Ole Winther
    Jes Frellsen

Teaching assistants",0.9053341746330261,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
How are students expected to communicate and evaluate their project work?,"Organize and present project results at the final project presentation and in report. Read, evaluate and give feedback to work of other students", LearningObjectives.txt,CoursePlan.txt,3.0,"Week 9-13 will be only project work

In the seven project weeks we will still meet on Mondays for project work and supervision.
Evaluation and peer grading during the course

​​Evaluation:

    The course is graded using the 7-step scale.
    The final grade is based solely on the evaluation of the final project, which starts in the 7th week of the course. The project group should consist of 3-4 students. In special circumstances we can also accept groups of 1 or 2 students. (In the course catalogue it says 1-3 students. We will correct that for next year but cannot change it now.)
    The evaluation of the final project is based on two parts, both of which are done in groups but evaluated individually:

    a poster exam presentation, where the project groups document the results of their project in a poster and present to two or more teachers acting as examiners and
    a report in which the project groups document their solution. The report should be a maximum of 6 pages plus references using this conference paper format.

More details are given below.

    The student gains access to the final project by passing 6 out of 8 lab sessions that precede it.
    A lab session is passed by:

    grading the reports from lab sessions of 3 other students on Peergrade and
    passing the lab as judged by the teacher. More details given below.",0.5658602714538574,LearningObjectives.txt,0.0,"Demonstrate knowledge of machine learning terminology such as likelihood function, maximum likelihood, Bayesian inference, feed-forward, convolutional and Transformer neural networks, and error back propagation.

Understand and explain the choices and limitations of a model for a given setting.

Apply and analyze results from deep learning models in exercises and own project work.

Plan, delimit and carry out an applied or methods-oriented project in collaboration with fellow students and project supervisor.

Assess and summarize the project results in relation to aims, methods and available data.

Carry out the project and interpret results by use of computational framework for GPU programming such as PyTorch.

Structure and write a final short technical report including problem formulation, description of methods, experiments, evaluation and conclusion.

Organize and present project results at the final project presentation and in report.

Read, evaluate and give feedback to work of other students.",0.6326117515563965,CoursePlan.txt,4.0,"grading the reports from lab sessions of 3 other students on Peergrade and
    passing the lab as judged by the teacher. More details given below.

More details on peer grading: The 8 lab sessions are evaluated using peer grading. We use peer grading to ensure more accurate evaluation and better feedback. Graders get 3 reports at each deadline and have one week to carry out the feedback. If you forget to perform your peer grading it is not nice to your fellow students, but you can still pass that lab for which you forgot to grade.  

Handing in and peer grading six of the eight labs reports is required for being able to execute the project and eventually pass the course. If a hand-in is not passed you will be contacted with the option of re-submitting the lab directly to the teacher so if you hear nothing assume that you have passed the lab. You can also contact the teacher directly on Slack if something went wrong with the submission of the lab. Peergrade deadlines are strict so no need to write about getting an extension.

The following reports should be handed in jupyter notebook format. The weeks refer to weeks in term, and the fall break week is not counted.",0.6873175501823425,CoursePlan.txt,6.0,"Project poster session. PhD students taking the course as part of their PhD will not have to make a poster and take part of the poster session. In mixed groups of PhD and non-PhD students, only the non-PhD students have to take part in the poster session. The exam date is December 7th from 9 to 17. We divide the day into half hour slots and your group will later be given the possibility to register for a slot. A link to sign up for the poster session will appear here in due time. So having another exam on the same day should not be a problem. We will also organise an extra exam date for those of you who cannot make it on the date. It will be group poster presentations. We will invite outside guests and we will walk around and ask questions to all groups. We will make a schedule for when the teachers visit your poster. Plan for a 2 minute presentation per group member and 1-2 minutes for questions. The remainder of the time you can either present your poster to other students and guests or go visit other posters. Remember that it is important for the overall impression that you divide the presentation and answering of the questions more or less equally between you. The poster should be in A1 format. Remember to put both your names and student numbers under title. Here and here are links to examples using the latex template and here is one in powerpoint. You do not have to use that. The DTU library offers poster printing for a not too high price.",0.8028222322463989,notebook 4_3,19.0,"'markdown' cell: '['**Assignment 3:** ', '', '1. Train the linear classifier on top of the pre-trained network, and observe how quickly you can get pretty good results, compared to training a smaller network from scratch as above.', '', '2. Go back and change argument to finetune entire network, maybe adjust learning rate, see if you can get better performance than before and if you run into any issues.', '', '3. Optional: experiment with `timm`: try smaller or larger models, including state-of-the-art models, e.g. based on vision transformers (ViT) or MLP-Mixers.', '', '4. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.', 'Did anything surprise you during the exercise?', '', '5. Write down key lessons/insights you got during this exercise.']'

 'code' cell: '[]'",0.8495585918426514,CoursePlan.txt,5.0,"Week 1 computer exercise. Deadline: Monday week 2.
    Week 2 computer exercise. Deadline: Monday week 3.
    Week 3 computer exercise and 1 exercise of your own choice from course material week 1. Deadline: Monday week 4
    Week 4 computer exercise  and 1 exercise of your own choice from course material week 1-2. Deadline: Monday week 5.
    Week 5 computer exercise. Deadline: Monday week 6.
    Week 6 computer exercise. Deadline: Monday week 7.
    Week 7 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 8
    Week 8 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 9.
    Project selection. Deadline Friday, Oct 20th 2023 at 23.59.
    Link to 2023 project selection sheet
    Project synopsis. Deadline: Monday week 9 at 23:59. The synopsis should be approximately half a page and maximum one page with a project title, motivation, background, milestones and references. It is important that the plan is realistic. The main purposes of the synopsis are to make sure the project size is well-calibrated and is concrete enough to start working from day one. The synopsis will not be used in the evaluation. The synopsis should be sent to your project supervisor.",0.8609125018119812,notebook 4_2,10.0,"'markdown' cell: '['We now evaluate the network as above, but on the entire test set.']'

 'code' cell: '['# Evaluate test set', 'confusion_matrix = np.zeros((n_classes, n_classes))', 'with torch.no_grad():', '    model.eval()', '    test_accuracies = []', '    for inputs, targets in test_loader:', '        inputs, targets = inputs.to(device), targets.to(device)', '        output = model(inputs)', '        loss = loss_fn(output, targets)', '', '        predictions = output.max(1)[1]', '', '        # Multiply by len(inputs) because the final batch of DataLoader may be smaller (drop_last=True).', '        test_accuracies.append(accuracy(targets, predictions) * len(inputs))', '        ', '        confusion_matrix += compute_confusion_matrix(targets, predictions)', '', '    test_accuracy = np.sum(test_accuracies) / len(test_set)', '    ', '    model.train()']'

 'markdown' cell: '['Here we report the **average test accuracy** (number of correct predictions divided by test set size).']'

 'code' cell: '['print(f""Test accuracy: {test_accuracy:.3f}"")']'",0.8657118082046509,CoursePlan.txt,10.0,"Install software on your laptop or go directly to Google CoLab (see above). Installation guide for laptop and cloud may be found here.
    Carry out computer exercises week 1. It is encouraged to work together with other students. Type in everything yourself. Code answers are fine not to differ much within the group and text answers should be in your own words. Note that the computer exercises may experience minor change up to 3 days before the actual session. The hand-in is the notebook with your modifications. It is only allowed to hand in .ipynb files. Each week you should only hand in one file. It is the file with EXE in its name. You hand in on peergrade.io. In order to be able to hand in on peergrade you can use this invitation link https://app.peergrade.io/join/JFFJBF or login into peergrade and use the invitation code JFFJBF. There you will receive information on handing in exercises and deadlines for activities. Some students have previously by accident signed up twice with different emails or forgotten the “student” in their DTU student mail address. If you then submit and check with different email addresses it will look as though you have not handed in.
    Peergrade exercise from three other students through peergrade.io.",0.8881858587265015,CoursePlan.txt,7.0,"Final report deadline December 21st at 23:59. [Note this was earlier set to a later date but according to DTU rules, the latest allowed deadline is December 21st.] The report should be a maximum 6 pages plus references using this conference paper format. The report should also contain a link to your project code Github repository. Among the files in the repository should be a jupyter notebook that ideally should recreate the main results of your report. If some of your data is confidential then use some shareable data instead. For MSc students, please also include your poster in the submission.",0.892407238483429,CoursePlan.txt,0.0,"02456 Deep learning 2023 - course plan and information

Time: Mondays at 13:00-17:00 (first session is August 28th, 2023)

Locations: We will use the following rooms - building/room - (Campus map):

B303A-A042

B303A-046

B303A-047

B303A-048

B303A-HOEST

Zoom (You need to sign-in with you DTU account)

We use flipped classroom teaching. During the weeks with labs, the teachers and teaching assistants will circulate between the rooms so there will be opportunity to meet all. Any short lectures/instructions will be repeated in all rooms. You are free to choose whatever room you prefer of course respecting the limits on room capacity. During the weeks with project work each room will cover specific topics.

If you are not able to be on campus or prefer to work remotely you will be able to participate through Zoom. One teaching assistant will be dedicated to the Zoom channel: Zoom.

We also use Slack for communication: We will make dedicated channels for labs and projects. Here is a Slack invite link. (In Slack you can add channels from the list of channels by clicking the “+” next to Channels in the left panel and click “Browse channels” to choose.)

Bring a laptop.

The first eight weeks of the course will be dedicated to lab work. There will be a brief introduction to the course at the first session and a number of dedicated meetings online or in person with project supervisors.

Teachers

    Ole Winther
    Jes Frellsen

Teaching assistants",0.9027102589607239,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
What are the expectations regarding the final project report and presentation?,"Structure and write a final short technical report including problem formulation, description of methods, experiments, evaluation and conclusion. Organize and present project results at the final project presentation and in report.", LearningObjectives.txt,CoursePlan.txt,7.0,"Final report deadline December 21st at 23:59. [Note this was earlier set to a later date but according to DTU rules, the latest allowed deadline is December 21st.] The report should be a maximum 6 pages plus references using this conference paper format. The report should also contain a link to your project code Github repository. Among the files in the repository should be a jupyter notebook that ideally should recreate the main results of your report. If some of your data is confidential then use some shareable data instead. For MSc students, please also include your poster in the submission.",0.5919259786605835,CoursePlan.txt,3.0,"Week 9-13 will be only project work

In the seven project weeks we will still meet on Mondays for project work and supervision.
Evaluation and peer grading during the course

​​Evaluation:

    The course is graded using the 7-step scale.
    The final grade is based solely on the evaluation of the final project, which starts in the 7th week of the course. The project group should consist of 3-4 students. In special circumstances we can also accept groups of 1 or 2 students. (In the course catalogue it says 1-3 students. We will correct that for next year but cannot change it now.)
    The evaluation of the final project is based on two parts, both of which are done in groups but evaluated individually:

    a poster exam presentation, where the project groups document the results of their project in a poster and present to two or more teachers acting as examiners and
    a report in which the project groups document their solution. The report should be a maximum of 6 pages plus references using this conference paper format.

More details are given below.

    The student gains access to the final project by passing 6 out of 8 lab sessions that precede it.
    A lab session is passed by:

    grading the reports from lab sessions of 3 other students on Peergrade and
    passing the lab as judged by the teacher. More details given below.",0.6316696405410767,LearningObjectives.txt,0.0,"Demonstrate knowledge of machine learning terminology such as likelihood function, maximum likelihood, Bayesian inference, feed-forward, convolutional and Transformer neural networks, and error back propagation.

Understand and explain the choices and limitations of a model for a given setting.

Apply and analyze results from deep learning models in exercises and own project work.

Plan, delimit and carry out an applied or methods-oriented project in collaboration with fellow students and project supervisor.

Assess and summarize the project results in relation to aims, methods and available data.

Carry out the project and interpret results by use of computational framework for GPU programming such as PyTorch.

Structure and write a final short technical report including problem formulation, description of methods, experiments, evaluation and conclusion.

Organize and present project results at the final project presentation and in report.

Read, evaluate and give feedback to work of other students.",0.7027618885040283,CoursePlan.txt,5.0,"Week 1 computer exercise. Deadline: Monday week 2.
    Week 2 computer exercise. Deadline: Monday week 3.
    Week 3 computer exercise and 1 exercise of your own choice from course material week 1. Deadline: Monday week 4
    Week 4 computer exercise  and 1 exercise of your own choice from course material week 1-2. Deadline: Monday week 5.
    Week 5 computer exercise. Deadline: Monday week 6.
    Week 6 computer exercise. Deadline: Monday week 7.
    Week 7 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 8
    Week 8 computer exercise  and 1 exercise of your own choice from course material week 1-3. Deadline: Monday week 9.
    Project selection. Deadline Friday, Oct 20th 2023 at 23.59.
    Link to 2023 project selection sheet
    Project synopsis. Deadline: Monday week 9 at 23:59. The synopsis should be approximately half a page and maximum one page with a project title, motivation, background, milestones and references. It is important that the plan is realistic. The main purposes of the synopsis are to make sure the project size is well-calibrated and is concrete enough to start working from day one. The synopsis will not be used in the evaluation. The synopsis should be sent to your project supervisor.",0.8008854389190674,CoursePlan.txt,6.0,"Project poster session. PhD students taking the course as part of their PhD will not have to make a poster and take part of the poster session. In mixed groups of PhD and non-PhD students, only the non-PhD students have to take part in the poster session. The exam date is December 7th from 9 to 17. We divide the day into half hour slots and your group will later be given the possibility to register for a slot. A link to sign up for the poster session will appear here in due time. So having another exam on the same day should not be a problem. We will also organise an extra exam date for those of you who cannot make it on the date. It will be group poster presentations. We will invite outside guests and we will walk around and ask questions to all groups. We will make a schedule for when the teachers visit your poster. Plan for a 2 minute presentation per group member and 1-2 minutes for questions. The remainder of the time you can either present your poster to other students and guests or go visit other posters. Remember that it is important for the overall impression that you divide the presentation and answering of the questions more or less equally between you. The poster should be in A1 format. Remember to put both your names and student numbers under title. Here and here are links to examples using the latex template and here is one in powerpoint. You do not have to use that. The DTU library offers poster printing for a not too high price.",0.8043417930603027,notebook 4_2,9.0,"'markdown' cell: '['## Test the network', '', 'Now we show a batch of test images and generate a table below with the true and predicted class for each of these images.']'

 'code' cell: '['inputs, targets = next(iter(test_loader))', 'inputs, targets = inputs.to(device), targets.to(device)', 'show_image(make_grid(inputs))', 'plt.show()', '', 'outputs = model(inputs)', '_, predicted = torch.max(outputs.data, 1)', '', 'print(""    TRUE        PREDICTED"")', 'print(""-----------------------------"")', 'for target, pred in zip(targets, predicted):', '    print(f""{classes[target.item()]:^13} {classes[pred.item()]:^13}"")']'

 'markdown' cell: '['We now evaluate the network as above, but on the entire test set.']'",0.8505872488021851,notebook 5_1,35.0,"'markdown' cell: '['**Exercise 1**: Explain in your own words what the plot shows. How would it look if we had a leakage of information from the future to the present?', '', '> *Insert your answer here.*']'

 'markdown' cell: '['**Exercise 2**: Implement the loss of the RNN language model.', '', '> answer in the code below.']'",0.8531253337860107,notebook 4_2,10.0,"'markdown' cell: '['We now evaluate the network as above, but on the entire test set.']'

 'code' cell: '['# Evaluate test set', 'confusion_matrix = np.zeros((n_classes, n_classes))', 'with torch.no_grad():', '    model.eval()', '    test_accuracies = []', '    for inputs, targets in test_loader:', '        inputs, targets = inputs.to(device), targets.to(device)', '        output = model(inputs)', '        loss = loss_fn(output, targets)', '', '        predictions = output.max(1)[1]', '', '        # Multiply by len(inputs) because the final batch of DataLoader may be smaller (drop_last=True).', '        test_accuracies.append(accuracy(targets, predictions) * len(inputs))', '        ', '        confusion_matrix += compute_confusion_matrix(targets, predictions)', '', '    test_accuracy = np.sum(test_accuracies) / len(test_set)', '    ', '    model.train()']'

 'markdown' cell: '['Here we report the **average test accuracy** (number of correct predictions divided by test set size).']'

 'code' cell: '['print(f""Test accuracy: {test_accuracy:.3f}"")']'",0.8603349924087524,notebook 4_1,12.0,"'        ', '            # Compute accuracies on validation set.', '            valid_accuracies_batches = []', '            with torch.no_grad():', '                model.eval()', '                for inputs, targets in valid_loader:', '                    output = model(inputs)', '                    loss = loss_fn(output, targets)', '', '                    predictions = output.max(1)[1]', '', '                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).', '                    valid_accuracies_batches.append(accuracy_score(targets, predictions) * len(inputs))', '', '                model.train()', '                ', '            # Append average validation accuracy to list.', '",0.8670190572738647,notebook 5_1,38.0,"<- YOUR CODE HERE ', '', '            # backward and optimize', '            loss.backward()', '            optimiser.step()', '            step += 1', '            pbar.update(1)', '', '            # Report', '            if step % 5 ==0 :', '                loss = loss.detach().cpu()', '                pbar.set_description(f""epoch={epoch}, step={step}, loss={loss:.1f}"")', '', '            # save checkpoint', '            if step % 50 ==0 :', '                torch.save(rnn.state_dict(), checkpoint_file)', '            if step >= num_steps:', '                break', '        epoch += 1']'",0.8729924559593201,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0
What is the schedule for the '02456 Deep Learning 2023' course and what teaching method is used?,"Time: Mondays at 13:00-17:00 (first session is August 28th, 2023)... We use flipped classroom teaching.", CoursePlan.txt,CoursePlan.txt,0.0,"02456 Deep learning 2023 - course plan and information

Time: Mondays at 13:00-17:00 (first session is August 28th, 2023)

Locations: We will use the following rooms - building/room - (Campus map):

B303A-A042

B303A-046

B303A-047

B303A-048

B303A-HOEST

Zoom (You need to sign-in with you DTU account)

We use flipped classroom teaching. During the weeks with labs, the teachers and teaching assistants will circulate between the rooms so there will be opportunity to meet all. Any short lectures/instructions will be repeated in all rooms. You are free to choose whatever room you prefer of course respecting the limits on room capacity. During the weeks with project work each room will cover specific topics.

If you are not able to be on campus or prefer to work remotely you will be able to participate through Zoom. One teaching assistant will be dedicated to the Zoom channel: Zoom.

We also use Slack for communication: We will make dedicated channels for labs and projects. Here is a Slack invite link. (In Slack you can add channels from the list of channels by clicking the “+” next to Channels in the left panel and click “Browse channels” to choose.)

Bring a laptop.

The first eight weeks of the course will be dedicated to lab work. There will be a brief introduction to the course at the first session and a number of dedicated meetings online or in person with project supervisors.

Teachers

    Ole Winther
    Jes Frellsen

Teaching assistants",0.3498866558074951,CoursePlan.txt,9.0,"During this week and the following two weeks read Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapters 1-3 (stop when reaching the section called Overfitting and regularization) and browse Chapter 4. Note that this is reading material for the first three weeks of the course. Also, in total six exercises of your own choice will be homework later in the course.
    Alternative textbooks: All topics are also covered in the deep learning book that may be read as a supplement. The book can also be bought from the DTU bookstore. You will get 10% discount with this link. Feed-forward neural networks are covered in this chapter. Chapter 1 gives an introduction to deep learning and Part II gives the necessary background on linear algebra, probability, numerical computation and machine learning. Alternative textbook 2: Chris Bishop, Pattern recognition and machine learning. If you need to up your game in mathematics, the book Mathematics for machine learning is an excellent resource and the note Mathematics for Machine Learning offers a concise and compressed collection of the mathematical concepts used in deep learning. These resources are freely available online and are very valuable sources of information.
    Install software on your laptop or go directly to Google CoLab (see above). Installation guide for laptop and cloud may be found here.",0.5005208849906921,CourseOutline.txt,0.0,"The purpose of this course is to give the student a detailed understanding of the deep artificial neural network models, their training, computational frameworks for deployment on fast graphical processing units, their limitations and how to formulate learning in a diverse range of settings. These settings include classification, regression, sequences and other types of structured input and outputs and for reasoning in complex environments.

The course outline is:
1. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part I do it yourself on pen and paper.
2. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part II do it yourself in NumPy.
3. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part III PyTorch.
4. Convolutional neural networks (CNN) + presentation of student projects.
5. Sequence modelling for text data with Transformers.
6. Tricks of the trade and data science with PyTorch + Start of student projects.
7. Variational learning and generative adversarial networks for unsupervised and semi-supervised learning.
8. Reinforcement learning - policy gradient and deep Q-learning.

Starting from week 6 and full time from week 9 and the rest of the term will be spent on tutored project work.",0.5075609683990479,LearningObjectives.txt,0.0,"Demonstrate knowledge of machine learning terminology such as likelihood function, maximum likelihood, Bayesian inference, feed-forward, convolutional and Transformer neural networks, and error back propagation.

Understand and explain the choices and limitations of a model for a given setting.

Apply and analyze results from deep learning models in exercises and own project work.

Plan, delimit and carry out an applied or methods-oriented project in collaboration with fellow students and project supervisor.

Assess and summarize the project results in relation to aims, methods and available data.

Carry out the project and interpret results by use of computational framework for GPU programming such as PyTorch.

Structure and write a final short technical report including problem formulation, description of methods, experiments, evaluation and conclusion.

Organize and present project results at the final project presentation and in report.

Read, evaluate and give feedback to work of other students.",0.5363016128540039,CoursePlan.txt,17.0,"and take notes for at least 3 questions to ask. Link to lecture slides 2016 slides and 2017 slides and 2020 slides.

    Reading material DL Chapter 14 and 20.10.3. (Further learning a course dedicated to generative modelling.)
    One exercise from the book chapters.
    Carry out computer exercises week 7 on autoencoder un- and semi-supervised. Hand in and peergrade on peergrade.io like in previous weeks.
    Project selection deadline is this week (see above).

Week 8 - Reinforcement learning 

    Watch week 6 video lectures 

    02456week6 1 1 reinforcement learning
    02456week6 1 2 reinforcement learning approaches
    02456week6 2 1 AlphaGo policy and value networks
    02456week6 2 2 AlphaGo steps 1 to 4
    02456week6 3 policy gradients
    02456week6 4 a few last words
    2017 Deep Q learning
    2017 Evolutionary strategies

and take notes for at least 3 questions to ask. Link to lectures here and here for 2017 update.

    Reading: another nice blog post by Andrei Karpathy. Optional reading material on the connection between variational and reinforcement learning.
    One exercise from the book chapters. 
    Computer exercises on reinforcement learning methods (policy gradient, deep Q learning, evolutionary strategies) in the openAI Gym. Carry out exercises week 8. Hand in and peergrade on peergrade.io like in previous weeks.
    Project work.",0.5426092743873596,CoursePlan.txt,2.0,"You can also sign up to Google cloud (GCP) and get free credits there. Here are instructions on how to set up GPU computation on Google cloud. Follow the instructions until Optional.
Topics in the first eight weeks

    Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part I do it yourself on pen and paper.
    Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part II do it yourself in Python.
    Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part III PyTorch.
    Convolutional neural networks (CNN)
    Transformers and recurrent neural networks (RNN)
    Tricks of the trade and data science with PyTorch
    Variational learning and generative adversarial networks for unsupervised and semi-supervised learning + deadline for selection of student projects on Friday, Oct 13th 2023 at 23.59.
    Reinforcement learning - policy gradient and deep Q-learning + start of student projects. 

Week 9-13 will be only project work

In the seven project weeks we will still meet on Mondays for project work and supervision.
Evaluation and peer grading during the course

​​Evaluation:",0.5474171042442322,CoursePlan.txt,15.0,"Week 6 - Tricks of the trade and data science challenge

    Watch week 4 video lectures 

    02456week4 1 1 Initialization and gradient clipping 
    02456week4 1 2 batch normalization
    02456week4 2 1 regularization
    02456week4 2 2 regularization methods
    02456week4 2 3 data augmentation
    02456week4 2 4 ensemble methods and dropout
    02456week4 3 recap
    2017 37 reasons your nn working (part 1 of 2) Walk through of the 37 reasons why your neural network is not working blog post.
    2017 37 reasons you not working (part 2 of 2)
    2020 Recipe to training neural networks - become one with data (part 1 of 3).
    2020 Recipe to training neural networks - baselines (part 2 of 3).
    2020 Recipe to training neural networks - overfit, tune and tune some more (part 3 of 3).

and take notes for at least 3 questions to ask. Link to lecture slides 2016 lecture slides, 2017 blog post and 2020 lecture slides.",0.5519322156906128,CoursePlan.txt,12.0,"and take notes for at least 3 questions to ask. Link to lecture slides is here and here for 2017 updates.

    Reading material Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapter 6 (stop when reaching section called Other approaches to deep neural nets).
    Alternative textbook chapter in the deep learning book.
    One exercise from the book chapters.
    Carry out computer exercises week 4.
    Hand in the notebook marked with EXE on peergrade.io.
    Peergrade exercise from three other students through peergrade.io. You will receive instructions about this from peergrade.io.

Week 5 - Transformers and recurrent neural networks

    Watch week 3 video lectures",0.5593087673187256,CoursePlan.txt,16.0,"and take notes for at least 3 questions to ask. Link to lecture slides 2016 lecture slides, 2017 blog post and 2020 lecture slides.  

    Reading material Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapter 3 from section Overfitting and regularization and Chapter 5.
    Alternative textbook chapters on regularization, optimization, deep learning practice and applications from the deep learning book.  
    Additional material: Andrei Karpathy blogpost on how to approach a data science problem with deep learning, blogpost on things that can go wrong in neural network training and interactive initialization demo.
    Computer exercises week 6 using PyTorch on the Kaggle competition leaf classification. Hand in and peergrade on peergrade.io like in previous weeks.

Week 7 - Un- and semi-supervised learning

    Watch week 5 video lectures

    02456week5 1 1 unsupervised learning
    02456week5 1 2 unsupervised learning latent variables
    02456week5 2 1 autoencoders
    02456week5 2 2 autoencoders layerwise pretraining
    02456week5 3 1 variational autoencoders
    02456week5 3 2 semi-supervised variational autoencoders 
    2017 Generative adversarial networks
    2020 Flows
    2020 Self-supervised learning
    2020 Self-training/noisy student
    2020 Distribution Augmentation
    2020 Flat minima",0.5711426138877869,CoursePlan.txt,13.0,"Week 5 - Transformers and recurrent neural networks

    Watch week 3 video lectures

    02456week3 1 RNN (PART 1 of 3)
    02456week3 1 RNN (PART 2 of 3)
    02456week3 1 RNN (PART 3 of 3)
    02456week3.2_RNN_training (PART 1 of 3)
    02456week3.2_RNN_training (PART 2 of 3)
    02456week3 2 RNN training (PART 3 of 3)
    02456week3 3 Attention (PART 1 of 2)
    02456week3 3 Attention (PART 2 of 2)
    02456week3 4 Supervised learning recap
    2017 Quasi RNN
    2017 Non-recurrent sequence to sequence models
    2017 Text summarization
    2020 Transformers (PART 1 of 2)
    2020 Transformers (PART 2 of 2)
    2020 Language modelling - GPT-2 and 3
    2020 BERT

and take notes for at least 3 questions to ask. Link to: 2016 lectures, 2017 lecture updates and 2020 lecture updates.",0.58079993724823,1.0,2.0,2.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0
What are the requirements for students to gain access to the final project in the '02456 Deep Learning 2023' course?,The student gains access to the final project by passing 6 out of 8 lab sessions that precede it. A lab session is passed by: grading the reports from lab sessions of 3 other students on Peergrade and passing the lab as judged by the teacher, CoursePlan.txt,CoursePlan.txt,0.0,"02456 Deep learning 2023 - course plan and information

Time: Mondays at 13:00-17:00 (first session is August 28th, 2023)

Locations: We will use the following rooms - building/room - (Campus map):

B303A-A042

B303A-046

B303A-047

B303A-048

B303A-HOEST

Zoom (You need to sign-in with you DTU account)

We use flipped classroom teaching. During the weeks with labs, the teachers and teaching assistants will circulate between the rooms so there will be opportunity to meet all. Any short lectures/instructions will be repeated in all rooms. You are free to choose whatever room you prefer of course respecting the limits on room capacity. During the weeks with project work each room will cover specific topics.

If you are not able to be on campus or prefer to work remotely you will be able to participate through Zoom. One teaching assistant will be dedicated to the Zoom channel: Zoom.

We also use Slack for communication: We will make dedicated channels for labs and projects. Here is a Slack invite link. (In Slack you can add channels from the list of channels by clicking the “+” next to Channels in the left panel and click “Browse channels” to choose.)

Bring a laptop.

The first eight weeks of the course will be dedicated to lab work. There will be a brief introduction to the course at the first session and a number of dedicated meetings online or in person with project supervisors.

Teachers

    Ole Winther
    Jes Frellsen

Teaching assistants",0.4220711588859558,LearningObjectives.txt,0.0,"Demonstrate knowledge of machine learning terminology such as likelihood function, maximum likelihood, Bayesian inference, feed-forward, convolutional and Transformer neural networks, and error back propagation.

Understand and explain the choices and limitations of a model for a given setting.

Apply and analyze results from deep learning models in exercises and own project work.

Plan, delimit and carry out an applied or methods-oriented project in collaboration with fellow students and project supervisor.

Assess and summarize the project results in relation to aims, methods and available data.

Carry out the project and interpret results by use of computational framework for GPU programming such as PyTorch.

Structure and write a final short technical report including problem formulation, description of methods, experiments, evaluation and conclusion.

Organize and present project results at the final project presentation and in report.

Read, evaluate and give feedback to work of other students.",0.44861432909965515,CourseOutline.txt,0.0,"The purpose of this course is to give the student a detailed understanding of the deep artificial neural network models, their training, computational frameworks for deployment on fast graphical processing units, their limitations and how to formulate learning in a diverse range of settings. These settings include classification, regression, sequences and other types of structured input and outputs and for reasoning in complex environments.

The course outline is:
1. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part I do it yourself on pen and paper.
2. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part II do it yourself in NumPy.
3. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part III PyTorch.
4. Convolutional neural networks (CNN) + presentation of student projects.
5. Sequence modelling for text data with Transformers.
6. Tricks of the trade and data science with PyTorch + Start of student projects.
7. Variational learning and generative adversarial networks for unsupervised and semi-supervised learning.
8. Reinforcement learning - policy gradient and deep Q-learning.

Starting from week 6 and full time from week 9 and the rest of the term will be spent on tutored project work.",0.548279881477356,CoursePlan.txt,2.0,"You can also sign up to Google cloud (GCP) and get free credits there. Here are instructions on how to set up GPU computation on Google cloud. Follow the instructions until Optional.
Topics in the first eight weeks

    Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part I do it yourself on pen and paper.
    Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part II do it yourself in Python.
    Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part III PyTorch.
    Convolutional neural networks (CNN)
    Transformers and recurrent neural networks (RNN)
    Tricks of the trade and data science with PyTorch
    Variational learning and generative adversarial networks for unsupervised and semi-supervised learning + deadline for selection of student projects on Friday, Oct 13th 2023 at 23.59.
    Reinforcement learning - policy gradient and deep Q-learning + start of student projects. 

Week 9-13 will be only project work

In the seven project weeks we will still meet on Mondays for project work and supervision.
Evaluation and peer grading during the course

​​Evaluation:",0.5536284446716309,CoursePlan.txt,9.0,"During this week and the following two weeks read Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapters 1-3 (stop when reaching the section called Overfitting and regularization) and browse Chapter 4. Note that this is reading material for the first three weeks of the course. Also, in total six exercises of your own choice will be homework later in the course.
    Alternative textbooks: All topics are also covered in the deep learning book that may be read as a supplement. The book can also be bought from the DTU bookstore. You will get 10% discount with this link. Feed-forward neural networks are covered in this chapter. Chapter 1 gives an introduction to deep learning and Part II gives the necessary background on linear algebra, probability, numerical computation and machine learning. Alternative textbook 2: Chris Bishop, Pattern recognition and machine learning. If you need to up your game in mathematics, the book Mathematics for machine learning is an excellent resource and the note Mathematics for Machine Learning offers a concise and compressed collection of the mathematical concepts used in deep learning. These resources are freely available online and are very valuable sources of information.
    Install software on your laptop or go directly to Google CoLab (see above). Installation guide for laptop and cloud may be found here.",0.5682278275489807,CoursePlan.txt,17.0,"and take notes for at least 3 questions to ask. Link to lecture slides 2016 slides and 2017 slides and 2020 slides.

    Reading material DL Chapter 14 and 20.10.3. (Further learning a course dedicated to generative modelling.)
    One exercise from the book chapters.
    Carry out computer exercises week 7 on autoencoder un- and semi-supervised. Hand in and peergrade on peergrade.io like in previous weeks.
    Project selection deadline is this week (see above).

Week 8 - Reinforcement learning 

    Watch week 6 video lectures 

    02456week6 1 1 reinforcement learning
    02456week6 1 2 reinforcement learning approaches
    02456week6 2 1 AlphaGo policy and value networks
    02456week6 2 2 AlphaGo steps 1 to 4
    02456week6 3 policy gradients
    02456week6 4 a few last words
    2017 Deep Q learning
    2017 Evolutionary strategies

and take notes for at least 3 questions to ask. Link to lectures here and here for 2017 update.

    Reading: another nice blog post by Andrei Karpathy. Optional reading material on the connection between variational and reinforcement learning.
    One exercise from the book chapters. 
    Computer exercises on reinforcement learning methods (policy gradient, deep Q learning, evolutionary strategies) in the openAI Gym. Carry out exercises week 8. Hand in and peergrade on peergrade.io like in previous weeks.
    Project work.",0.5944523811340332,CoursePlan.txt,12.0,"and take notes for at least 3 questions to ask. Link to lecture slides is here and here for 2017 updates.

    Reading material Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapter 6 (stop when reaching section called Other approaches to deep neural nets).
    Alternative textbook chapter in the deep learning book.
    One exercise from the book chapters.
    Carry out computer exercises week 4.
    Hand in the notebook marked with EXE on peergrade.io.
    Peergrade exercise from three other students through peergrade.io. You will receive instructions about this from peergrade.io.

Week 5 - Transformers and recurrent neural networks

    Watch week 3 video lectures",0.6015702486038208,CoursePlan.txt,11.0,"Week 2 - Feed-forward neural networks - do it yourself in NumPy

    See 1. and 2. from Week 1.
    Carry out computer exercises week 2.
    Peergrade exercise from three other students through peergrade.io. 

Week 3 - Feed-forward neural networks in PyTorch

    See 1. and 2. from Week 1.
    Carry out computer exercises week 3.
    Peergrade exercise from three other students through peergrade.io.
    Hand in the notebook marked with EXE on peergrade.io. It should contain your added code in the exercises and the answer of one exercise from Michael Nielsen's book (see point 3. above). The answer to the book exercise should be in a markdown cell at the bottom of the notebook.
    Peergrade exercise from three other students through peergrade.io.  

Week 4 - Convolutional neural networks

    Watch week 2 video lectures  

    Part 1 Introduction to CNNs (PART 1/2)
    Part 1 Introduction to CNNs (PART 2/2)
    Part 2 CNNs the details (PART 1/2)
    Part 2 CNNs the details (PART 2/2)
    2017 CNN update
    2017 Activation functions update
    2017 Image segmentation

and take notes for at least 3 questions to ask. Link to lecture slides is here and here for 2017 updates.",0.6353549957275391,notebook 5_2,1.0,"How to build and train an RNN in Nanograd', '* How to build and train an LSTM network in Nanograd', '* How to build and train an LSTM network in PyTorch', '', '', '[Numpy version of the Notebook (previous version)](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/5_Recurrent/OLD-5.1-Numpy-Recurrent-Neural-Networks.ipynb)']'",0.6384862661361694,CoursePlan.txt,16.0,"and take notes for at least 3 questions to ask. Link to lecture slides 2016 lecture slides, 2017 blog post and 2020 lecture slides.  

    Reading material Michael Nielsen, Neural networks and deep learning http://neuralnetworksanddeeplearning.com/ Chapter 3 from section Overfitting and regularization and Chapter 5.
    Alternative textbook chapters on regularization, optimization, deep learning practice and applications from the deep learning book.  
    Additional material: Andrei Karpathy blogpost on how to approach a data science problem with deep learning, blogpost on things that can go wrong in neural network training and interactive initialization demo.
    Computer exercises week 6 using PyTorch on the Kaggle competition leaf classification. Hand in and peergrade on peergrade.io like in previous weeks.

Week 7 - Un- and semi-supervised learning

    Watch week 5 video lectures

    02456week5 1 1 unsupervised learning
    02456week5 1 2 unsupervised learning latent variables
    02456week5 2 1 autoencoders
    02456week5 2 2 autoencoders layerwise pretraining
    02456week5 3 1 variational autoencoders
    02456week5 3 2 semi-supervised variational autoencoders 
    2017 Generative adversarial networks
    2020 Flows
    2020 Self-supervised learning
    2020 Self-training/noisy student
    2020 Distribution Augmentation
    2020 Flat minima",0.6418927907943726,1.0,1.0,1.0,2.0,3.0,4.0,5.0,6.0,6.0,7.0
