{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langchain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    NotebookLoader,\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    \n",
    ")\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import utils\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadingDocuments import read_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is PyTorch?</td>\n",
       "      <td>It’s a Python based scientific computing packa...</td>\n",
       "      <td>notebook 3_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the MNIST dataset?</td>\n",
       "      <td>MNIST is a dataset that is often used for benc...</td>\n",
       "      <td>notebook 3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which optimizers are mentioned in the exercise...</td>\n",
       "      <td>Optimizer and learning rate:\\nSGD + Momentum: ...</td>\n",
       "      <td>notebook 3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe the model given in the exercise noteb...</td>\n",
       "      <td>The provided code defines a PyTorch neural net...</td>\n",
       "      <td>notebook 3_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the initial assignment in exercise not...</td>\n",
       "      <td>The first task is to use Kaiming He initializa...</td>\n",
       "      <td>notebook 3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What do we expect to learn from week4?</td>\n",
       "      <td>In this lab, we will learn how to create your ...</td>\n",
       "      <td>notebook 4_1, notebook 4_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is CIFAR-10 dataset?</td>\n",
       "      <td>The images in CIFAR-10 are RGB images (3 chann...</td>\n",
       "      <td>notebook 4_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are convolutional neural networks?</td>\n",
       "      <td>The standard ConvNets are organised into layer...</td>\n",
       "      <td>notebook 4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can you provide some suggestions to improve th...</td>\n",
       "      <td>Tell us something like increase the depth of t...</td>\n",
       "      <td>notebook 4_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What do RNN and LSTM stand for?</td>\n",
       "      <td>RNN stands for Reccurent Neural Network and LS...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How can I give text as input to my network?</td>\n",
       "      <td>Before text can be used as input for a neural ...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What variables are used in the attention funct...</td>\n",
       "      <td>The attention mechanism is defined using the q...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is sampling for a language model?</td>\n",
       "      <td>Sampling text means that the language model is...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is a rnn?</td>\n",
       "      <td>A recurrent neural network (RNN) is a type of ...</td>\n",
       "      <td>notebook 5_2, notebook 5_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What topics are covered in the first three wee...</td>\n",
       "      <td>. Introduction to statistical machine learning...</td>\n",
       "      <td>CourseOutline.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>When does project work start?</td>\n",
       "      <td>Starting from week 6 and full time from week 9...</td>\n",
       "      <td>CourseOutline.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How are students expected to communicate and e...</td>\n",
       "      <td>Organize and present project results at the fi...</td>\n",
       "      <td>LearningObjectives.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What are the expectations regarding the final ...</td>\n",
       "      <td>Structure and write a final short technical re...</td>\n",
       "      <td>LearningObjectives.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the schedule for the '02456 Deep Learn...</td>\n",
       "      <td>Time: Mondays at 13:00-17:00 (first session is...</td>\n",
       "      <td>CoursePlan.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What are the requirements for students to gain...</td>\n",
       "      <td>The student gains access to the final project ...</td>\n",
       "      <td>CoursePlan.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "0                                    What is PyTorch?   \n",
       "1                          What is the MNIST dataset?   \n",
       "2   Which optimizers are mentioned in the exercise...   \n",
       "3   Describe the model given in the exercise noteb...   \n",
       "4   What is the initial assignment in exercise not...   \n",
       "5              What do we expect to learn from week4?   \n",
       "6                           What is CIFAR-10 dataset?   \n",
       "7             What are convolutional neural networks?   \n",
       "8   Can you provide some suggestions to improve th...   \n",
       "9                     What do RNN and LSTM stand for?   \n",
       "10        How can I give text as input to my network?   \n",
       "11  What variables are used in the attention funct...   \n",
       "12             What is sampling for a language model?   \n",
       "13                                     What is a rnn?   \n",
       "14  What topics are covered in the first three wee...   \n",
       "15                      When does project work start?   \n",
       "16  How are students expected to communicate and e...   \n",
       "17  What are the expectations regarding the final ...   \n",
       "18  What is the schedule for the '02456 Deep Learn...   \n",
       "19  What are the requirements for students to gain...   \n",
       "\n",
       "                                               Answer  \\\n",
       "0   It’s a Python based scientific computing packa...   \n",
       "1   MNIST is a dataset that is often used for benc...   \n",
       "2   Optimizer and learning rate:\\nSGD + Momentum: ...   \n",
       "3   The provided code defines a PyTorch neural net...   \n",
       "4   The first task is to use Kaiming He initializa...   \n",
       "5   In this lab, we will learn how to create your ...   \n",
       "6   The images in CIFAR-10 are RGB images (3 chann...   \n",
       "7   The standard ConvNets are organised into layer...   \n",
       "8   Tell us something like increase the depth of t...   \n",
       "9   RNN stands for Reccurent Neural Network and LS...   \n",
       "10  Before text can be used as input for a neural ...   \n",
       "11  The attention mechanism is defined using the q...   \n",
       "12  Sampling text means that the language model is...   \n",
       "13  A recurrent neural network (RNN) is a type of ...   \n",
       "14  . Introduction to statistical machine learning...   \n",
       "15  Starting from week 6 and full time from week 9...   \n",
       "16  Organize and present project results at the fi...   \n",
       "17  Structure and write a final short technical re...   \n",
       "18  Time: Mondays at 13:00-17:00 (first session is...   \n",
       "19  The student gains access to the final project ...   \n",
       "\n",
       "                         Source  \n",
       "0                  notebook 3_1  \n",
       "1                  notebook 3_4  \n",
       "2                  notebook 3_4  \n",
       "3                  notebook 3_3  \n",
       "4                  notebook 3_4  \n",
       "5    notebook 4_1, notebook 4_2  \n",
       "6                  notebook 4_2  \n",
       "7                  notebook 4_1  \n",
       "8                  notebook 4_2  \n",
       "9                  notebook 5_1  \n",
       "10                 notebook 5_1  \n",
       "11                 notebook 5_1  \n",
       "12                 notebook 5_1  \n",
       "13   notebook 5_2, notebook 5_3  \n",
       "14            CourseOutline.txt  \n",
       "15            CourseOutline.txt  \n",
       "16       LearningObjectives.txt  \n",
       "17       LearningObjectives.txt  \n",
       "18               CoursePlan.txt  \n",
       "19               CoursePlan.txt  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_path = Path('eval_data/evaluation_dataset.csv')\n",
    "read_data(qa_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test/split course description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knowledgeBase\\\\GeneralInformation\\\\CourseOutline.txt', 'knowledgeBase\\\\GeneralInformation\\\\CoursePlan.txt', 'knowledgeBase\\\\GeneralInformation\\\\LearningObjectives.txt']\n"
     ]
    }
   ],
   "source": [
    "# get all files name from .\\knowledgeBase\\GeneralInformation\n",
    "path = Path(\".\\knowledgeBase\\GeneralInformation\")\n",
    "files = path.glob('**/*.txt')\n",
    "files = [str(x) for x in files]\n",
    "print(files)\n",
    "total_info_docs = []\n",
    "for file in files:\n",
    "    loader = TextLoader(file)\n",
    "    total_info_docs.append(loader.load()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: knowledgeBase\\GeneralInformation\\CourseOutline.txt  has 274 tokens\n",
      "Doc: knowledgeBase\\GeneralInformation\\CoursePlan.txt  has 4477 tokens\n",
      "Doc: knowledgeBase\\GeneralInformation\\LearningObjectives.txt  has 174 tokens\n"
     ]
    }
   ],
   "source": [
    "for doc in total_info_docs:\n",
    "    print(f\"Doc: {doc.metadata['source']}  has {utils.get_tokens_count(doc.page_content)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_tokens = 350\n",
    "overlap_tokens = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_tokens,\n",
    "            chunk_overlap=overlap_tokens,\n",
    "            length_function=utils.get_tokens_count, #len\n",
    "            add_start_index=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info doc knowledgeBase\\GeneralInformation\\CourseOutline.txt has 1 chunks\n",
      "Info doc knowledgeBase\\GeneralInformation\\CoursePlan.txt has 18 chunks\n",
      "Info doc knowledgeBase\\GeneralInformation\\LearningObjectives.txt has 1 chunks\n"
     ]
    }
   ],
   "source": [
    "total_info_chunks =  []\n",
    "for doc in total_info_docs:\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk.metadata['id'] = idx\n",
    "    total_info_chunks.extend(chunks)\n",
    "    print(\"Info doc {} has {} chunks\".format(doc.metadata['source'], len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test/split notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_paths = [\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_1_EXE_deep_learning_with_transformers.ipynb\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb\",\n",
    "    r\"knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.1-autoencoder.ipynb\",\n",
    "    r\"knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.2-EXE-variational-autoencoder.ipynb\",\n",
    "    r\"knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.3-generative-adversarial-networks.ipynb\",\n",
    "    r\"knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.4-SUPP-flow-models.ipynb\",\n",
    "    #r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.1_Introduction.ipynb\",\n",
    "    r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.2_Prerequisites.ipynb\",\n",
    "    r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.3-EXE_Policy_Gradient.ipynb\",\n",
    "    r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.4_Q-Network.ipynb\",\n",
    "    r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.5_Deep_Q-network.ipynb\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb has 1451 tokens\n",
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb has 1154 tokens\n",
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb has 4221 tokens\n",
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb has 3678 tokens\n",
      "Notebok: knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb has 3552 tokens\n",
      "Notebok: knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb has 3298 tokens\n",
      "Notebok: knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb has 3939 tokens\n",
      "Notebok: knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_1_EXE_deep_learning_with_transformers.ipynb has 19766 tokens\n",
      "Notebok: knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb has 14619 tokens\n",
      "Notebok: knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb has 17289 tokens\n",
      "Notebok: knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.1-autoencoder.ipynb has 4035 tokens\n",
      "Notebok: knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.2-EXE-variational-autoencoder.ipynb has 14696 tokens\n",
      "Notebok: knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.3-generative-adversarial-networks.ipynb has 3891 tokens\n",
      "Notebok: knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.4-SUPP-flow-models.ipynb has 3631 tokens\n",
      "Notebok: knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.2_Prerequisites.ipynb has 797 tokens\n",
      "Notebok: knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.3-EXE_Policy_Gradient.ipynb has 3355 tokens\n",
      "Notebok: knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.4_Q-Network.ipynb has 2007 tokens\n",
      "Notebok: knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.5_Deep_Q-network.ipynb has 2898 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n"
     ]
    }
   ],
   "source": [
    "total_notebook_docs = []\n",
    "for notebook_path in notebook_paths:\n",
    "    notebook_path = Path(notebook_path)\n",
    "\n",
    "    loader = NotebookLoader(str(notebook_path), include_outputs=False, max_output_length=20, remove_newline=True)\n",
    "    doc_notebook = loader.load()[0]\n",
    "    total_notebook_docs.append(doc_notebook)\n",
    "    print(\"Notebok: {} has {} tokens\".format(notebook_path, utils.get_tokens_count(doc_notebook.page_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_notebook_content(content):\n",
    "\n",
    "    cells = re.split(r\"\\'(markdown|code)\\' cell: \", content)[1:]\n",
    "\n",
    "    parsed_content = []\n",
    "    for i in range(0, len(cells), 2):\n",
    "        cell_type = cells[i]\n",
    "        cell_content = cells[i + 1]\n",
    "\n",
    "        \n",
    "\n",
    "        cell_content = cells[i + 1][3:-20] + cells[i + 1][-20:].replace(\"']'\", \"'\")\n",
    "        cell_items_list = cell_content.replace(\"\\\\n\",\"\").split(\"', '\") \n",
    "        \n",
    "        # Append the cell type and content to the parsed content\n",
    "        parsed_content.append({'type': cell_type, 'content': cell_items_list})\n",
    "    return parsed_content\n",
    "\n",
    "\n",
    "def get_parsed_notebook_text(parsed_content):\n",
    "    parsed_text = []\n",
    "    for item in parsed_content:\n",
    "        parsed_text.append(item['type']+ \":\" + \"\\n\")\n",
    "        for content_text in item['content']:\n",
    "            parsed_text.append(content_text + \"\\n\")\n",
    "    return \"\".join(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for notebook_doc in total_notebook_docs:\n",
    "#     notebook_doc.page_content = get_parsed_notebook_text(parse_notebook_content(notebook_doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb has 6 chunks\n",
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb has 5 chunks\n",
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb has 18 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb has 17 chunks\n",
      "Notebook knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb has 18 chunks\n",
      "Notebook knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb has 15 chunks\n",
      "Notebook knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb has 20 chunks\n",
      "Notebook knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_1_EXE_deep_learning_with_transformers.ipynb has 84 chunks\n",
      "Notebook knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb has 69 chunks\n",
      "Notebook knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb has 83 chunks\n",
      "Notebook knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.1-autoencoder.ipynb has 18 chunks\n",
      "Notebook knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.2-EXE-variational-autoencoder.ipynb has 58 chunks\n",
      "Notebook knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.3-generative-adversarial-networks.ipynb has 18 chunks\n",
      "Notebook knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.4-SUPP-flow-models.ipynb has 16 chunks\n",
      "Notebook knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.2_Prerequisites.ipynb has 3 chunks\n",
      "Notebook knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.3-EXE_Policy_Gradient.ipynb has 15 chunks\n",
      "Notebook knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.4_Q-Network.ipynb has 9 chunks\n",
      "Notebook knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.5_Deep_Q-network.ipynb has 14 chunks\n"
     ]
    }
   ],
   "source": [
    "total_notebook_chunks =  []\n",
    "for doc in total_notebook_docs:\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk.metadata['id'] = idx\n",
    "    total_notebook_chunks.extend(chunks)\n",
    "    print(\"Notebook {} has {} chunks\".format(doc.metadata['source'], len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openai embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 3369, which is longer than the specified 1000\n",
      "Created a chunk of size 2531, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1159, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1196, which is longer than the specified 1000\n",
      "Created a chunk of size 1283, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1894, which is longer than the specified 1000\n",
      "Created a chunk of size 3455, which is longer than the specified 1000\n",
      "Created a chunk of size 1038, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1180, which is longer than the specified 1000\n",
      "Created a chunk of size 2895, which is longer than the specified 1000\n",
      "Created a chunk of size 2774, which is longer than the specified 1000\n",
      "Created a chunk of size 1872, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1118, which is longer than the specified 1000\n",
      "Created a chunk of size 1696, which is longer than the specified 1000\n",
      "Created a chunk of size 1543, which is longer than the specified 1000\n",
      "Created a chunk of size 1173, which is longer than the specified 1000\n",
      "Created a chunk of size 1133, which is longer than the specified 1000\n",
      "Created a chunk of size 2773, which is longer than the specified 1000\n",
      "Created a chunk of size 1035, which is longer than the specified 1000\n",
      "Created a chunk of size 2238, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1104, which is longer than the specified 1000\n",
      "Created a chunk of size 2104, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1011, which is longer than the specified 1000\n",
      "Created a chunk of size 1207, which is longer than the specified 1000\n",
      "Created a chunk of size 3209, which is longer than the specified 1000\n",
      "Created a chunk of size 1052, which is longer than the specified 1000\n",
      "Created a chunk of size 2441, which is longer than the specified 1000\n",
      "Created a chunk of size 1211, which is longer than the specified 1000\n",
      "Created a chunk of size 2821, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 2153, which is longer than the specified 1000\n",
      "Created a chunk of size 3553, which is longer than the specified 1000\n",
      "Created a chunk of size 1417, which is longer than the specified 1000\n",
      "Created a chunk of size 1055, which is longer than the specified 1000\n",
      "Created a chunk of size 1416, which is longer than the specified 1000\n",
      "Created a chunk of size 3253, which is longer than the specified 1000\n",
      "Created a chunk of size 2618, which is longer than the specified 1000\n",
      "Created a chunk of size 1662, which is longer than the specified 1000\n",
      "Created a chunk of size 1440, which is longer than the specified 1000\n",
      "Created a chunk of size 1806, which is longer than the specified 1000\n",
      "Created a chunk of size 2400, which is longer than the specified 1000\n",
      "Created a chunk of size 1357, which is longer than the specified 1000\n",
      "Created a chunk of size 3029, which is longer than the specified 1000\n",
      "Created a chunk of size 1149, which is longer than the specified 1000\n",
      "Created a chunk of size 2399, which is longer than the specified 1000\n",
      "Created a chunk of size 3560, which is longer than the specified 1000\n",
      "Created a chunk of size 2718, which is longer than the specified 1000\n",
      "Created a chunk of size 2601, which is longer than the specified 1000\n",
      "Created a chunk of size 1870, which is longer than the specified 1000\n",
      "Created a chunk of size 2156, which is longer than the specified 1000\n",
      "Created a chunk of size 1404, which is longer than the specified 1000\n",
      "Created a chunk of size 2284, which is longer than the specified 1000\n",
      "Created a chunk of size 1826, which is longer than the specified 1000\n",
      "Created a chunk of size 2775, which is longer than the specified 1000\n",
      "Created a chunk of size 1398, which is longer than the specified 1000\n",
      "Created a chunk of size 2106, which is longer than the specified 1000\n",
      "Created a chunk of size 2552, which is longer than the specified 1000\n",
      "Created a chunk of size 1392, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1649, which is longer than the specified 1000\n",
      "Created a chunk of size 1472, which is longer than the specified 1000\n",
      "Created a chunk of size 1163, which is longer than the specified 1000\n",
      "Created a chunk of size 1947, which is longer than the specified 1000\n",
      "Created a chunk of size 2354, which is longer than the specified 1000\n",
      "Created a chunk of size 2152, which is longer than the specified 1000\n",
      "Created a chunk of size 1615, which is longer than the specified 1000\n",
      "Created a chunk of size 2355, which is longer than the specified 1000\n",
      "Created a chunk of size 2163, which is longer than the specified 1000\n",
      "Created a chunk of size 1844, which is longer than the specified 1000\n",
      "Created a chunk of size 1985, which is longer than the specified 1000\n",
      "Created a chunk of size 3370, which is longer than the specified 1000\n",
      "Created a chunk of size 1294, which is longer than the specified 1000\n",
      "Created a chunk of size 1496, which is longer than the specified 1000\n",
      "Created a chunk of size 1365, which is longer than the specified 1000\n",
      "Created a chunk of size 2251, which is longer than the specified 1000\n",
      "Created a chunk of size 1400, which is longer than the specified 1000\n",
      "Created a chunk of size 1709, which is longer than the specified 1000\n",
      "Created a chunk of size 3699, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1574, which is longer than the specified 1000\n",
      "Created a chunk of size 1328, which is longer than the specified 1000\n",
      "Created a chunk of size 1163, which is longer than the specified 1000\n",
      "Created a chunk of size 1949, which is longer than the specified 1000\n",
      "Created a chunk of size 2354, which is longer than the specified 1000\n",
      "Created a chunk of size 1488, which is longer than the specified 1000\n",
      "Created a chunk of size 2355, which is longer than the specified 1000\n",
      "Created a chunk of size 2410, which is longer than the specified 1000\n",
      "Created a chunk of size 1697, which is longer than the specified 1000\n",
      "Created a chunk of size 3512, which is longer than the specified 1000\n",
      "Created a chunk of size 3415, which is longer than the specified 1000\n",
      "Created a chunk of size 3446, which is longer than the specified 1000\n",
      "Created a chunk of size 2030, which is longer than the specified 1000\n",
      "Created a chunk of size 1365, which is longer than the specified 1000\n",
      "Created a chunk of size 2209, which is longer than the specified 1000\n",
      "Created a chunk of size 4030, which is longer than the specified 1000\n",
      "Created a chunk of size 5044, which is longer than the specified 1000\n",
      "Created a chunk of size 3648, which is longer than the specified 1000\n",
      "Created a chunk of size 3699, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw document...knowledgeBase\\GeneralInformation\\CourseOutline.txt\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\GeneralInformation\\CoursePlan.txt\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\GeneralInformation\\LearningObjectives.txt\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_1_EXE_deep_learning_with_transformers.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb\n",
      "Splitting text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1754, which is longer than the specified 1000\n",
      "Created a chunk of size 1075, which is longer than the specified 1000\n",
      "Created a chunk of size 1379, which is longer than the specified 1000\n",
      "Created a chunk of size 1111, which is longer than the specified 1000\n",
      "Created a chunk of size 1568, which is longer than the specified 1000\n",
      "Created a chunk of size 1201, which is longer than the specified 1000\n",
      "Created a chunk of size 2558, which is longer than the specified 1000\n",
      "Created a chunk of size 3430, which is longer than the specified 1000\n",
      "Created a chunk of size 2134, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 4257, which is longer than the specified 1000\n",
      "Created a chunk of size 1347, which is longer than the specified 1000\n",
      "Created a chunk of size 1531, which is longer than the specified 1000\n",
      "Created a chunk of size 2039, which is longer than the specified 1000\n",
      "Created a chunk of size 1110, which is longer than the specified 1000\n",
      "Created a chunk of size 1285, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 18863, which is longer than the specified 1000\n",
      "Created a chunk of size 1312, which is longer than the specified 1000\n",
      "Created a chunk of size 1500, which is longer than the specified 1000\n",
      "Created a chunk of size 1257, which is longer than the specified 1000\n",
      "Created a chunk of size 4489, which is longer than the specified 1000\n",
      "Created a chunk of size 1390, which is longer than the specified 1000\n",
      "Created a chunk of size 1341, which is longer than the specified 1000\n",
      "Created a chunk of size 1875, which is longer than the specified 1000\n",
      "Created a chunk of size 3429, which is longer than the specified 1000\n",
      "Created a chunk of size 4507, which is longer than the specified 1000\n",
      "Created a chunk of size 1444, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1507, which is longer than the specified 1000\n",
      "Created a chunk of size 1652, which is longer than the specified 1000\n",
      "Created a chunk of size 1248, which is longer than the specified 1000\n",
      "Created a chunk of size 3417, which is longer than the specified 1000\n",
      "Created a chunk of size 1441, which is longer than the specified 1000\n",
      "Created a chunk of size 2761, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1316, which is longer than the specified 1000\n",
      "Created a chunk of size 1312, which is longer than the specified 1000\n",
      "Created a chunk of size 1805, which is longer than the specified 1000\n",
      "Created a chunk of size 1614, which is longer than the specified 1000\n",
      "Created a chunk of size 1666, which is longer than the specified 1000\n",
      "Created a chunk of size 1048, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 2679, which is longer than the specified 1000\n",
      "Created a chunk of size 2700, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 1134, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw document...knowledgeBase\\Week6-Tricks-of-the-trade-and-data-science-challenge\\Notebooks\\6.1-EXE-Kaggle-Leaf-Challenge.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.1-autoencoder.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.2-EXE-variational-autoencoder.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.3-generative-adversarial-networks.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.4-SUPP-flow-models.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.2_Prerequisites.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.3-EXE_Policy_Gradient.ipynb\n",
      "Splitting text...\n",
      "Loading raw document...knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.4_Q-Network.ipynb\n",
      "Splitting text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2089, which is longer than the specified 1000\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "Created a chunk of size 2770, which is longer than the specified 1000\n",
      "Created a chunk of size 3763, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw document...knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.5_Deep_Q-network.ipynb\n",
      "Splitting text...\n"
     ]
    }
   ],
   "source": [
    "from loadingDocuments import Loading_files\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True\n",
    ")\n",
    "folder_paths = [\n",
    "    r\"knowledgeBase\\GeneralInformation\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\",\n",
    "    r\"knowledgeBase\\Week6-Tricks-of-the-trade-and-data-science-challenge\\Notebooks\",\n",
    "    r\"knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\",\n",
    "    r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\"\n",
    "    ]\n",
    "all_documents = []\n",
    "\n",
    "for folder_path in folder_paths:\n",
    "    filePaths = Loading_files(folder_path)\n",
    "    loaders = [read_data(file_path) for file_path in filePaths]\n",
    "\n",
    "    for loader in loaders:\n",
    "        print(\"Loading raw document...\" + loader.file_path)\n",
    "        raw_documents = loader.load()\n",
    "\n",
    "        print(\"Splitting text...\")\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            separator=\"\\n\\n\",\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=100,\n",
    "            length_function=len,\n",
    "            add_start_index=True\n",
    "        )\n",
    "        documents = text_splitter.split_documents(raw_documents)\n",
    "        for idx, chunk in enumerate(documents):\n",
    "            chunk.metadata['id'] = idx\n",
    "        all_documents.extend(documents)\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(all_documents,embeddings)\n",
    "vectorstore_name=\"openai_naive_500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_embedder = OpenAIEmbeddings()\n",
    "# model_name=\"openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "#     print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "# hf_embedder = HuggingFaceEmbeddings(model_name=model_name) # \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# #if torch.cuda.is_available():\n",
    "# #    hf_embedder = hf_embedder.to('cuda')\n",
    "# model_name = model_name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_info_chunks.extend(total_notebook_chunks)\n",
    "# vectorstore = FAISS.from_documents(total_info_chunks, hf_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore_name = \"faiss_{}_{}_{}_not-cleaned-notebook-contents-v1\".format(chunk_tokens,overlap_tokens,model_name)\n",
    "# vector_store_path = \"vector_stores\" + \"/\" + vectorstore_name\n",
    "# vectorstore.save_local(vector_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='The course outline is:\\n1. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part I do it yourself on pen and paper.\\n2. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part II do it yourself in NumPy.\\n3. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part III PyTorch.\\n4. Convolutional neural networks (CNN) + presentation of student projects.\\n5. Sequence modelling for text data with Transformers.\\n6. Tricks of the trade and data science with PyTorch + Start of student projects.\\n7. Variational learning and generative adversarial networks for unsupervised and semi-supervised learning.\\n8. Reinforcement learning - policy gradient and deep Q-learning.\\n\\nStarting from week 6 and full time from week 9 and the rest of the term will be spent on tutored project work.', metadata={'source': 'knowledgeBase\\\\GeneralInformation\\\\CourseOutline.txt', 'start_index': 446, 'id': 1}),\n",
       "  0.44331372),\n",
       " (Document(page_content='\\'markdown\\' cell: \\'[\\'----\\', \\'## IV. Transformers\\', \\'\\', \\'<img src=\"images/transformer.png\" alt=\"Transformer architecture\" width=\"600\"/>\\', \\'\\', \\'In this section we are going to introduce the [Transformer (\"Attention is all you need\", Vaswani (2017))](https://arxiv.org/abs/1706.03762) architecture.\\', \\'\\', \\'For further information, see the excellent PyTorch tutorial [\"language translation using Transformers\"](https://pytorch.org/tutorials/beginner/translation_transformer.html) and blog article [\"Annotated Transformer\"](https://nlp.seas.harvard.edu/2018/04/03/attention.html), which review the original in great details and provide additional content such as visualizations of the learned attention maps.\\', \\'\\', \\'**Architecture** A Transformer is composed of two main components: a decoder which implements a language model and an encoder. The encoder is only required for conditional language models like those used in translation tasks. Each of the two components is made by stacking Transformer layers (layers with and without conditioning). Each layer transforms a sequence of hidden state $\\\\\\\\mathbf{h}_{1:T}^l$ into another sequence $\\\\\\\\mathbf{h}_{1:T}^{l+1}$. The input tokens are converted into the first state $\\\\\\\\mathbf{h}_{1:T}^0$ using an embedding layer coupled with positioal encodings. the last hidden state $\\\\\\\\mathbf{h}_{1:T}^{L}$ is projected into the vocabulary space using a liner layer.\\']\\'', metadata={'source': 'knowledgeBase\\\\Week5-Transformers-and-recurrent-neural-networks\\\\Notebooks\\\\5_1_EXE_deep_learning_with_transformers.ipynb', 'start_index': 53418, 'id': 35}),\n",
       "  0.44532937),\n",
       " (Document(page_content='\\'markdown\\' cell: \\'[\\'# Purpose and goals\\', \\'In this notebook you will implement a simple neural network in PyTorch.\\', \\'\\', \\'The building blocks of PyTorch are Tensors, and Operations, with these we can form dynamic computational graphs that represent neural networks.\\', \"In this exercise we\\'ll start right away by defining a logistic regression model using these simple building blocks.\", \"We\\'ll initially start with a simple 2D and binary (i.e. two-class) classification problem where the class decision boundary can be visualized.\", \\'Initially we show that logistic regression can only separate classes linearly.\\', \\'Adding a nonlinear hidden layer to the algorithm permits nonlinear class separation.\\', \\'\\', \\'In this notebook you should:\\', \\'* **First** run the code as is, and see what it does.\\', \\'* **Then** modify the code, following the instructions in the bottom of the notebook.\\', \\'* **Lastly** play around a bit, and do some small experiments that you come up with.\\', \\'\\', \\'> We assume that you are already familiar with backpropagation (if not please see [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) or [Michal Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html)).\\']\\'', metadata={'source': 'knowledgeBase\\\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\\\Notebooks\\\\3.3-FFN-Half-Moon.ipynb', 'start_index': 0, 'id': 0}),\n",
       "  0.45466423),\n",
       " (Document(page_content='\\'markdown\\' cell: \\'[\\'Following we define the PyTorch functions for training and evaluation.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'import torch.optim as optim\\', \\'\\', \\'# if you want L2 regularization, then add weight_decay to SGD\\', \\'optimizer = optim.SGD(net.parameters(), lr=0.25)\\', \\'\\', \\'# We will use pixel wise mean-squared error as our loss function\\', \\'loss_function = nn.MSELoss()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'We can test the forward pass by checking whether the output shape is the same as the as the input.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'# test the forward pass\\', \\'# expect output size of [32, num_features]\\', \\'x, y = next(iter(train_loader))\\', \\'print(f\"x.shape = {x.shape}\")\\', \\'\\', \\'if cuda:\\', \\'    x = x.cuda()\\', \\'\\', \\'outputs = net(x)\\', \\'print(f\"x_hat.shape = {outputs[\\\\\\'x_hat\\\\\\'].shape}\")\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'In the training loop we sample each batch and evaluate the error, latent space, and reconstructions on every epoch.\\', \\'\\', \\'**NOTE** this will take a while on CPU.\\']\\'', metadata={'source': 'knowledgeBase\\\\Week7-Un-and-semi-supervised-learning\\\\Notebooks\\\\7.1-autoencoder.ipynb', 'start_index': 9122, 'id': 6}),\n",
       "  0.4584892),\n",
       " (Document(page_content='\\'code\\' cell: \\'[\\'# The output of torchvision datasets are PIL images in the range [0, 1]. \\', \\'# We transform them to PyTorch tensors and rescale them to be in the range [-1, 1].\\', \\'transform = transforms.Compose(\\', \\'    [transforms.ToTensor(),\\', \\'     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # subtract 0.5 and divide by 0.5\\', \\'    ]\\', \\')\\', \\'\\', \\'batch_size = 64  # both for training and testing\\', \\'\\', \\'# Load datasets\\', \"train_set = torchvision.datasets.CIFAR10(root=\\'./data\\', train=True, download=True, transform=transform)\", \"test_set = torchvision.datasets.CIFAR10(root=\\'./data\\', train=False, download=True, transform=transform)\", \\'train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)\\', \\'test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=True)\\', \\'\\', \\'# Map from class index to class name.\\', \\'classes = {index: name for name, index in train_set.class_to_idx.items()}\\']\\'', metadata={'source': 'knowledgeBase\\\\Week4-Convolutional-Neural-Networks\\\\Notebooks\\\\4.2-EXE-CNN-CIFAR-10.ipynb', 'start_index': 1624, 'id': 2}),\n",
       "  0.4618674)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\"What is PyTorch?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve relevant notebooks per evaluation question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Question', 'Answer', 'Source'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_path = Path('eval_data/evaluation_dataset.csv')\n",
    "evaluation_df = read_data(qa_dataset_path)\n",
    "evaluation_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(evaluation_df.iterrows(), total=evaluation_df.shape[0]):\n",
    "    query = row['Question']\n",
    "    retrieved_docs = vectorstore.similarity_search_with_score(query, k=retrieval_k)\n",
    "    for idx, retrieved_doc in enumerate(retrieved_docs):\n",
    "        idx = idx + 1\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_source'.format(idx)] = retrieved_doc[0].metadata['source']\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_id'.format(idx)] = int(retrieved_doc[0].metadata['id'])\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_content'.format(idx)] = retrieved_doc[0].page_content\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_score'.format(idx)] = retrieved_doc[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_notebook_id(strings):\n",
    "    \"\"\"\n",
    "    Convert file paths to simplified notebook identifiers.\n",
    "\n",
    "    Args:\n",
    "    strings (list of str): List of file paths as strings.\n",
    "\n",
    "    Returns:\n",
    "    list of str: List of simplified notebook identifiers.\n",
    "    \"\"\"\n",
    "    file_ids = []\n",
    "    for string in strings:\n",
    "        # Split the string by '\\\\' to get the components\n",
    "        parts = string.split('\\\\')[-1]\n",
    "        # Combine with fix\n",
    "        if string.endswith('ipynb'):\n",
    "            notebook_number = parts.split('-')[0]\n",
    "            if len(notebook_number.split('_'))>1:\n",
    "                notebook_number = notebook_number.split('_')[0] +\"_\"+ notebook_number.split('_')[1]\n",
    "            notebook_number = notebook_number.replace('.', '_')\n",
    "            file_id = f'notebook {notebook_number}'\n",
    "        else:\n",
    "            file_id = parts\n",
    "        file_ids.append(file_id)\n",
    "\n",
    "    return file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the retrieved docs\n",
    "for idx in range(1,retrieval_k+1):\n",
    "    evaluation_df['Retrived_doc_{}_source'.format(idx)] = convert_string_to_notebook_id(evaluation_df['Retrived_doc_{}_source'.format(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 2000.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# find Recall@k\n",
    "for index, row in tqdm(evaluation_df.iterrows(), total=evaluation_df.shape[0]):\n",
    "    sources = [source.strip() for source in row['Source'].split(',')]\n",
    "    correct_retrieved_docs = 0\n",
    "    for idx in range(1,retrieval_k+1):\n",
    "        if row['Retrived_doc_{}_source'.format(idx)] in sources:\n",
    "            correct_retrieved_docs += 1\n",
    "        evaluation_df.at[index, 'recall@{}'.format(idx)] = correct_retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # store evaluation_df\n",
    "# evaluation_df_store_path = r\"eval_data\\retrieval_data\\{}_evaluation_dataset.csv\".format(vectorstore_name)\n",
    "# evaluation_df.to_csv(evaluation_df_store_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_k = {}\n",
    "for idx in range(1,retrieval_k+1):\n",
    "    queries_with_no_relevant_docs = int(evaluation_df['recall@{}'.format(idx)].value_counts()[0])\n",
    "    queries_with_relevant_docs = int(evaluation_df.shape[0] - queries_with_no_relevant_docs)\n",
    "    recall_k[idx] = round(queries_with_relevant_docs/evaluation_df.shape[0] * 100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_json = {\n",
    "    'recall@{}'.format(idx): recall_k[idx] for idx in range(1,retrieval_k+1)\n",
    "}\n",
    "recall_k_json_name = r\"eval_data\\retrieval_recall_results\\recall_k_{}.json\".format(vectorstore_name)\n",
    "with open(recall_k_json_name, 'w') as f:\n",
    "    json.dump(result_json, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
