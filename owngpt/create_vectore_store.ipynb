{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langchain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    NotebookLoader,\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    \n",
    ")\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import utils\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadingDocuments import read_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is PyTorch?</td>\n",
       "      <td>It’s a Python based scientific computing packa...</td>\n",
       "      <td>notebook 3_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the MNIST dataset?</td>\n",
       "      <td>MNIST is a dataset that is often used for benc...</td>\n",
       "      <td>notebook 3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which optimizers are mentioned in the exercise...</td>\n",
       "      <td>Optimizer and learning rate:\\nSGD + Momentum: ...</td>\n",
       "      <td>notebook 3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe the model given in the exercise noteb...</td>\n",
       "      <td>The provided code defines a PyTorch neural net...</td>\n",
       "      <td>notebook 3_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the initial assignment in exercise not...</td>\n",
       "      <td>The first task is to use Kaiming He initializa...</td>\n",
       "      <td>notebook 3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What do we expect to learn from week4?</td>\n",
       "      <td>In this lab, we will learn how to create your ...</td>\n",
       "      <td>notebook 4_1, notebook 4_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is CIFAR-10 dataset?</td>\n",
       "      <td>The images in CIFAR-10 are RGB images (3 chann...</td>\n",
       "      <td>notebook 4_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are convolutional neural networks?</td>\n",
       "      <td>The standard ConvNets are organised into layer...</td>\n",
       "      <td>notebook 4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can you provide some suggestions to improve th...</td>\n",
       "      <td>Tell us something like increase the depth of t...</td>\n",
       "      <td>notebook 4_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What do RNN and LSTM stand for?</td>\n",
       "      <td>RNN stands for Reccurent Neural Network and LS...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How can I give text as input to my network?</td>\n",
       "      <td>Before text can be used as input for a neural ...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What variables are used in the attention funct...</td>\n",
       "      <td>The attention mechanism is defined using the q...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is sampling for a language model?</td>\n",
       "      <td>Sampling text means that the language model is...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is a rnn?</td>\n",
       "      <td>A recurrent neural network (RNN) is a type of ...</td>\n",
       "      <td>notebook 5_2, notebook 5_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What topics are covered in the first three wee...</td>\n",
       "      <td>. Introduction to statistical machine learning...</td>\n",
       "      <td>CourseOutline.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>When does project work start?</td>\n",
       "      <td>Starting from week 6 and full time from week 9...</td>\n",
       "      <td>CourseOutline.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>How are students expected to communicate and e...</td>\n",
       "      <td>Organize and present project results at the fi...</td>\n",
       "      <td>LearningObjectives.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>What are the expectations regarding the final ...</td>\n",
       "      <td>Structure and write a final short technical re...</td>\n",
       "      <td>LearningObjectives.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the schedule for the '02456 Deep Learn...</td>\n",
       "      <td>Time: Mondays at 13:00-17:00 (first session is...</td>\n",
       "      <td>CoursePlan.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What are the requirements for students to gain...</td>\n",
       "      <td>The student gains access to the final project ...</td>\n",
       "      <td>CoursePlan.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "0                                    What is PyTorch?   \n",
       "1                          What is the MNIST dataset?   \n",
       "2   Which optimizers are mentioned in the exercise...   \n",
       "3   Describe the model given in the exercise noteb...   \n",
       "4   What is the initial assignment in exercise not...   \n",
       "5              What do we expect to learn from week4?   \n",
       "6                           What is CIFAR-10 dataset?   \n",
       "7             What are convolutional neural networks?   \n",
       "8   Can you provide some suggestions to improve th...   \n",
       "9                     What do RNN and LSTM stand for?   \n",
       "10        How can I give text as input to my network?   \n",
       "11  What variables are used in the attention funct...   \n",
       "12             What is sampling for a language model?   \n",
       "13                                     What is a rnn?   \n",
       "14  What topics are covered in the first three wee...   \n",
       "15                      When does project work start?   \n",
       "16  How are students expected to communicate and e...   \n",
       "17  What are the expectations regarding the final ...   \n",
       "18  What is the schedule for the '02456 Deep Learn...   \n",
       "19  What are the requirements for students to gain...   \n",
       "\n",
       "                                               Answer  \\\n",
       "0   It’s a Python based scientific computing packa...   \n",
       "1   MNIST is a dataset that is often used for benc...   \n",
       "2   Optimizer and learning rate:\\nSGD + Momentum: ...   \n",
       "3   The provided code defines a PyTorch neural net...   \n",
       "4   The first task is to use Kaiming He initializa...   \n",
       "5   In this lab, we will learn how to create your ...   \n",
       "6   The images in CIFAR-10 are RGB images (3 chann...   \n",
       "7   The standard ConvNets are organised into layer...   \n",
       "8   Tell us something like increase the depth of t...   \n",
       "9   RNN stands for Reccurent Neural Network and LS...   \n",
       "10  Before text can be used as input for a neural ...   \n",
       "11  The attention mechanism is defined using the q...   \n",
       "12  Sampling text means that the language model is...   \n",
       "13  A recurrent neural network (RNN) is a type of ...   \n",
       "14  . Introduction to statistical machine learning...   \n",
       "15  Starting from week 6 and full time from week 9...   \n",
       "16  Organize and present project results at the fi...   \n",
       "17  Structure and write a final short technical re...   \n",
       "18  Time: Mondays at 13:00-17:00 (first session is...   \n",
       "19  The student gains access to the final project ...   \n",
       "\n",
       "                         Source  \n",
       "0                  notebook 3_1  \n",
       "1                  notebook 3_4  \n",
       "2                  notebook 3_4  \n",
       "3                  notebook 3_3  \n",
       "4                  notebook 3_4  \n",
       "5    notebook 4_1, notebook 4_2  \n",
       "6                  notebook 4_2  \n",
       "7                  notebook 4_1  \n",
       "8                  notebook 4_2  \n",
       "9                  notebook 5_1  \n",
       "10                 notebook 5_1  \n",
       "11                 notebook 5_1  \n",
       "12                 notebook 5_1  \n",
       "13   notebook 5_2, notebook 5_3  \n",
       "14            CourseOutline.txt  \n",
       "15            CourseOutline.txt  \n",
       "16       LearningObjectives.txt  \n",
       "17       LearningObjectives.txt  \n",
       "18               CoursePlan.txt  \n",
       "19               CoursePlan.txt  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_path = Path('eval_data/evaluation_dataset.csv')\n",
    "read_data(qa_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test/split course description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knowledgeBase\\\\GeneralInformation\\\\CourseOutline.txt', 'knowledgeBase\\\\GeneralInformation\\\\CoursePlan.txt', 'knowledgeBase\\\\GeneralInformation\\\\LearningObjectives.txt']\n"
     ]
    }
   ],
   "source": [
    "# get all files name from .\\knowledgeBase\\GeneralInformation\n",
    "path = Path(\".\\knowledgeBase\\GeneralInformation\")\n",
    "files = path.glob('**/*.txt')\n",
    "files = [str(x) for x in files]\n",
    "print(files)\n",
    "total_info_docs = []\n",
    "for file in files:\n",
    "    loader = TextLoader(file)\n",
    "    total_info_docs.append(loader.load()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: knowledgeBase\\GeneralInformation\\CourseOutline.txt  has 274 tokens\n",
      "Doc: knowledgeBase\\GeneralInformation\\CoursePlan.txt  has 4477 tokens\n",
      "Doc: knowledgeBase\\GeneralInformation\\LearningObjectives.txt  has 174 tokens\n"
     ]
    }
   ],
   "source": [
    "for doc in total_info_docs:\n",
    "    print(f\"Doc: {doc.metadata['source']}  has {utils.get_tokens_count(doc.page_content)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_tokens = 350\n",
    "overlap_tokens = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_tokens,\n",
    "            chunk_overlap=overlap_tokens,\n",
    "            length_function=utils.get_tokens_count, #len\n",
    "            add_start_index=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info doc knowledgeBase\\GeneralInformation\\CourseOutline.txt has 1 chunks\n",
      "Info doc knowledgeBase\\GeneralInformation\\CoursePlan.txt has 18 chunks\n",
      "Info doc knowledgeBase\\GeneralInformation\\LearningObjectives.txt has 1 chunks\n"
     ]
    }
   ],
   "source": [
    "total_info_chunks =  []\n",
    "for doc in total_info_docs:\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk.metadata['id'] = idx\n",
    "    total_info_chunks.extend(chunks)\n",
    "    print(\"Info doc {} has {} chunks\".format(doc.metadata['source'], len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test/split notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_paths = [\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_1_EXE_deep_learning_with_transformers.ipynb\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb\",\n",
    "    r\"knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.1-autoencoder.ipynb\",\n",
    "    r\"knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.2-EXE-variational-autoencoder.ipynb\",\n",
    "    r\"knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.3-generative-adversarial-networks.ipynb\",\n",
    "    r\"knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.4-SUPP-flow-models.ipynb\",\n",
    "    #r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.1_Introduction.ipynb\",\n",
    "    r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.2_Prerequisites.ipynb\",\n",
    "    r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.3-EXE_Policy_Gradient.ipynb\",\n",
    "    r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.4_Q-Network.ipynb\",\n",
    "    r\"knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.5_Deep_Q-network.ipynb\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb has 1451 tokens\n",
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb has 1154 tokens\n",
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb has 4221 tokens\n",
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb has 3678 tokens\n",
      "Notebok: knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb has 3552 tokens\n",
      "Notebok: knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb has 3298 tokens\n",
      "Notebok: knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb has 3939 tokens\n",
      "Notebok: knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_1_EXE_deep_learning_with_transformers.ipynb has 19766 tokens\n",
      "Notebok: knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb has 14619 tokens\n",
      "Notebok: knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb has 17289 tokens\n",
      "Notebok: knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.1-autoencoder.ipynb has 4035 tokens\n",
      "Notebok: knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.2-EXE-variational-autoencoder.ipynb has 14696 tokens\n",
      "Notebok: knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.3-generative-adversarial-networks.ipynb has 3891 tokens\n",
      "Notebok: knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.4-SUPP-flow-models.ipynb has 3631 tokens\n",
      "Notebok: knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.2_Prerequisites.ipynb has 797 tokens\n",
      "Notebok: knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.3-EXE_Policy_Gradient.ipynb has 3355 tokens\n",
      "Notebok: knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.4_Q-Network.ipynb has 2007 tokens\n",
      "Notebok: knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.5_Deep_Q-network.ipynb has 2898 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n"
     ]
    }
   ],
   "source": [
    "total_notebook_docs = []\n",
    "for notebook_path in notebook_paths:\n",
    "    notebook_path = Path(notebook_path)\n",
    "\n",
    "    loader = NotebookLoader(str(notebook_path), include_outputs=False, max_output_length=20, remove_newline=True)\n",
    "    doc_notebook = loader.load()[0]\n",
    "    total_notebook_docs.append(doc_notebook)\n",
    "    print(\"Notebok: {} has {} tokens\".format(notebook_path, utils.get_tokens_count(doc_notebook.page_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_notebook_content(content):\n",
    "\n",
    "    cells = re.split(r\"\\'(markdown|code)\\' cell: \", content)[1:]\n",
    "\n",
    "    parsed_content = []\n",
    "    for i in range(0, len(cells), 2):\n",
    "        cell_type = cells[i]\n",
    "        cell_content = cells[i + 1]\n",
    "\n",
    "        \n",
    "\n",
    "        cell_content = cells[i + 1][3:-20] + cells[i + 1][-20:].replace(\"']'\", \"'\")\n",
    "        cell_items_list = cell_content.replace(\"\\\\n\",\"\").split(\"', '\") \n",
    "        \n",
    "        # Append the cell type and content to the parsed content\n",
    "        parsed_content.append({'type': cell_type, 'content': cell_items_list})\n",
    "    return parsed_content\n",
    "\n",
    "\n",
    "def get_parsed_notebook_text(parsed_content):\n",
    "    parsed_text = []\n",
    "    for item in parsed_content:\n",
    "        parsed_text.append(item['type']+ \":\" + \"\\n\")\n",
    "        for content_text in item['content']:\n",
    "            parsed_text.append(content_text + \"\\n\")\n",
    "    return \"\".join(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for notebook_doc in total_notebook_docs:\n",
    "#     notebook_doc.page_content = get_parsed_notebook_text(parse_notebook_content(notebook_doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb has 6 chunks\n",
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb has 5 chunks\n",
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb has 18 chunks\n",
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb has 17 chunks\n",
      "Notebook knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb has 18 chunks\n",
      "Notebook knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb has 15 chunks\n",
      "Notebook knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb has 20 chunks\n",
      "Notebook knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_1_EXE_deep_learning_with_transformers.ipynb has 84 chunks\n",
      "Notebook knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb has 69 chunks\n",
      "Notebook knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb has 83 chunks\n",
      "Notebook knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.1-autoencoder.ipynb has 18 chunks\n",
      "Notebook knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.2-EXE-variational-autoencoder.ipynb has 58 chunks\n",
      "Notebook knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.3-generative-adversarial-networks.ipynb has 18 chunks\n",
      "Notebook knowledgeBase\\Week7-Un-and-semi-supervised-learning\\Notebooks\\7.4-SUPP-flow-models.ipynb has 16 chunks\n",
      "Notebook knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.2_Prerequisites.ipynb has 3 chunks\n",
      "Notebook knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.3-EXE_Policy_Gradient.ipynb has 15 chunks\n",
      "Notebook knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.4_Q-Network.ipynb has 9 chunks\n",
      "Notebook knowledgeBase\\Week8-Reinforcement-learning\\Notebooks\\8.5_Deep_Q-network.ipynb has 14 chunks\n"
     ]
    }
   ],
   "source": [
    "total_notebook_chunks =  []\n",
    "for doc in total_notebook_docs:\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk.metadata['id'] = idx\n",
    "    total_notebook_chunks.extend(chunks)\n",
    "    print(\"Notebook {} has {} chunks\".format(doc.metadata['source'], len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Openai embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from loadingDocuments import Loading_files\n",
    "# text_splitter = CharacterTextSplitter(\n",
    "#     separator=\"\\n\\n\",\n",
    "#     chunk_size=1000,\n",
    "#     chunk_overlap=100,\n",
    "#     length_function=len,\n",
    "#     add_start_index=True\n",
    "# )\n",
    "# folder_paths = [\n",
    "#     r\"knowledgeBase\\GeneralInformation\",\n",
    "#     r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\",\n",
    "#     r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\",\n",
    "#     r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\Notebooks\"\n",
    "#     ]\n",
    "# all_documents = []\n",
    "\n",
    "# for folder_path in folder_paths:\n",
    "#     filePaths = Loading_files(folder_path)\n",
    "#     loaders = [read_data(file_path) for file_path in filePaths]\n",
    "\n",
    "#     for loader in loaders:\n",
    "#         print(\"Loading raw document...\" + loader.file_path)\n",
    "#         raw_documents = loader.load()\n",
    "\n",
    "#         print(\"Splitting text...\")\n",
    "#         text_splitter = CharacterTextSplitter(\n",
    "#             separator=\"\\n\\n\",\n",
    "#             chunk_size=1000,\n",
    "#             chunk_overlap=100,\n",
    "#             length_function=len,\n",
    "#             add_start_index=True\n",
    "#         )\n",
    "#         documents = text_splitter.split_documents(raw_documents)\n",
    "#         for idx, chunk in enumerate(documents):\n",
    "#             chunk.metadata['id'] = idx\n",
    "#         all_documents.extend(documents)\n",
    "\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstore = FAISS.from_documents(all_documents,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_embedder = OpenAIEmbeddings()\n",
    "model_name=\"openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.cuda.is_available())  # Should return True if CUDA is available\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "#     print(\"Using CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "# hf_embedder = HuggingFaceEmbeddings(model_name=model_name) # \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# #if torch.cuda.is_available():\n",
    "# #    hf_embedder = hf_embedder.to('cuda')\n",
    "# model_name = model_name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_info_chunks.extend(total_notebook_chunks)\n",
    "vectorstore = FAISS.from_documents(total_info_chunks, hf_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_name = \"faiss_{}_{}_{}_not-cleaned-notebook-contents-v1\".format(chunk_tokens,overlap_tokens,model_name)\n",
    "vector_store_path = \"vector_stores\" + \"/\" + vectorstore_name\n",
    "vectorstore.save_local(vector_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"we will show you:', '* How to represent sequences of categorical variables', '* How to build and train an RNN in NumPy', '* How to build and train an LSTM network in NumPy', '* How to build and train an LSTM network in PyTorch']'\", metadata={'source': 'knowledgeBase\\\\Week5-Transformers-and-recurrent-neural-networks\\\\Notebooks\\\\5_3-Recurrent-Neural-Networks-Numpy.ipynb', 'start_index': 1345, 'id': 1}),\n",
       "  0.4372698),\n",
       " (Document(page_content='The purpose of this course is to give the student a detailed understanding of the deep artificial neural network models, their training, computational frameworks for deployment on fast graphical processing units, their limitations and how to formulate learning in a diverse range of settings. These settings include classification, regression, sequences and other types of structured input and outputs and for reasoning in complex environments.\\n\\nThe course outline is:\\n1. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part I do it yourself on pen and paper.\\n2. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part II do it yourself in NumPy.\\n3. Introduction to statistical machine learning, feed-forward neural networks (FFNN) and error back-propagation. Part III PyTorch.\\n4. Convolutional neural networks (CNN) + presentation of student projects.\\n5. Sequence modelling for text data with Transformers.\\n6. Tricks of the trade and data science with PyTorch + Start of student projects.\\n7. Variational learning and generative adversarial networks for unsupervised and semi-supervised learning.\\n8. Reinforcement learning - policy gradient and deep Q-learning.\\n\\nStarting from week 6 and full time from week 9 and the rest of the term will be spent on tutored project work.', metadata={'source': 'knowledgeBase\\\\GeneralInformation\\\\CourseOutline.txt', 'start_index': 0, 'id': 0}),\n",
       "  0.43962866),\n",
       " (Document(page_content='but in general this is trial and error.\", \\'\\', \\'__Parameter initialization:__\\', \\'Parameter initialization is extremely important.\\', \\'PyTorch has a lot of different initializers, check the [PyTorch API](http://pytorch.org/docs/master/nn.html#torch-nn-init). Often used initializer are\\', \\'1. Kaiming He\\', \\'2. Xavier Glorot\\', \\'3. Uniform or Normal with small scale (0.1 - 0.01)\\', \\'4. Orthogonal (this usually works very well for RNNs)\\', \\'\\', \\'Bias is nearly always initialized to zero using the [torch.nn.init.constant(tensor, val)](http://pytorch.org/docs/master/nn.html#torch.nn.init.constant)\\', \\'\\', \\'__Mini-batch size:__\\', \\'Usually people use 16-256. Bigger is not allways better. With smaller mini-batch size you get more updates and your model might converge faster. Also small batch sizes use less memory, which means you can train a model with more parameters.\\', \\'\\', \\'__Nonlinearity:__ [The most commonly used nonliearities are](http://pytorch.org/docs/master/nn.html#non-linear-activations)\\', \\'1. ReLU\\', \\'2. Leaky ReLU\\', \\'3. ELU\\', \\'3. Sigmoid (rarely, if ever, used in hidden layers anymore, squashes the output to the interval [0, 1] - appropriate if the targets are binary.\\', \\'4. Tanh is similar to the sigmoid, but squashes to [-1, 1]. Rarely used any more.\\', \\'4. Softmax normalizes the output to 1, usrful if', metadata={'source': 'knowledgeBase\\\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\\\Notebooks\\\\3.4-EXE-FFN-MNIST.ipynb', 'start_index': 4772, 'id': 5}),\n",
       "  0.44018823),\n",
       " (Document(page_content='\\'markdown\\' cell: \\'[\\'----\\', \\'## IV. Transformers\\', \\'\\', \\'<img src=\"images/transformer.png\" alt=\"Transformer architecture\" width=\"600\"/>\\', \\'\\', \\'In this section we are going to introduce the [Transformer (\"Attention is all you need\", Vaswani (2017))](https://arxiv.org/abs/1706.03762) architecture.\\', \\'\\', \\'For further information, see the excellent PyTorch tutorial [\"language translation using Transformers\"](https://pytorch.org/tutorials/beginner/translation_transformer.html) and blog article [\"Annotated Transformer\"](https://nlp.seas.harvard.edu/2018/04/03/attention.html), which review the original in great details and provide additional content such as visualizations of the learned attention maps.\\', \\'\\', \\'**Architecture** A Transformer is composed of two main components: a decoder which implements a language model and an encoder. The encoder is only required for conditional language models like those used in translation tasks. Each of the two components is made by stacking Transformer layers (layers with and without conditioning). Each layer transforms a sequence of hidden state $\\\\\\\\mathbf{h}_{1:T}^l$ into another sequence $\\\\\\\\mathbf{h}_{1:T}^{l+1}$. The input tokens are converted into the first state $\\\\\\\\mathbf{h}_{1:T}^0$ using an embedding layer coupled with positioal encodings. the last hidden state $\\\\\\\\mathbf{h}_{1:T}^{L}$ is projected into the vocabulary space using a liner layer.\\']\\'', metadata={'source': 'knowledgeBase\\\\Week5-Transformers-and-recurrent-neural-networks\\\\Notebooks\\\\5_1_EXE_deep_learning_with_transformers.ipynb', 'start_index': 53418, 'id': 61}),\n",
       "  0.44532937),\n",
       " (Document(page_content=\"How to build and train an RNN in Nanograd', '* How to build and train an LSTM network in Nanograd', '* How to build and train an LSTM network in PyTorch', '', '', '[Numpy version of the Notebook (previous version)](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/5_Recurrent/OLD-5.1-Numpy-Recurrent-Neural-Networks.ipynb)']'\", metadata={'source': 'knowledgeBase\\\\Week5-Transformers-and-recurrent-neural-networks\\\\Notebooks\\\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb', 'start_index': 1293, 'id': 1}),\n",
       "  0.4484774)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\"What is PyTorch?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve relevant notebooks per evaluation question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Question', 'Answer', 'Source'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_path = Path('eval_data/evaluation_dataset.csv')\n",
    "evaluation_df = read_data(qa_dataset_path)\n",
    "evaluation_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(evaluation_df.iterrows(), total=evaluation_df.shape[0]):\n",
    "    query = row['Question']\n",
    "    retrieved_docs = vectorstore.similarity_search_with_score(query, k=retrieval_k)\n",
    "    for idx, retrieved_doc in enumerate(retrieved_docs):\n",
    "        idx = idx + 1\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_source'.format(idx)] = retrieved_doc[0].metadata['source']\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_id'.format(idx)] = int(retrieved_doc[0].metadata['id'])\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_content'.format(idx)] = retrieved_doc[0].page_content\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_score'.format(idx)] = retrieved_doc[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_notebook_id(strings):\n",
    "    \"\"\"\n",
    "    Convert file paths to simplified notebook identifiers.\n",
    "\n",
    "    Args:\n",
    "    strings (list of str): List of file paths as strings.\n",
    "\n",
    "    Returns:\n",
    "    list of str: List of simplified notebook identifiers.\n",
    "    \"\"\"\n",
    "    file_ids = []\n",
    "    for string in strings:\n",
    "        # Split the string by '\\\\' to get the components\n",
    "        parts = string.split('\\\\')[-1]\n",
    "        # Combine with fix\n",
    "        if string.endswith('ipynb'):\n",
    "            notebook_number = parts.split('-')[0]\n",
    "            if len(notebook_number.split('_'))>1:\n",
    "                notebook_number = notebook_number.split('_')[0] +\"_\"+ notebook_number.split('_')[1]\n",
    "            notebook_number = notebook_number.replace('.', '_')\n",
    "            file_id = f'notebook {notebook_number}'\n",
    "        else:\n",
    "            file_id = parts\n",
    "        file_ids.append(file_id)\n",
    "\n",
    "    return file_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the retrieved docs\n",
    "for idx in range(1,retrieval_k+1):\n",
    "    evaluation_df['Retrived_doc_{}_source'.format(idx)] = convert_string_to_notebook_id(evaluation_df['Retrived_doc_{}_source'.format(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 2857.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# find Recall@k\n",
    "for index, row in tqdm(evaluation_df.iterrows(), total=evaluation_df.shape[0]):\n",
    "    sources = [source.strip() for source in row['Source'].split(',')]\n",
    "    correct_retrieved_docs = 0\n",
    "    for idx in range(1,retrieval_k+1):\n",
    "        if row['Retrived_doc_{}_source'.format(idx)] in sources:\n",
    "            correct_retrieved_docs += 1\n",
    "        evaluation_df.at[index, 'recall@{}'.format(idx)] = correct_retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store evaluation_df\n",
    "evaluation_df_store_path = r\"eval_data\\retrieval_data\\{}_evaluation_dataset.csv\".format(vectorstore_name)\n",
    "evaluation_df.to_csv(evaluation_df_store_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_k = {}\n",
    "for idx in range(1,retrieval_k+1):\n",
    "    queries_with_no_relevant_docs = int(evaluation_df['recall@{}'.format(idx)].value_counts()[0])\n",
    "    queries_with_relevant_docs = int(evaluation_df.shape[0] - queries_with_no_relevant_docs)\n",
    "    recall_k[idx] = round(queries_with_relevant_docs/evaluation_df.shape[0] * 100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_json = {\n",
    "    'recall@{}'.format(idx): recall_k[idx] for idx in range(1,retrieval_k+1)\n",
    "}\n",
    "recall_k_json_name = r\"eval_data\\retrieval_recall_results\\recall_k_{}.json\".format(vectorstore_name)\n",
    "with open(recall_k_json_name, 'w') as f:\n",
    "    json.dump(result_json, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
