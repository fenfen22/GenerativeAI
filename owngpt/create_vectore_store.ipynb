{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langchain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    NotebookLoader,\n",
    "    PyPDFLoader,\n",
    "    PyMuPDFLoader,\n",
    "    UnstructuredPDFLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    \n",
    ")\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import utils\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadingDocuments import read_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is PyTorch?</td>\n",
       "      <td>It’s a Python based scientific computing packa...</td>\n",
       "      <td>notebook 3_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the MNIST dataset?</td>\n",
       "      <td>MNIST is a dataset that is often used for benc...</td>\n",
       "      <td>notebook 3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which optimizers are mentioned in the exercise...</td>\n",
       "      <td>Optimizer and learning rate:\\nSGD + Momentum: ...</td>\n",
       "      <td>notebook 3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe the model given in the exercise noteb...</td>\n",
       "      <td>The provided code defines a PyTorch neural net...</td>\n",
       "      <td>notebook 3_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the initial assignment in exercise not...</td>\n",
       "      <td>The first task is to use Kaiming He initializa...</td>\n",
       "      <td>notebook 3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What do we expect to learn from week4?</td>\n",
       "      <td>In this lab, we will learn how to create your ...</td>\n",
       "      <td>notebook 4_1, notebook 4_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is CIFAR-10 dataset?</td>\n",
       "      <td>The images in CIFAR-10 are RGB images (3 chann...</td>\n",
       "      <td>notebook 4_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are convolutional neural networks?</td>\n",
       "      <td>The standard ConvNets are organised into layer...</td>\n",
       "      <td>notebook 4_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can you provide some suggestions to improve th...</td>\n",
       "      <td>Tell us something like increase the depth of t...</td>\n",
       "      <td>notebook 4_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What do RNN and LSTM stand for?</td>\n",
       "      <td>RNN stands for Reccurent Neural Network and LS...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How can I give text as input to my network?</td>\n",
       "      <td>Before text can be used as input for a neural ...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What variables are used in the attention funct...</td>\n",
       "      <td>The attention mechanism is defined using the q...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is sampling for a language model?</td>\n",
       "      <td>Sampling text means that the language model is...</td>\n",
       "      <td>notebook 5_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is a rnn?</td>\n",
       "      <td>A recurrent neural network (RNN) is a type of ...</td>\n",
       "      <td>notebook 5_2, notebook 5_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "0                                    What is PyTorch?   \n",
       "1                          What is the MNIST dataset?   \n",
       "2   Which optimizers are mentioned in the exercise...   \n",
       "3   Describe the model given in the exercise noteb...   \n",
       "4   What is the initial assignment in exercise not...   \n",
       "5              What do we expect to learn from week4?   \n",
       "6                           What is CIFAR-10 dataset?   \n",
       "7             What are convolutional neural networks?   \n",
       "8   Can you provide some suggestions to improve th...   \n",
       "9                     What do RNN and LSTM stand for?   \n",
       "10        How can I give text as input to my network?   \n",
       "11  What variables are used in the attention funct...   \n",
       "12             What is sampling for a language model?   \n",
       "13                                     What is a rnn?   \n",
       "\n",
       "                                               Answer  \\\n",
       "0   It’s a Python based scientific computing packa...   \n",
       "1   MNIST is a dataset that is often used for benc...   \n",
       "2   Optimizer and learning rate:\\nSGD + Momentum: ...   \n",
       "3   The provided code defines a PyTorch neural net...   \n",
       "4   The first task is to use Kaiming He initializa...   \n",
       "5   In this lab, we will learn how to create your ...   \n",
       "6   The images in CIFAR-10 are RGB images (3 chann...   \n",
       "7   The standard ConvNets are organised into layer...   \n",
       "8   Tell us something like increase the depth of t...   \n",
       "9   RNN stands for Reccurent Neural Network and LS...   \n",
       "10  Before text can be used as input for a neural ...   \n",
       "11  The attention mechanism is defined using the q...   \n",
       "12  Sampling text means that the language model is...   \n",
       "13  A recurrent neural network (RNN) is a type of ...   \n",
       "\n",
       "                         Source  \n",
       "0                  notebook 3_1  \n",
       "1                  notebook 3_4  \n",
       "2                  notebook 3_4  \n",
       "3                  notebook 3_3  \n",
       "4                  notebook 3_4  \n",
       "5    notebook 4_1, notebook 4_2  \n",
       "6                  notebook 4_2  \n",
       "7                  notebook 4_1  \n",
       "8                  notebook 4_2  \n",
       "9                  notebook 5_1  \n",
       "10                 notebook 5_1  \n",
       "11                 notebook 5_1  \n",
       "12                 notebook 5_1  \n",
       "13   notebook 5_2, notebook 5_3  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_path = Path('eval_data/evaluation_dataset.csv')\n",
    "read_data(qa_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test/split course description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['knowledgeBase\\\\GeneralInformation\\\\CourseOutline.txt', 'knowledgeBase\\\\GeneralInformation\\\\CoursePlan.txt', 'knowledgeBase\\\\GeneralInformation\\\\LearningObjectives.txt']\n"
     ]
    }
   ],
   "source": [
    "# get all files name from .\\knowledgeBase\\GeneralInformation\n",
    "path = Path(\".\\knowledgeBase\\GeneralInformation\")\n",
    "files = path.glob('**/*.txt')\n",
    "files = [str(x) for x in files]\n",
    "print(files)\n",
    "total_info_docs = []\n",
    "for file in files:\n",
    "    loader = TextLoader(file)\n",
    "    total_info_docs.append(loader.load()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: knowledgeBase\\GeneralInformation\\CourseOutline.txt  has 274 tokens\n",
      "Doc: knowledgeBase\\GeneralInformation\\CoursePlan.txt  has 4477 tokens\n",
      "Doc: knowledgeBase\\GeneralInformation\\LearningObjectives.txt  has 174 tokens\n"
     ]
    }
   ],
   "source": [
    "for doc in total_info_docs:\n",
    "    print(f\"Doc: {doc.metadata['source']}  has {utils.get_tokens_count(doc.page_content)} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=700,\n",
    "            chunk_overlap=50,\n",
    "            length_function=utils.get_tokens_count, #len\n",
    "            add_start_index=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info doc knowledgeBase\\GeneralInformation\\CourseOutline.txt has 1 chunks\n",
      "Info doc knowledgeBase\\GeneralInformation\\CoursePlan.txt has 9 chunks\n",
      "Info doc knowledgeBase\\GeneralInformation\\LearningObjectives.txt has 1 chunks\n"
     ]
    }
   ],
   "source": [
    "total_info_chunks =  []\n",
    "for doc in total_info_docs:\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk.metadata['id'] = idx\n",
    "    total_info_chunks.extend(chunks)\n",
    "    print(\"Info doc {} has {} chunks\".format(doc.metadata['source'], len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test/split notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_paths = [\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb\",\n",
    "    r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb\",\n",
    "    r\"knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\5_1_EXE_deep_learning_with_transformers.ipynb\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb\",\n",
    "    r\"knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb has 1451 tokens\n",
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb has 1154 tokens\n",
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb has 4221 tokens\n",
      "Notebok: knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb has 3678 tokens\n",
      "Notebok: knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb has 3552 tokens\n",
      "Notebok: knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb has 3298 tokens\n",
      "Notebok: knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb has 3939 tokens\n",
      "Notebok: knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\5_1_EXE_deep_learning_with_transformers.ipynb has 19766 tokens\n",
      "Notebok: knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb has 14619 tokens\n",
      "Notebok: knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb has 17289 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n",
      "c:\\Users\\xmullaym\\anaconda3\\envs\\deeplearning-final-project\\lib\\site-packages\\langchain\\document_loaders\\notebook.py:121: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  filtered_data = filtered_data.applymap(remove_newlines)\n"
     ]
    }
   ],
   "source": [
    "notebook_path = Path(r\"knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch-20231117T091040Z-001\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb\")\n",
    "total_notebook_docs = []\n",
    "for notebook_path in notebook_paths:\n",
    "    notebook_path = Path(notebook_path)\n",
    "\n",
    "    loader = NotebookLoader(str(notebook_path), include_outputs=False, max_output_length=20, remove_newline=True)\n",
    "    doc_notebook = loader.load()[0]\n",
    "    total_notebook_docs.append(doc_notebook)\n",
    "    print(\"Notebok: {} has {} tokens\".format(notebook_path, utils.get_tokens_count(doc_notebook.page_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_notebook_content(content):\n",
    "\n",
    "    cells = re.split(r\"\\'(markdown|code)\\' cell: \", content)[1:]\n",
    "\n",
    "    parsed_content = []\n",
    "    for i in range(0, len(cells), 2):\n",
    "        cell_type = cells[i]\n",
    "        cell_content = cells[i + 1]\n",
    "\n",
    "        \n",
    "\n",
    "        cell_content = cells[i + 1][3:-20] + cells[i + 1][-20:].replace(\"']'\", \"'\")\n",
    "        cell_items_list = cell_content.replace(\"\\\\n\",\"\").split(\"', '\") \n",
    "        \n",
    "        # Append the cell type and content to the parsed content\n",
    "        parsed_content.append({'type': cell_type, 'content': cell_items_list})\n",
    "    return parsed_content\n",
    "\n",
    "\n",
    "def get_parsed_notebook_text(parsed_content):\n",
    "    parsed_text = []\n",
    "    for item in parsed_content:\n",
    "        parsed_text.append(item['type']+ \":\" + \"\\n\")\n",
    "        for content_text in item['content']:\n",
    "            parsed_text.append(content_text + \"\\n\")\n",
    "    return \"\".join(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = total_notebook_docs[0].page_content\n",
    "with open(\"notebook.txt\", \"w\") as f:\n",
    "    f.write(content)\n",
    "\n",
    "parsed_content = parse_notebook_content(content)\n",
    "with open(\"notebook_parsed.txt\", \"w\") as f:\n",
    "    f.write(get_parsed_notebook_text(parsed_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for notebook_doc in total_notebook_docs:\n",
    "#     notebook_doc.page_content = get_parsed_notebook_text(parse_notebook_content(notebook_doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.1-what-is-pytorch.ipynb has 3 chunks\n",
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.2-automatic-differentiation.ipynb has 2 chunks\n",
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.3-FFN-Half-Moon.ipynb has 8 chunks\n",
      "Notebook knowledgeBase\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\Notebooks\\3.4-EXE-FFN-MNIST.ipynb has 9 chunks\n",
      "Notebook knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.1-CNN-Introduction.ipynb has 7 chunks\n",
      "Notebook knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.2-EXE-CNN-CIFAR-10.ipynb has 6 chunks\n",
      "Notebook knowledgeBase\\Week4-Convolutional-Neural-Networks\\Notebooks\\4.3-CNN-transfer.ipynb has 8 chunks\n",
      "Notebook knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\5_1_EXE_deep_learning_with_transformers.ipynb has 42 chunks\n",
      "Notebook knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb has 27 chunks\n",
      "Notebook knowledgeBase\\Week5-Transformers-and-recurrent-neural-networks\\5_3-Recurrent-Neural-Networks-Numpy.ipynb has 39 chunks\n"
     ]
    }
   ],
   "source": [
    "total_notebook_chunks =  []\n",
    "for doc in total_notebook_docs:\n",
    "    chunks = text_splitter.split_documents([doc])\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk.metadata['id'] = idx\n",
    "    total_notebook_chunks.extend(chunks)\n",
    "    print(\"Notebook {} has {} chunks\".format(doc.metadata['source'], len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "hf_embedder = HuggingFaceEmbeddings(model_name=model_name) # \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_name = model_name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_info_chunks.extend(total_notebook_chunks)\n",
    "vectorstore = FAISS.from_documents(total_info_chunks, hf_embedder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_name = \"faiss_700_50_{}_not-cleaned-notebook-contents-v1\".format(model_name)\n",
    "vector_store_path = \"vector_stores\" + \"/\" + vectorstore_name\n",
    "vectorstore.save_local(vector_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='\\'markdown\\' cell: \\'[\\'# Credits\\', \\'\\', \\'This is heavily based on https://github.com/pytorch/tutorials\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# What is PyTorch?\\', \\'\\', \\'> **NOTE** In the last part of this lab cuda is used. If you have a cuda enabled machine, read the README.md in the root of this repo on how to use nvidia-docker.\\', \\'\\', \\'\\', \\'It’s a Python based scientific computing package targeted at two sets of\\', \\'audiences:\\', \\'-  A replacement for numpy to use the power of GPUs\\', \\'-  a deep learning research platform that provides maximum flexibility\\', \\'   and speed\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Getting Started\\', \\'\\', \\'In this lab you will get a quick start on what pytorch is and how to use it.\\', \\'\\', \\'## 1. Tensors\\', \\'\\', \\'Tensors are similar to numpy’s ndarrays, with the addition being that\\', \\'Tensors can also be used on a GPU to accelerate computing.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'import torch\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Construct a 5x3 matrix, uninitialized\\']\\'\\n\\n \\'code\\' cell: \\'[\\'x = torch.Tensor(5, 3)\\', \\'print(x)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Construct a randomly initialized matrix\\']\\'\\n\\n \\'code\\' cell: \\'[\\'x = torch.rand(5, 3)\\', \\'print(x)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Get its size\\']\\'\\n\\n \\'code\\' cell: \\'[\\'print(x.size())\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'**NOTE**: `torch.Size` is in fact a tuple, so it supports the same operations that a tuple supports.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'x[1:3] = 2\\', \\'print(x)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Assignment\\', \\'\\', \\'Make use of the pytorch docs <http://pytorch.org/docs/torch>\\', \\'1. Make a tensor of size (2, 17)\\', \\'2. Make a torch.FloatTensor of size (3, 1)\\', \\'3. Make a torch.LongTensor of size (5, 2, 1)\\', \\'  - fill the entire tensor with 7s\\', \\'4. Make a torch.ByteTensor of size (5,)\\', \\'  - fill the middle 3 indices with ones such that it records [0, 1, 1, 1, 0]\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## 2. Operations\\', \"There are multiple syntaxes for operations. Let\\'s see addition as an example:\", \\'\\', \\'### 2.1 Addition: syntax 1\\']\\'\\n\\n \\'code\\' cell: \\'[\\'y = torch.rand(5, 3)\\', \\'print(x + y)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### 2.2 Addition: syntax 2\\']\\'\\n\\n \\'code\\' cell: \\'[\\'print(torch.add(x, y))\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### 2.3 Addition: giving an output tensor\\']\\'\\n\\n \\'code\\' cell: \\'[\\'result = torch.Tensor(5, 3)\\', \\'torch.add(x, y, out=result)\\', \\'print(result)\\']\\'', metadata={'source': 'knowledgeBase\\\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\\\Notebooks\\\\3.1-what-is-pytorch.ipynb', 'start_index': 0, 'id': 0}),\n",
       "  0.95865774),\n",
       " (Document(page_content='\\'markdown\\' cell: \\'[\\'# Credits\\', \\'\\', \\'This is heavily influenced or copied from https://github.com/pytorch/tutorials\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Autograd: automatic differentiation\\', \\'\\', \\'Central to all neural networks in PyTorch is the ``autograd`` package.\\', \\'Let’s first briefly visit this, and we will then go to training our first neural network.\\', \\'\\', \\'The `autograd` package provides automatic differentiation for all operations on Tensors.\\', \\'It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\\', \\'\\', \\'Let us see this in more simple terms with some examples.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## 1. Tensor\\', \\'\\', \\'`torch.Tensor` is the central class of the package. Setting the attribute `.requires_grad` to `True` will make the tensor \"record\" all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into the `.grad` attribute.\\', \\'\\', \\'![autograd.Variable](../static_files/autograd-variable.png)\\', \\'\\', \\'There’s one more class which is very important for autograd implementation - a `Function`.\\', \\'\\', \\'`Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a `Function` that has created the `Tensor` (except for Tensors created by the user - their `grad_fn` is `None`).\\', \\'\\', \\'If you want to compute the derivatives, you can call `.backward()` on a Tensor. If `Tensor` is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a `gradient` argument that is a tensor of matching shape.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'import torch\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Create a tensor\\']\\'\\n\\n \\'code\\' cell: \\'[\\'x = torch.ones(2, 2, requires_grad=True)\\', \\'print(x)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Do a tensor operation:\\']\\'\\n\\n \\'code\\' cell: \\'[\\'y = x + 2\\', \\'print(y)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'`y` was created as a result of an operation, so it has a `grad_fn`.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'print(y.grad_fn)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Do more operations on y\\']\\'\\n\\n \\'code\\' cell: \\'[\\'z = y * y * 3\\', \\'out = z.mean()\\', \\'\\', \\'print(z)\\', \\'print(out)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Assignments\\', \\'\\', \\'1. Create a Tensor that `requires_grad` of size (5, 5)\\', \\'2. Sum the values in the Tensor\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## 2. Gradients\\', \\'\\', \\'Let’s backprop now. Because `out` contains a single scalar, `out.backward()` is equivalent to `out.backward(torch.tensor([1.0]))`\\']\\'\\n\\n \\'code\\' cell: \\'[\\'out.backward()\\']\\'', metadata={'source': 'knowledgeBase\\\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\\\Notebooks\\\\3.2-automatic-differentiation.ipynb', 'start_index': 0, 'id': 0}),\n",
       "  1.044452),\n",
       " (Document(page_content=\"'code' cell: '['result = torch.Tensor(5, 3)', 'torch.add(x, y, out=result)', 'print(result)']'\\n\\n 'markdown' cell: '['### 2.4 Addition: in-place', '', 'adds `x`to `y`']'\\n\\n 'code' cell: '['y.add_(x)', 'print(y)']'\\n\\n 'markdown' cell: '['**NOTE**: Any operation that mutates a tensor in-place is post-fixed with an `_`. For example: `x.copy_(y)`, `x.t_()`, will change `x`.']'\\n\\n 'markdown' cell: '['You can use standard numpy-like indexing with all bells and whistles!']'\\n\\n 'code' cell: '['print(x[:, 1])']'\\n\\n 'markdown' cell: '['**Read later** 100+ Tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc are described here <http://pytorch.org/docs/torch>']'\\n\\n 'markdown' cell: '['# Assignment', '', '1. multiplication of two tensors (see [torch.Tensor.mul](http://pytorch.org/docs/master/tensors.html#torch.Tensor.mul))', '2. do the same, but inplace', '3. division of two tensors (see [torch.Tensor.div](http://pytorch.org/docs/master/tensors.html#torch.Tensor.div))', '4. perform a matrix multiplication of two tensors of size (2, 4) and (4, 2)']'\\n\\n 'markdown' cell: '['## 3. Numpy Bridge', '', 'Converting a torch Tensor to a numpy array and vice versa is a breeze.', '', 'The torch Tensor and numpy array will share their underlying memory locations, and changing one will change the other.', '', '### 3.1 Converting torch Tensor to numpy Array']'\\n\\n 'code' cell: '['a = torch.ones(5)', 'print(a)']'\\n\\n 'code' cell: '['b = a.numpy()', 'print(b)']'\\n\\n 'markdown' cell: '['See how the numpy array changed in value: the `numpy()` method provides a *view* of the original tensor, not a copy.']'\\n\\n 'code' cell: '['a.add_(1)', 'print(a)', 'print(b)']'\\n\\n 'markdown' cell: '['### 3.2 Converting numpy Array to torch Tensor', '', 'See how changing the np array changed the torch Tensor automatically']'\\n\\n 'code' cell: '['import numpy as np', 'a = np.ones(5)', 'b = torch.from_numpy(a)', 'np.add(a, 1, out=a)', 'print(a)', 'print(b)']'\\n\\n 'markdown' cell: '['# Assignment', '', '1. create a tensor of size (5, 2) containing ones', '2. now convert it to a numpy array', '3. now convert it back to a torch tensor']'\", metadata={'source': 'knowledgeBase\\\\Week3-Feed-forward-NeuralNetworks-in-PyTorch\\\\Notebooks\\\\3.1-what-is-pytorch.ipynb', 'start_index': 2140, 'id': 1}),\n",
       "  1.0811372),\n",
       " (Document(page_content='\\'markdown\\' cell: \\'[\\'## Implementing an RNN\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'We will implement the forward pass, backward pass, optimization and training loop for an RNN in numpy so that you can get familiar with the recurrent nature of RNNs. Later, we will go back to PyTorch and appreciate how convenient the implementation becomes!\\']\\'\\n\\n \\'markdown\\' cell: \\'[\"Let\\'s first define the necessary model parameters. Recall that an $n \\\\\\\\times m$ weight matrix maps $\\\\\\\\mathbb{R}^{m} \\\\\\\\rightarrow \\\\\\\\mathbb{R}^{n}$.\"]\\'', metadata={'source': 'knowledgeBase\\\\Week5-Transformers-and-recurrent-neural-networks\\\\5_3-Recurrent-Neural-Networks-Numpy.ipynb', 'start_index': 16251, 'id': 7}),\n",
       "  1.0931538),\n",
       " (Document(page_content='\\'markdown\\' cell: \\'[\\'# Week 5 - Recurrent Neural Networks\\', \\'\\', \\'In this lab, we will introduce different ways of learning from sequential data.\\', \\'\\', \\'As a recurring example, we will train neural networks to do language modelling, i.e. predict the next token in a sentence. In the context of natural language processing a token could be a character or a word, but mind you that the concepts introduced here apply to all kinds of sequential data, such as e.g. protein sequences, weather measurements, audio signals, or videos, just to name a few.\\', \\'\\', \\'To really get a grasp of what is going on inside a recurrent neural network (RNN), we will carry out a substantial part of this exercise in Nanograd rather than PyTorch. \\', \\'\\', \"We start off with a simple toy problem, build an RNN using Nanograd, train it, and see for ourselves that it really works. Once we\\'re convinced, you will implement the Long Short-Term Memory (LSTM) cell, also in Nanograd. \", \\'\\', \\'This is *not* simple but with the DenseLayer class we already have, it is doable. Having done it yourself will help you understand what happens under the hood of the PyTorch code we will use throughout the course.\\', \\'\\', \\'To summarize, in this notebook we will show you:\\', \\'* How to represent sequences of categorical variables\\', \\'* How to build and train an RNN in Nanograd\\', \\'* How to build and train an LSTM network in Nanograd\\', \\'* How to build and train an LSTM network in PyTorch\\', \\'\\', \\'\\', \\'[Numpy version of the Notebook (previous version)](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/5_Recurrent/OLD-5.1-Numpy-Recurrent-Neural-Networks.ipynb)\\']\\'', metadata={'source': 'knowledgeBase\\\\Week5-Transformers-and-recurrent-neural-networks\\\\5_2_Recurrent_Neural_Networks_Nanograd.ipynb', 'start_index': 0, 'id': 0}),\n",
       "  1.0999012)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score(\"What is PyTorch?\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve relevant notebooks per evaluation question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Question', 'Answer', 'Source'], dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset_path = Path('eval_data/evaluation_dataset.csv')\n",
    "evaluation_df = read_data(qa_dataset_path)\n",
    "evaluation_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 25.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(evaluation_df.iterrows(), total=evaluation_df.shape[0]):\n",
    "    query = row['Question']\n",
    "    retrieved_docs = vectorstore.similarity_search_with_score(query, k=retrieval_k)\n",
    "    for idx, retrieved_doc in enumerate(retrieved_docs):\n",
    "        idx = idx + 1\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_source'.format(idx)] = retrieved_doc[0].metadata['source']\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_id'.format(idx)] = int(retrieved_doc[0].metadata['id'])\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_content'.format(idx)] = retrieved_doc[0].page_content\n",
    "        evaluation_df.at[index, 'Retrived_doc_{}_score'.format(idx)] = retrieved_doc[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_notebook_id(strings):\n",
    "    \"\"\"\n",
    "    Convert file paths to simplified notebook identifiers.\n",
    "\n",
    "    Args:\n",
    "    strings (list of str): List of file paths as strings.\n",
    "\n",
    "    Returns:\n",
    "    list of str: List of simplified notebook identifiers.\n",
    "    \"\"\"\n",
    "    notebook_ids = []\n",
    "    for string in strings:\n",
    "        # Split the string by '\\\\' to get the components\n",
    "        parts = string.split('\\\\')\n",
    "        # Get the last part (filename), split by '-' and take the first element\n",
    "        notebook_number = parts[-1].split('-')[0]\n",
    "        # Replace '.' with '_' in the notebook number\n",
    "        notebook_number = notebook_number.replace('.', '_')\n",
    "        # Combine with 'notebook ' prefix\n",
    "        notebook_id = f'notebook {notebook_number}'\n",
    "        notebook_ids.append(notebook_id)\n",
    "\n",
    "    return notebook_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the retrieved docs\n",
    "for idx in range(1,retrieval_k+1):\n",
    "    evaluation_df['Retrived_doc_{}_source'.format(idx)] = convert_string_to_notebook_id(evaluation_df['Retrived_doc_{}_source'.format(idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 1626.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# find Recall@k\n",
    "for index, row in tqdm(evaluation_df.iterrows(), total=evaluation_df.shape[0]):\n",
    "    sources = [source.strip() for source in row['Source'].split(',')]\n",
    "    correct_retrieved_docs = 0\n",
    "    for idx in range(1,retrieval_k+1):\n",
    "        if row['Retrived_doc_{}_source'.format(idx)] in sources:\n",
    "            correct_retrieved_docs += 1\n",
    "        evaluation_df.at[index, 'recall@{}'.format(idx)] = correct_retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_k = {}\n",
    "for idx in range(1,retrieval_k+1):\n",
    "    queries_with_no_relevant_docs = int(evaluation_df['recall@{}'.format(idx)].value_counts()[0])\n",
    "    queries_with_relevant_docs = int(evaluation_df.shape[0] - queries_with_no_relevant_docs)\n",
    "    recall_k[idx] = round(queries_with_relevant_docs/evaluation_df.shape[0] * 100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_json = {\n",
    "    'recall@{}'.format(idx): recall_k[idx] for idx in range(1,retrieval_k+1)\n",
    "}\n",
    "recall_k_json_name = \"recall_k_{}.json\".format(vectorstore_name)\n",
    "with open(recall_k_json_name, 'w') as f:\n",
    "    json.dump(result_json, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
